{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBPiQyZQB-hs",
        "outputId": "58f8254a-9209-47b1-c6bd-83e6327fbf6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.43)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.4.26)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Downloading pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-client\n",
            "Successfully installed pinecone-client-6.0.0 pinecone-plugin-interface-0.0.7\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pinecone-client\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PppuhaNCbPm",
        "outputId": "2b9f4b40-856a-4fea-ac92-60f3fc7e1552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvRJoBuVDya9",
        "outputId": "180d0109-8321-4cc6-e20d-31e9051748b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.63)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.25)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.43)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwb_fL7MC3Oc"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xidv8rfQcIQj"
      },
      "outputs": [],
      "source": [
        "!unzip -q pdfs.zip -d pdfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvgdgeyTmeyM"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgf6FM7MmuVY",
        "outputId": "597e557f-ea17-4cdd-ccc9-c0adb9d0341f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL\\nENHANCEMENTS\\nRahima Khanam* and Muhammad Hussain\\nDepartment of Computer Science, Huddersfield University, Queensgate, Huddersfield HD1 3DH, UK;\\n*Correspondence: rahima.khanam@hud.ac.uk;\\nOctober 24, 2024\\nABSTRACT\\nThis study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only\\nLook Once) series of object detection models. We examine the models architectural innovations,\\nincluding the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object\\ndetection (OBB). We review the model’s performance improvements in terms of mean Average\\nPrecision (mAP) and computational efficiency compared to its predecessors, with a focus on the\\ntrade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11’s\\nversatility across different model sizes, from nano to extra-large, catering to diverse application needs\\nfrom edge devices to high-performance computing environments. Our research provides insights into\\nYOLOv11’s position within the broader landscape of object detection and its potential impact on\\nreal-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction\\nComputer vision, a rapidly advancing field, enables machines to interpret and understand visual data [ 1]. A crucial\\naspect of this domain is object detection[2], which involves the precise identification and localization of objects within\\nimages or video streams[3]. Recent years have witnessed remarkable progress in algorithmic approaches to address this\\nchallenge [4].\\nA pivotal breakthrough in object detection came with the introduction of the You Only Look Once (YOLO) algorithm\\nby Redmon et al. in 2015 [5]. This innovative approach, as its name suggests, processes the entire image in a single pass\\nto detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by\\nframing object detection as a regression problem [5]. It employs a single convolutional neural network to simultaneously\\npredict bounding boxes and class probabilities across the entire image [6], streamlining the detection pipeline compared\\nto more complex traditional methods.\\nYOLOv11 is the latest iteration in the YOLO series, building upon the foundation established by YOLOv1. Unveiled at\\nthe YOLO Vision 2024 (YV24) conference, YOLOv11 represents a significant leap forward in real-time object detection\\ntechnology. This new version introduces substantial enhancements in both architecture and training methodologies,\\npushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail\\ncapture while maintaining a lean parameter count. This results in improved accuracy across a diverse range of computer\\narXiv:2410.17725v1  [cs.CV]  23 Oct 2024'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nvision (CV) tasks, from object detection to classification. Furthermore, YOLOv11 achieves remarkable gains in\\nprocessing speed, substantially enhancing real-time performance capabilities.\\nIn the following sections, this paper will provide a comprehensive analysis of YOLOv11’s architecture, exploring its\\nkey components and innovations. We will examine the evolution of YOLO models, leading up to the development\\nof YOLOv11. The study will delve into the model’s expanded capabilities across various CV tasks, including object\\ndetection, instance segmentation, pose estimation, and oriented object detection. We will also review YOLOv11’s\\nperformance improvements in terms of accuracy and computational efficiency compared to its predecessors, with a\\nparticular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11\\non real-time CV applications and its position within the broader landscape of object detection technologies.\\n2 Evolution of YOLO models\\nTable 1 illustrates the progression of YOLO models from their inception to the most recent versions. Each iteration has\\nbrought significant improvements in object detection capabilities, computational efficiency, and versatility in handling\\nvarious CV tasks.\\nTable 1: YOLO: Evolution of models\\nRelease Year Tasks Contributions Framework\\nYOLO [5] 2015 Object Detection, Basic Classifica-\\ntion\\nSingle-stage object detector Darknet\\nYOLOv2 [7] 2016 Object Detection, Improved Classi-\\nfication\\nMulti-scale training, dimension clus-\\ntering\\nDarknet\\nYOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone\\nDarknet\\nYOLOv5 [10] 2020 Object Detection, Basic Instance\\nSegmentation (via custom modifica-\\ntions)\\nAnchor-free detection, SWISH acti-\\nvation, PANet\\nPyTorch\\nYOLOv6 [11] 2022 Object Detection, Instance Segmen-\\ntation\\nSelf-attention, anchor-free OD PyTorch\\nYOLOv7 [12] 2022 Object Detection, Object Tracking,\\nInstance Segmentation\\nTransformers, E-ELAN reparame-\\nterisation\\nPyTorch\\nYOLOv8 [13] 2023 Object Detection, Instance Segmen-\\ntation, Panoptic Segmentation, Key-\\npoint Estimation\\nGANs, anchor-free detection PyTorch\\nYOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch\\nThis evolution showcases the rapid advancement in object detection technologies, with each version introducing novel\\nfeatures and expanding the range of supported tasks. From the original YOLO’s groundbreaking single-stage detection\\nto YOLOv10’s NMS-free training, the series has consistently pushed the boundaries of real-time object detection.\\nThe latest iteration, YOLO11, builds upon this legacy with further enhancements in feature extraction, efficiency,\\nand multi-task capabilities. Our subsequent analysis will delve into YOLO11’s architectural innovations, including\\nits improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nspatial attention in the feature maps. This spatial attention mechanism allows the model to focus more effectively on\\nimportant regions within the image. By pooling features spatially, the C2PSA block enables YOLO11 to concentrate on\\nspecific areas of interest, potentially improving detection accuracy for objects of varying sizes and positions.\\n4.2 Neck\\nThe neck combines features at different scales and transmits them to the head for prediction. This process typically\\ninvolves upsampling and concatenation of feature maps from different levels, enabling the model to capture multi-scale\\ninformation effectively.\\n4.2.1 C3k2 Block\\nYOLO11 introduces a significant change by replacing the C2f block in the neck with the C3k2 block. The C3k2 block\\nis designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After\\nupsampling and concatenation, the neck in YOLO11 incorporates this improved block, resulting in enhanced speed and\\nperformance [18].\\n4.2.2 Attention Mechanism\\nA notable addition to YOLO11 is its increased focus on spatial attention through the C2PSA module. This attention\\nmechanism enables the model to concentrate on key regions within the image, potentially leading to more accurate\\ndetection, especially for smaller or partially occluded objects. The inclusion of C2PSA sets YOLO11 apart from its\\npredecessor, YOLOv8, which lacks this specific attention mechanism [18].\\n4.3 Head\\nThe head of YOLOv11 is responsible for generating the final predictions in terms of object detection and classification.\\nIt processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block\\nIn the head section, YOLOv11 utilizes multiple C3k2 blocks to efficiently process and refine the feature maps. The\\nC3k2 blocks are placed in several pathways within the head, functioning to process multi-scale features at different\\ndepths. The C3k2 block exhibits flexibility depending on the value of the c3k parameter:\\n• When c3k = False, the C3k2 module behaves similarly to the C2f block, utilizing a standard bottleneck\\nstructure.\\n• When c3k = True, the bottleneck structure is replaced by the C3 module, which allows for deeper and more\\ncomplex feature extraction.\\nKey characteristics of the C3k2 block:\\n• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.\\n• Parameter efficiency: C3k2 is a more compact version of the CSP bottleneck, making the architecture more\\nefficient in terms of the number of trainable parameters.\\nAnother notable addition is the C3k block, which offers enhanced flexibility by allowing customizable kernel sizes. The\\nadaptability of C3k is particularly useful for extracting more detailed features from images, contributing to improved\\ndetection accuracy.\\n4.3.2 CBS Blocks\\nThe head of YOLOv11 includes several CBS (Convolution-BatchNorm-Silu) [19] layers after the C3k2 blocks. These\\nlayers further refine the feature maps by:\\n• Extracting relevant features for accurate object detection.\\n• Stabilizing and normalizing the data flow through batch normalization.\\n4'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n• Utilizing the Sigmoid Linear Unit (SiLU) activation function for non-linearity, which improves model perfor-\\nmance.\\nCBS blocks serve as foundational components in both feature extraction and the detection process, ensuring that the\\nrefined feature maps are passed to the subsequent layers for bounding box and classification predictions.\\n4.3.3 Final Convolutional Layers and Detect Layer\\nEach detection branch ends with a set of Conv2D layers, which reduce the features to the required number of outputs for\\nbounding box coordinates and class predictions. The final Detect layer consolidates these predictions, which include:\\n• Bounding box coordinates for localizing objects in the image.\\n• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11\\nYOLO11 supports a diverse range of CV tasks, showcasing its versatility and power in various applications. Here’s an\\noverview of the key tasks:\\n1. Object Detection: YOLO11 excels in identifying and localizing objects within images or video frames,\\nproviding bounding boxes for each detected item [ 20]. This capability finds applications in surveillance\\nsystems, autonomous vehicles, and retail analytics, where precise object identification is crucial [21].\\n2. Instance Segmentation: Going beyond simple detection, YOLO11 can identify and separate individual\\nobjects within an image down to the pixel level [20]. This fine-grained segmentation is particularly valuable in\\nmedical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].\\n3. Image Classification: YOLOv11 is capable of classifying entire images into predetermined categories,\\nmaking it ideal for applications like product categorization in e-commerce platforms or wildlife monitoring in\\necological studies [21].\\n4. Pose Estimation: The model can detect specific key points within images or video frames to track movements\\nor poses. This capability is beneficial for fitness tracking applications, sports performance analysis, and various\\nhealthcare applications requiring motion assessment [21].\\n5. Oriented Object Detection (OBB): YOLO11 introduces the ability to detect objects with an orientation angle,\\nallowing for more precise localization of rotated objects. This feature is especially valuable in aerial imagery\\nanalysis, robotics, and warehouse automation tasks where object orientation is crucial [21].\\n6. Object Tracking: It identifies and traces the path of objects in a sequence of images or video frames[ 21].\\nThis real-time tracking capability is essential for applications such as traffic monitoring, sports analysis, and\\nsecurity systems.\\nTable 2 outlines the YOLOv11 model variants and their corresponding tasks. Each variant is designed for specific\\nuse cases, from object detection to pose estimation. Moreover, all variants support core functionalities like inference,\\nvalidation, training, and export, making YOLOv11 a versatile tool for various CV applications.\\n6 Advancements and Key Features of YOLOv11\\nYOLOv11 represents a significant advancement in object detection technology, building upon the foundations laid by\\nits predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics\\nshowcases enhanced architectural designs, more sophisticated feature extraction techniques, and refined training\\nmethodologies. The synergy of YOLOv11’s rapid processing, high accuracy, and computational efficiency positions it\\nas one of the most formidable models in Ultralytics’ portfolio to date [22]. A key strength of YOLOv11 lies in its refined\\narchitecture, which facilitates the detection of subtle details even in challenging scenarios. The model’s improved\\nfeature extraction capabilities allow it to identify and process a broader range of patterns and intricate elements within\\nimages. Compared to earlier versions, YOLOv11 introduces several notable enhancements:\\n5'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nTable 2: YOLOv11 Model Variants and Tasks\\nModel Variants Task Inference Validation Training Export\\nYOLOv11 yolo11-nano yolo11-small\\nyolo11-medium yolo11-\\nlarge yolo11-xlarge\\nDetection ✓ ✓ ✓ ✓\\nYOLOv11-seg yolo11-nano-seg yolo11-\\nsmall-seg yolo11-medium-\\nseg yolo11-large-seg\\nyolo11-xlarge-seg\\nInstance Segmen-\\ntation\\n✓ ✓ ✓ ✓\\nYOLOv11-pose yolo11-nano-pose yolo11-\\nsmall-pose yolo11-medium-\\npose yolo11-large-pose\\nyolo11-xlarge-pose\\nPose/Keypoints ✓ ✓ ✓ ✓\\nYOLOv11-obb yolo11-nano-obb yolo11-\\nsmall-obb yolo1-medium-\\nobb yolo11-large-obb\\nyolo11-xlarge-obb\\nOriented Detec-\\ntion\\n✓ ✓ ✓ ✓\\nYOLOv11-cls yolo11-nano-cls yolo11-\\nsmall-cls yolo11-medium-\\ncls yolo11-large-cls yolo11-\\nxlarge-cls\\nClassification ✓ ✓ ✓ ✓\\n1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m\\ncounterpart, demonstrating improved computational efficiency without compromising accuracy [23].\\n2. Versatility in CV tasks: YOLOv11 exhibits proficiency across a diverse array of CV applications, including\\npose estimation, object recognition, image classification, instance segmentation, and oriented bounding box\\n(OBB) detection [23].\\n3. Optimized speed and performance: Through refined architectural designs and streamlined training pipelines,\\nYOLOv11 achieves faster processing speeds while maintaining a balance between accuracy and computational\\nefficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].\\n5. Advanced feature extraction: YOLOv11 incorporates improvements in both its backbone and neck architec-\\ntures, resulting in enhanced feature extraction capabilities and, consequently, more precise object detection\\n[23].\\n6. Contextual adaptability: YOLOv11 demonstrates versatility across various deployment scenarios, including\\ncloud platforms, edge devices, and systems optimized for NVIDIA GPUs [23].\\nYOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].\\nThe performance comparison graph depicted in Figure 2 overs several key insights. The YOLOv11 variants (11n, 11s,\\n11m, and 11x) form a distinct performance frontier, with each model achieving higher COCO mAP 50−95 scores at\\ntheir respective latency points. Notably, the YOLOv11x achieves approximately 54.5% mAP50−95 at 13ms latency,\\nsurpassing all previous YOLO iterations. The intermediate variants, particularly YOLOv11m, demonstrate exceptional\\nefficiency by achieving comparable accuracy to larger models from previous generations while requiring significantly\\nless processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s\\nmaintains high accuracy (approximately 47% mAP50−95) while operating at speeds previously associated with much\\nless accurate models. This represents a crucial advancement for real-time applications where both speed and accuracy\\nare critical. The improvement curve of YOLOv11 also shows better scaling characteristics across its model variants,\\nsuggesting more efficient utilization of additional computational resources compared to previous generations.\\n6'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nFigure 2: Benchmarking YOLOv11 Against Previous Versions [23]\\n7 Discussion\\nYOLO11 marks a significant leap forward in object detection technology, building upon its predecessors while\\nintroducing innovative enhancements. This latest iteration demonstrates remarkable versatility and efficiency across\\nvarious CV tasks.\\n1. Efficiency and Scalability: YOLO11 introduces a range of model sizes, from nano to extra-large, catering\\nto diverse application needs. This scalability allows for deployment in scenarios ranging from resource-\\nconstrained edge devices to high-performance computing environments. The nano variant, in particular,\\nshowcases impressive speed and efficiency improvements over its predecessor, making it ideal for real-time\\napplications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature\\nextraction and processing capabilities. The incorporation of novel elements such as the C3k2 block, SPPF, and\\nC2PSA contributes to more effective feature extraction and processing. These enhancements allow the model\\nto better analyze and interpret complex visual information, potentially leading to improved detection accuracy\\nacross various scenarios.\\n3. Multi-Task Proficiency: YOLO11’s versatility extends beyond object detection, encompassing tasks such as\\ninstance segmentation, image classification, pose estimation, and oriented object detection. This multi-faceted\\napproach positions YOLO11 as a comprehensive solution for diverse CV challenges.\\n4. Enhanced Attention Mechanisms: A key advancement in YOLO11 is the integration of sophisticated spatial\\nattention mechanisms, particularly the C2PSA component. This feature enables the model to focus more\\neffectively on critical regions within an image, enhancing its ability to detect and analyze objects. The\\nimproved attention capability is especially beneficial for identifying complex or partially occluded objects,\\naddressing a common challenge in object detection tasks. This refinement in spatial awareness contributes to\\nYOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.\\n6. Implications for Real-World Applications: The advancements in YOLO11 have significant implications\\nfor various industries. Its improved efficiency and multi-task capabilities make it particularly suitable for\\napplications in autonomous vehicles, surveillance systems, and industrial automation. The model’s ability to\\nperform well across different scales also opens up new possibilities for deployment in resource-constrained\\nenvironments without compromising on performance.\\n7'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n8 Conclusion\\nYOLOv11 represents a significant advancement in the field of CV , offering a compelling combination of enhanced\\nperformance and versatility. This latest iteration of the YOLO architecture demonstrates marked improvements in\\naccuracy and processing speed, while simultaneously reducing the number of parameters required. Such optimizations\\nmake YOLOv11 particularly well-suited for a wide range of applications, from edge computing to cloud-based analysis.\\nThe model’s adaptability across various tasks, including object detection, instance segmentation, and pose estimation,\\npositions it as a valuable tool for diverse industries such as emotion detection [26], healthcare [27] and various other\\nindustries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses\\nseeking to implement or upgrade their CV systems. In summary, YOLOv11’s blend of enhanced feature extraction,\\noptimized performance, and broad task support establishes it as a formidable solution for addressing complex visual\\nrecognition challenges in both research and practical applications.\\nReferences\\n[1] Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image processing, analysis and machine vision. Springer, 2013.\\n[2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\\nProceedings of the IEEE, 111(3):257–276, 2023.\\n[3] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review.\\nIEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-\\nvoltaic defect detection. In Solar, volume 4, pages 351–386. MDPI, 2024.\\n[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,\\n2016.\\n[6] Juan Du. Understanding of object detection based on cnn family and yolo. In Journal of Physics: Conference\\nSeries, volume 1004, page 012029. IOP Publishing, 2018.\\n[7] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\\n[10] Roboflow Blog Jacob Solawetz. What is yolov5? a guide for beginners., 2020. Accessed: 21 October 2024.\\n[11] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint\\narXiv:2209.02976, 2022.\\n[12] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new\\nstate-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.\\n[13] Francesco Jacob Solawetz. What is yolov8? the ultimate guide, 2023. Accessed: 21 October 2024.\\n[14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using\\nprogrammable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time\\nend-to-end object detection. arXiv preprint arXiv:2405.14458, 2024.\\n[16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024.\\n[17] Rahima Khanam, Muhammad Hussain, Richard Hill, and Paul Allen. A comprehensive review of convolutional\\nneural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.\\n[19] Jingwen Feng, Qiaofeng An, Jiahao Zhang, Shuxun Zhou, Guangwei Du, and Kai Yang. Application of yolov7-tiny\\nin the detection of steel surface defects. In 2024 5th International Seminar on Artificial Intelligence, Networking\\nand Information Technology (AINIT), pages 2241–2245. IEEE, 2024.\\n8'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n[20] Ultralytics. Instance segmentation and tracking, 2024. Accessed: 2024-10-21.\\n[21] Ultralytics Abirami Vina. Ultralytics yolo11 has arrived: Redefine what’s possible in ai, 2024. Accessed:\\n2024-10-21.\\n[22] Viso.AI Gaudenz Boesch. Yolov11: A new iteration of “you only look once. https://viso.ai/\\ncomputer-vision/yolov11/, 2024. Accessed: 2024-10-21.\\n[23] Ultralytics. Ultralytics yolov11. https://docs.ultralytics.com/models/yolo11/s, 2024. Accessed:\\n21-Oct-2024.\\n[24] Rahima Khanam and Muhammad Hussain. What is yolov5: A deep look into the internal features of the popular\\nobject detector. arXiv preprint arXiv:2407.20892, 2024.\\n[25] DigitalOcean. What’s new in yolov11 transforming object detection once again part 1, 2024. Accessed: 2024-10-\\n21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.\\nIn Kids Cybersecurity Using Computational Intelligence Techniques, pages 165–174. Springer, 2023.\\n[27] Burcu Ataer Aydin, Muhammad Hussain, Richard Hill, and Hussain Al-Aqrabi. Domain modelling for a\\nlightweight convolutional network focused on automated exudate detection in retinal fundus images. In 2023 9th\\nInternational Conference on Information Technology Trends (ITT), pages 145–150. IEEE, 2023.\\n9'),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-12-23T05:25:35+00:00', 'author': '', 'keywords': '', 'moddate': '2024-12-23T05:25:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/Pranav Balaji R S_CV(92).pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Pranav Balaji R S\\n214-A, Engineers Avenue 1st Street, S Kolathur, Chennai-117\\n♂phone+91-9342579245 /envel⌢pepranavbalajirs@gmail.com /linkedinlinkedin.com/in/pranavbalajirs /githubgithub.com/PRANAVBALAJIRS\\nEducation\\nVellore Institute of Technology Sep. 2021 – Jul 2025\\nB.Tech Computer Science and Engineering with spec. in AI and ML Chennai, India\\nRelevant Coursework\\n• Artificial Intelligence\\n• Probability and Statistics\\n• Machine Learning\\n• Deep Learning\\n• Data Structures\\n• Software Engineering\\n• Database Management\\n• Operating Systems\\nExperience\\nDverse Technologies Nov 2023 – Jan 2024\\nSoftware Development Intern Chennai, India\\n• Conceived and deployed innovative HCI solutions tailored for visually impaired users using Computer Vision and Hand\\ntracking, reducing task completion time by 35% and significantly improving accessibility and user satisfaction.\\n• Engineered advanced Human-Computer Interface (HCI) devices for visually impaired individuals, increasing usability by\\n40% and enhancing overall user experience through rigorous testing and iterative design improvements.\\n• Strengthened codebase integrity through comprehensive Git documentation and exhaustive testing protocols; decreased\\ncritical issues by 35% and enhanced overall system stability, leading to a 25% improvement in user satisfaction.\\nProjects\\nVision Revive - Image Dehazing and De-smoking System | Python, PyTorch, NumPy, Matplotlib Apr 2024\\n• Designed Feature Fusion Attention Network (FFA-Net) that accurately removes smoke and haze from images, enhancing\\nvisual clarity for over 1500 images tested.\\n• Improved Peak Signal to Noise Ratio (PSNR) by 30% over the original FFA-Net paper by optimizing network parameters\\nand implementing advanced image processing techniques.\\nLyric Muse - Lyric Conditioned Melody Generator | Python, TensorFlow, NumPy Feb 2024\\n• Created a lyric conditioned melody generator using the Conditional LSTM-GAN approach with a dataset having over\\n12,200 MIDI songs, improving melody generation quality.\\n• Generated melodies demonstrated an average increase of 3.5 points in Melodic Contour Similarity Index (MCSI)\\ncompared to baseline LSTM models, indicating enhanced melodic coherence and fidelity.\\nSnow Sentry - Avalanche Prediction and Mitigation System | Python, TensorFlow, SciKit-Learn Jan 2024\\n• Developed an AI-powered avalanche detection system leveraging deep learning models for real-time risk assessment and\\nearly warning, analysing over 18,000 images.\\n• Reached a prediction accuracy of 92% on a test dataset of historical avalanche data, with a false positive rate reduced to\\n5%. Successfully provided timely warnings 24 hours in advance with 85% reliability.\\nTechnical Skills\\nLanguages: Python, C, C++, Java, SQL\\nFrameworks: TensorFlow, PyTorch, SciKit-Learn, Flask, keras, Hugging Face\\nTools and Platforms: Git, GitHub, Jupyter, VS Code\\nCertifications: TensorFlow Developer Certificate, TensorFlow (2023) — AI and ML Externship, Google Developers (2023)\\nAchievements\\nTech Researchers Club Sep 2023 - Dec 2023\\nProject Author VIT Chennai\\n• Implemented a robust dataset pre-processing solution, automating data cleansing processes, allowing research teams\\nwithin the Tech Researchers Club to focus more on analysis.\\nAmazon ML Summer School Sep 2023 - Oct 2023\\nMentee Amazon\\n• Among the top 3000 out of 61,000 participants to qualify for and successfully complete an Machine Learning intensive\\nprogram.')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO-kiBNnmvSE"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VTP_bUhm_89"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNaqSAgxnIVM",
        "outputId": "5d8c390e-bb32-4e14-c193-294f2aab7419"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpCCdfmTnJzV",
        "outputId": "1b85b0d0-3f31-4724-e097-3a3b60168f0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'pdfs/pdfs/yolo11.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYAeWSX9uN3F"
      },
      "source": [
        "Download Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p1DFKoDuCcI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfWbN-o1uZuN",
        "outputId": "2972e50a-4233-46c6-b40b-418f7411a48d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-73ad2f8e367a>:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7zTGG3xuefG"
      },
      "outputs": [],
      "source": [
        "result = embeddings.embed_query(\"Hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXrAIUiqufxw",
        "outputId": "7642edc8-db6f-489c-ca07-4de578f50edd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1f-pYwwJvS"
      },
      "source": [
        "Intialize the PineCone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRPiULh8usg9"
      },
      "outputs": [],
      "source": [
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV','us-east-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Y310fWcvv1"
      },
      "outputs": [],
      "source": [
        "texts = [doc.page_content for doc in text_chunks]\n",
        "metadatas = [doc.metadata for doc in text_chunks]\n",
        "vectors = embeddings.embed_documents(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJIdUZJqwsc3",
        "outputId": "5afb2fb4-57fe-4467-be2e-f821b516d4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents successfully indexed.\n"
          ]
        }
      ],
      "source": [
        "client = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = client.Index(index_name)\n",
        "\n",
        "items = [\n",
        "    {\"id\": f\"doc-{i}\", \"values\": vector, \"metadata\": metadatas[i]}\n",
        "    for i, vector in enumerate(vectors)\n",
        "]\n",
        "index.upsert(vectors=items)\n",
        "\n",
        "print(\"Documents successfully indexed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AtsrZ6zdidr",
        "outputId": "f10d5711-f1fc-43ae-e06a-d9961047e0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (0.3.63)\n",
            "Requirement already satisfied: pinecone<8.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (7.0.2)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain-pinecone) (2.0.2)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain-pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain-pinecone)\n",
            "  Downloading langchain_openai-0.3.19-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.126 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.3.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (4.13.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.11.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain-pinecone) (1.82.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain-pinecone) (0.9.0)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2025.4.26)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.6.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.4.0)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (3.11.15)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.20.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai>=0.3.11->langchain-pinecone) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (0.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<8.0.0,>=6.0.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain-pinecone) (1.17.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain-pinecone) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (9.0.0)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.126->langchain-core<1.0.0,>=0.3.34->langchain-pinecone) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone) (0.1.2)\n",
            "Downloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading langchain_openai-0.3.19-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (25 kB)\n",
            "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: vcrpy, syrupy, pytest-socket, pytest-benchmark, pytest-asyncio, pytest-recording, pytest-codspeed, aiohttp-retry, langchain-tests, langchain-openai, langchain-pinecone\n",
            "Successfully installed aiohttp-retry-2.9.1 langchain-openai-0.3.19 langchain-pinecone-0.2.8 langchain-tests-0.3.20 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-3.2.0 pytest-recording-0.13.4 pytest-socket-0.7.0 syrupy-4.9.1 vcrpy-7.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U langchain-pinecone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8MiCdwH0JfW"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_core.vectorstores import VectorStoreRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbk4FIchODxX"
      },
      "outputs": [],
      "source": [
        "vectorstore = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings,\n",
        "    text_key=\"text\",\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFu2venSf1O5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILT5SnmedoBK"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "texts = [t.page_content for t in text_chunks]\n",
        "\n",
        "# Create and index embeddings (this performs upsert automatically)\n",
        "docsearch = PineconeVectorStore.from_texts(\n",
        "    texts=texts,\n",
        "    embedding=embeddings,\n",
        "    index_name=\"test\",  # your existing index name\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWaIl6NFe2La"
      },
      "outputs": [],
      "source": [
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtyMbhhcf66Z",
        "outputId": "17f95133-ddb9-4ff3-c105-a2f0b5ffdcc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7d35162b2e50>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNpARPPOgAAM"
      },
      "outputs": [],
      "source": [
        "query = \"YOLOv11 outperforms which models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xef4UYIJgErV",
        "outputId": "7e063e1d-9b01-4054-dc3d-d2307c569deb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(id='ac145641-ae9d-4e42-8946-ef15fa93e08a', metadata={}, page_content='YOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].'),\n",
              " Document(id='a568a53a-d0ee-4d7e-b973-b795fceeaebb', metadata={}, page_content='YOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = docsearch.similarity_search(query, k=3)\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oLtRnXbgG5r",
        "outputId": "f6ae4d34-08f1-4a98-ebcc-bb8b0e051c38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-45-4ab60154b5e1>:1: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI()\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuLI6cypgOGH"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "N-hbVIKigQk4",
        "outputId": "32ad38bf-fbcf-4f56-c6bc-1e3b103eb0da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-47-fbc3ad05536a>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  qa.run(query)\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' YOLOv5, YOLOv10'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajZnZae8gU74"
      },
      "outputs": [],
      "source": [
        "query = \"Pranav Balaji R S experience and certifications\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "miPfuwhIggp7",
        "outputId": "30b47ea3-62ae-4df8-edb2-8b7189a5d8b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n",
            "WARNING:langchain_pinecone.vectorstores:Found document with no `text` key. Skipping.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Pranav Balaji R S has experience with frameworks such as TensorFlow, PyTorch, SciKit-Learn, Flask, keras, and Hugging Face, as well as tools and platforms such as Git, GitHub, Jupyter, and VS Code. He also has certifications in TensorFlow, AI and ML Externship, and Google Developers. Additionally, he has experience as a Project Author at the Tech Researchers Club at VIT Chennai.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmJZNTY9ghtQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
