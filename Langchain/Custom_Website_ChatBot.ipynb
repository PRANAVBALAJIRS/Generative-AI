{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBtRo632xxW7",
        "outputId": "f7a0f251-5d39-4177-9c09-9cf318a666d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain langchain-community\n",
        "!pip -q install pypdf\n",
        "!pip -q install sentence_transformers\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRSxHjdkyk4m",
        "outputId": "a5b28a22-01f8-4aea-b864-3202923aacbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.4.26)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "!pip install faiss-cpu\n",
        "!pip -q install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "9eAfl0mBzTP5",
        "outputId": "8451ba33-f1b8-46d4-9d65-f3eb6649971a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.3.25 requires numpy>=1.26.2; python_version < \"3.13\", but you have numpy 1.24.4 which is incompatible.\n",
            "faiss-cpu 1.11.0 requires numpy<3.0,>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3c7294e19ce349309e5d3b1681eb0036",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install nltk==3.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2tgQF1zzbSG"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import textwrap\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X6CrsjN0TKM",
        "outputId": "44e9401b-0ed3-40d3-9094-1cc096a7b80d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwa67nFe0oPV"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI0RIgYf0zwI"
      },
      "outputs": [],
      "source": [
        "URLs = [\n",
        "    \"https://www.datacamp.com/blog/introduction-to-meta-ai-llama\",\n",
        "    \"https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d\",\n",
        "    \"https://blogs.nvidia.com/blog/what-is-agentic-ai/\",\n",
        "    \"https://aws.amazon.com/what-is/artificial-general-intelligence/\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY4FeTsb3Pww"
      },
      "outputs": [],
      "source": [
        "loaders = UnstructuredURLLoader(urls=URLs)\n",
        "data = loaders.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcCCpFPH3dcb",
        "outputId": "cf38b998-3c24-4038-9d79-4ec36c85b1af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://www.datacamp.com/blog/introduction-to-meta-ai-llama'}, page_content=''),\n",
              " Document(metadata={'source': 'https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d'}, page_content='Sitemap\\n\\nSign in\\n\\nWrite\\n\\nSign in\\n\\nStable Diffusion Explained\\n\\nHow does Stable diffusion work? Explaining the tech behind text to image generation.\\n\\nOnkar Mishra\\n\\nOnkar Mishra\\n\\n6 min read\\n\\nJun 8, 2023\\n\\n--\\n\\nLarge text to image models have achieved remarkable success in enabling high quality synthesis of images from text prompts. Diffusion models can be applied to text to image generation tasks to achieve state of art image generating results.\\n\\nStable Diffusion model has achieved state of the art results for image generation. Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion model, proposed in High-Resolution Image Synthesis with Latent Diffusion Models and created by the researchers and engineers from CompVis, LMU and RunwayML. The model was initially trained on 512x512 images from a subset of the LAION-5B database.\\n\\nThis is particularly achieved by encoding text inputs into latent vectors using pretrained language models like CLIP. Diffusion models can achieve state-of-the-art results for generating image data from texts. But the process of denoising is very slow and consumes a lot of memory when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference.\\n\\nIn this regard, latent diffusion can reduce the memory and computational time by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. In latent diffusion, the model is trained to generate latent (compressed) representations of the images.\\n\\nTraining of Diffusion Model\\n\\nStable Diffusion is a large text to image diffusion model trained on billions of images. Image diffusion model learn to denoise images to generate output images. Stable Diffusion uses latent images encoded from training data as input. Further, given an image zo, the diffusion algorithm progressively add noise to the image and produces a noisy image zt, with t being how many times noise is added. When t is large enough, the image approximates pure noise. Given a set of inputs such as time step t, text prompt, image diffusion algorithms learn a network to predict the noise added to the noisy image zt.\\n\\nThere are mainly three main components in latent diffusion:\\n\\nAn autoencoder (VAE).\\n\\nA U-Net.\\n\\nA text-encoder, e.g. CLIP’s Text Encoder.\\n\\n1. The autoencoder (VAE)\\n\\nThe VAE model has two parts, an encoder and a decoder. During latent diffusion training, the encoder converts a 512*512*3 image into a low dimensional latent representation of image of size say 64*64*4 for the forward diffusion process. We call these small encoded versions of images as latents. We apply more and more noise to these latents at each step of training. This encoded latent representation of images acts as the input to the U-Net model.\\n\\nHere, we are converting an image of shape (3, 512, 512) into a latent of shape(4, 64, 64), which requires 48 times less memory. This leads to reduced memory and compute requirements compared to pixel-space diffusion models. Thus, we are able to generate 512 × 512 images very quickly on 16GB Colab GPUs as well.\\n\\nThe decoder transforms the latent representation back into an image. We convert the denoised latents generated by the reverse diffusion process into images using the VAE decoder.\\n\\nDuring inference, we only need the VAE decoder to convert the denoised image into actual images.\\n\\nfrom torchvision import transforms as tfms\\nfrom diffusers import AutoencoderKL\\n\\n# Load the autoencoder model which will be used to decode the latents into image space. \\nvae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\\n\\n# To the GPU we go!\\nvae = vae.to(torch_device)\\n\\n# Convert PIL image to latents\\n\\ndef pil_to_latent(input_im):\\n    # Single image -> single latent in a batch (so size 1, 4, 64, 64)\\n    with torch.no_grad():\\n        latent = vae.encode(tfms.ToTensor()(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\\n    return 0.18215 * latent.latent_dist.sample()\\n\\n2. UNet\\n\\nThe U-Net predicts denoised image representation of noisy latents. Here, noisy latents act as input to Unet and the output of UNet is noise in the latents. Using this, we are able to get actual latents by subtracting the noise from the noisy latents.\\n\\nThe Unet that takes in the noisy latents (x) and predicts the noise. We use a conditional model that also takes in the timestep (t) and our text embedding as guidance.\\n\\nThus, the model looks like this:\\n\\nfrom diffusers import UNet2DConditionModel\\n\\n# The UNet model for generating the latents.\\nunet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\\n\\n# To the GPU\\nunet = unet.to(torch_device);\\nnoise_pred = unet(latents, t, encoder_hidden_states=text_embeddings)[\"sample\"]\\n\\nThe model is essentially a UNet with an encoder(12 blocks), a middle block and a skip connected decoder(12 blocks). In these 25 blocks, 8 blocks are down sampling or upsampling convolution layer and 17 blocks are main blocks that each contain four resnet layers and two Vision Transformers(ViTs). Here the encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher resolution image representation that is supposedly less noisy.\\n\\n3. The Text-encoder\\n\\nThe text-encoder transforms the input prompt into an embedding space that goes as input to the U-Net. This acts as guidance for noisy latents when we train Unet for its denoising process. The text encoder is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text-embeddings. Stable Diffusion does not train a new text encoder and instead uses an already trained text encoder, CLIP. The text encoder creates embeddings corresponding to the input text.\\n\\nTokenization\\n\\nfrom transformers import CLIPTextModel, CLIPTokenizer\\n\\n# Load the tokenizer and text encoder to tokenize and encode the text. \\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\\ntext_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\\n\\n# To the GPU\\ntext_encoder = text_encoder.to(torch_device)\\n\\nprompt = \\'An astronaut riding a horse\\'\\n# Turn the text into a sequnce of tokens:\\ntext_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\\ninput_ids = text_input.input_ids.to(torch_device)\\n\\nOutput Embedding\\n\\n# Get output embeddings from tokens\\noutput_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\\nprint(\\'Shape:\\', output_embeddings.shape)\\n\\nPutting it all together, the model works as follow during inference process:\\n\\nScheduler\\n\\nApart from above 3, we have Scheduler which is used to add noise to an image and then use model to predict the noise.\\n\\nfrom diffusers import LMSDiscreteScheduler\\nscheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\\n\\nAbove sets up a scheduler used to train the model. In case, we want to set up a scheduler for smaller number of steps, we set up scheduler as follow:\\n\\n# Set the number of sampling steps:\\nscheduler.set_timesteps(15)\\n\\nLatent Diffusion Model like Stable Diffusion enable various creative applications like:\\n\\nText-to-Image Generation\\n\\nImage-to-Image Generation — Generate or modify new images based on a starting point\\n\\nImage Upscaling — Enlarge an image into larger image\\n\\nInpainting — Modify a specific area of an image by masking out the area and then generating new details on the area based on a provided prompt.\\n\\nLatent Diffusion Model also reduces the cost of training and inference that have the potential to democratise high resolution image synthesis to masses.\\n\\nIn my next blog, I will be discussing about textual inversion, which is a technique to fine tune Stable Diffusion to learn a novel concept or task.\\n\\nReference:\\n\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684–10695).\\n\\nZhang, L., & Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543.\\n\\nhttps://huggingface.co/docs/diffusers/index\\n\\nStable Diffusion\\n\\nAi Art\\n\\nImage Processing\\n\\nAI\\n\\nGenerative Art\\n\\n--\\n\\n--\\n\\nOnkar Mishra\\n\\nOnkar Mishra\\n\\nWritten by Onkar Mishra\\n\\n1.5K followers\\n\\n588 following\\n\\nResponses (8)\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nPress\\n\\nBlog\\n\\nPrivacy\\n\\nRules\\n\\nTerms\\n\\nText to speech'),\n",
              " Document(metadata={'source': 'https://blogs.nvidia.com/blog/what-is-agentic-ai/'}, page_content='What Is Agentic AI?\\n\\nAgentic AI uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems.\\n\\nOctober 22, 2024 by Erik Pounds\\n\\n\\n\\nShare\\n\\nEmail0\\n\\nEditor’s note: The name of NIM Agent Blueprints was changed to NVIDIA Blueprints in October 2024. All references to the name have been updated in this blog.\\n\\nAI chatbots use generative AI to provide responses based on a single interaction. A person makes a query and the chatbot uses natural language processing to reply.\\n\\nThe next frontier of artificial intelligence is agentic AI, which uses sophisticated reasoning and iterative planning to autonomously solve complex, multi-step problems. And it’s set to enhance productivity and operations across industries.\\n\\nAn AI agent for customer service, for instance, could operate beyond simple question-answering. With agentic AI, it could check a user’s outstanding balance and recommend which accounts could pay it off — all while waiting for the user to make a decision so it could complete the transaction accordingly when prompted.\\n\\nAgentic AI systems ingest vast amounts of data from multiple data sources and third-party applications to independently analyze challenges, develop strategies and execute tasks. Businesses are implementing agentic AI to personalize customer service, streamline software development and even facilitate patient interactions.\\n\\nAgentic AI uses sophisticated reasoning and iterative planning to solve complex, multi-step problems.\\n\\nHow Does Agentic AI Work?\\n\\nAgentic AI uses a four-step process for problem-solving:\\n\\nPerceive: AI agents gather and process data from various sources, such as sensors, databases and digital interfaces. This involves extracting meaningful features, recognizing objects or identifying relevant entities in the environment.\\n\\nReason: A large language model acts as the orchestrator, or reasoning engine, that understands tasks, generates solutions and coordinates specialized models for specific functions like content creation, visual processing or recommendation systems. This step uses techniques like retrieval-augmented generation (RAG) to access proprietary data sources and deliver accurate, relevant outputs.\\n\\nAct: By integrating with external tools and software via application programming interfaces, agentic AI can quickly execute tasks based on the plans it has formulated. Guardrails can be built into AI agents to help ensure they execute tasks correctly. For example, a customer service AI agent may be able to process claims up to a certain amount, while claims above the amount would have to be approved by a human.\\n\\nLearn: Agentic AI continuously improves through a feedback loop, or “data flywheel,” where the data generated from its interactions is fed into the system to enhance models. This ability to adapt and become more effective over time offers businesses a powerful tool for driving better decision-making and operational efficiency.\\n\\n\\n\\nFueling Agentic AI With Enterprise Data\\n\\nAcross industries and job functions, generative AI is transforming organizations by turning vast amounts of data into actionable knowledge, helping employees work more efficiently.\\n\\nAI agents build on this potential by accessing diverse data through accelerated AI query engines, which process, store and retrieve information to enhance generative AI models. A key technique for achieving this is RAG, which allows AI to intelligently retrieve the right information from a broader range of data sources.\\n\\nOver time, AI agents learn and improve by creating a data flywheel, where data generated through interactions is fed back into the system, refining models and increasing their effectiveness.\\n\\nThe end-to-end NVIDIA AI platform, including NVIDIA NeMo microservices for developing custom generative AI applications, provides the ability to manage and access data efficiently, which is crucial for building responsive agentic AI applications.\\n\\nAgentic AI in Action\\n\\nThe potential applications of agentic AI are vast, limited only by creativity and expertise. From simple tasks like generating and distributing content to more complex use cases such as orchestrating enterprise software, AI agents are transforming industries.\\n\\n\\n\\nCustomer Service: AI agents are improving customer support by enhancing self-service capabilities and automating routine communications. Over half of service professionals report significant improvements in customer interactions, reducing response times and boosting satisfaction.\\n\\nThere’s also growing interest in digital humans — AI-powered agents that embody a company’s brand and offer lifelike, real-time interactions to help sales representatives answer customer queries or solve issues directly when call volumes are high.\\n\\nContent Creation: Agentic AI can help quickly create high-quality, personalized marketing content. Generative AI agents can save marketers an average of three hours per content piece, allowing them to focus on strategy and innovation. By streamlining content creation, businesses can stay competitive while improving customer engagement.\\n\\nSoftware Engineering: AI agents are boosting developer productivity by automating repetitive coding tasks. It’s projected that by 2030 AI could automate up to 30% of work hours, freeing developers to focus on more complex challenges and drive innovation.\\n\\nHealthcare: For doctors analyzing vast amounts of medical and patient data, AI agents can distill critical information to help them make better-informed care decisions. Automating administrative tasks and capturing clinical notes in patient appointments reduces the burden of time-consuming tasks, allowing doctors to focus on developing a doctor-patient connection.\\n\\nAI agents can also provide 24/7 support, offering information on prescribed medication usage, appointment scheduling and reminders, and more to help patients adhere to treatment plans.\\n\\nVideo analytics: Enterprises and public sector organizations around the world are developing video analytics AI agents to boost the capabilities of workforces that rely on visual information from a growing number of devices — including cameras, IoT sensors and vehicles. Video analytics AI agents can analyze large amounts of live or archived videos, request tasks via natural language and perform complex operations like video search, summarization and visual question-answering. These agents can also be used to deliver anomaly alerts, draft incident reports, improve quality control through visual inspection and enhance predictive maintenance.\\n\\nHow to Get Started\\n\\nWith its ability to plan and interact with a wide variety of tools and software, agentic AI marks the next chapter of artificial intelligence, offering the potential to enhance productivity and revolutionize the way organizations operate.\\n\\nTo accelerate the adoption of generative AI-powered applications and agents, NVIDIA Blueprints provide sample applications, reference code, sample data, tools and comprehensive documentation.\\n\\nNVIDIA partners including Accenture are helping enterprises use agentic AI with solutions built with NVIDIA Blueprints.\\n\\nVisit ai.nvidia.com to learn more about the tools and software NVIDIA offers to help enterprises build their own AI agents.\\n\\nCategories: Explainer | Generative AI\\n\\nTags: Artificial Intelligence | NVIDIA Blueprints\\n\\nSubscribe Widget\\n\\nAll NVIDIA News\\n\\n\\n\\nCisco and NVIDIA Advance Security for Enterprise AI Factories\\n\\n\\n\\nClear Skies Ahead: New NVIDIA Earth-2 Generative AI Foundation Model Simulates Global Climate at Kilometer-Scale Resolution\\n\\n\\n\\nThe Blue Lion Supercomputer Will Run on NVIDIA Vera Rubin — Here’s Why That Matters\\n\\n\\n\\nAdeola Adesoba Puts Data to Work for a Safer Future\\n\\n\\n\\nUK Prime Minister, NVIDIA CEO Set the Stage as AI Lights Up Europe'),\n",
              " Document(metadata={'source': 'https://aws.amazon.com/what-is/artificial-general-intelligence/'}, page_content=\"What is Cloud Computing?›\\n\\nCloud Computing Concepts Hub›\\n\\nArtificial Intelligence\\n\\nWhat is AGI (Artificial General Intelligence)?\\n\\nCreate an AWS Account\\n\\nWhat is artificial general intelligence? What is the difference between artificial intelligence and artificial general intelligence? What are the theoretical approaches to artificial general intelligence research? What are the technologies driving artificial general intelligence research? What are the challenges in artificial general intelligence research? How can AWS help with your AI and AGI efforts?\\n\\nWhat is artificial general intelligence?\\n\\nArtificial general intelligence (AGI) is a field of theoretical AI research that attempts to create software with human-like intelligence and the ability to self-teach. The aim is for the software to be able to perform tasks that it is not necessarily trained or developed for.\\n\\nCurrent artificial intelligence (AI) technologies all function within a set of pre-determined parameters. For example, AI models trained in image recognition and generation cannot build websites. AGI is a theoretical pursuit to develop AI systems that possess autonomous self-control, a reasonable degree of self-understanding, and the ability to learn new skills. It can solve complex problems in settings and contexts that were not taught to it at the time of its creation. AGI with human abilities remains a theoretical concept and research goal.\\n\\nWhat is the difference between artificial intelligence and artificial general intelligence?\\n\\nOver the decades, AI researchers have charted several milestones that significantly advanced machine intelligence—even to degrees that mimic human intelligence in specific tasks. For example, AI summarizers use machine learning (ML) models to extract important points from documents and generate an understandable summary. AI is thus a computer science discipline that enables software to solve novel and difficult tasks with human-level performance.\\n\\nIn contrast, an AGI system can solve problems in various domains, like a human being, without manual intervention. Instead of being limited to a specific scope, AGI can self-teach and solve problems it was never trained for. AGI is thus a theoretical representation of a complete artificial intelligence that solves complex tasks with generalized human cognitive abilities.\\n\\nSome computer scientists believe that AGI is a hypothetical computer program with human comprehension and cognitive capabilities. AI systems can learn to handle unfamiliar tasks without additional training in such theories. Alternately, AI systems that we use today require substantial training before they can handle related tasks within the same domain. For example, you must fine-tune a pre-trained large language model (LLM) with medical datasets before it can operate consistently as a medical chatbot.\\n\\nStrong AI compared with weak AI\\n\\nStrong AI is full artificial intelligence, or AGI, capable of performing tasks with human cognitive levels despite having little background knowledge. Science fiction often depicts strong AI as a thinking machine with human comprehension not confined to domain limitations.\\n\\nIn contrast, weak AI or narrow AI are AI systems limited to computing specifications, algorithms, and specific tasks they are designed for. For example, previous AI models have limited memories and only rely on real-time data to make decisions. Even emerging generative AI applications with better memory retention are considered weak AI because they cannot be repurposed for other domains.\\n\\nWhat are the theoretical approaches to artificial general intelligence research?\\n\\nAchieving AGI requires a broader spectrum of technologies, data, and interconnectivity than what powers AI models today. Creativity, perception, learning, and memory are essential to create AI that mimics complex human behavior. AI experts have proposed several methods to drive AGI research.\\n\\nSymbolic\\n\\nThe symbolic approach assumes that computer systems can develop AGI by representing human thoughts with expanding logic networks. The logic network symbolizes physical objects with an if-else logic, allowing the AI system to interpret ideas at a higher thinking level. However, symbolic representation cannot replicate subtle cognitive abilities at the lower level, such as perception.\\n\\nConnectionist\\n\\nThe connectionist (or emergentist) approach focuses on replicating the human brain structure with neural-network architecture. Brain neurons can alter their transmission paths as humans interact with external stimuli. Scientists hope AI models adopting this sub-symbolic approach can replicate human-like intelligence and demonstrate low-level cognitive capabilities. Large language models are an example of AI that uses the connectionist method to understand natural languages.\\n\\nUniversalists\\n\\nResearchers taking the universalist approach focus on addressing the AGI complexities at the calculation level. They attempt to formulate theoretical solutions that they can repurpose into practical AGI systems.\\n\\nWhole organism architecture\\n\\nThe whole organism architecture approach involves integrating AI models with a physical representation of the human body. Scientists supporting this theory believe AGI is only achievable when the system learns from physical interactions.\\n\\nHybrid\\n\\nThe hybrid approach studies symbolic and sub-symbolic methods of representing human thoughts to achieve results beyond a single approach. AI researchers may attempt to assimilate different known principles and methods to develop AGI.\\n\\nWhat are the technologies driving artificial general intelligence research?\\n\\nAGI remains a distant goal for researchers. Efforts to build AGI systems are ongoing and encouraged by emerging developments. The following sections describe emerging technologies.\\n\\nDeep learning\\n\\nDeep learning is an AI discipline that focuses on training neural networks with multiple hidden layers to extract and understand complex relationships from raw data. AI experts use deep learning to build systems capable of understanding text, audio, images, video, and other information types. For example, developers use Amazon SageMaker to build lightweight deep learning models for the Internet of Things (IoT) and mobile devices.\\n\\nGenerative AI\\n\\nGenerative artificial intelligence (generative AI) is a subset of deep learning wherein an AI system can produce unique and realistic content from learned knowledge. Generative AI models train with massive datasets, which enables them to respond to human queries with text, audio, or visuals that naturally resemble human creations. For example, LLMs from AI21 Labs, Anthropic, Cohere, and Meta are generative AI algorithms that organizations can use to solve complex tasks. Software teams use Amazon Bedrock to deploy these models quickly on the cloud without provisioning servers.\\n\\nNLP\\n\\nNatural language processing (NLP) is a branch of AI that allows computer systems to understand and generate human language. NLP systems use computational linguistics and machine learning technologies to turn language data into simple representations called tokens and understand their contextual relationship. For example, Amazon Lex is an NLP engine that allows organizations to build conversational aichatbots.\\n\\nComputer vision\\n\\nComputer vision is a technology that allows systems to extract, analyze, and comprehend spatial information from visual data. Self-driving cars use computer vision models to analyze real-time feeds from cameras and navigate the vehicle safely away from obstacles. Deep learning technologies allow computer vision systems to automate large-scale object recognition, classification, monitoring, and other image-processing tasks. For example, engineers use Amazon Rekognition to automate image analysis for various computer vision applications.\\n\\nRobotics\\n\\nRobotics is an engineering discipline wherein organizations can build mechanical systems that automatically perform physical maneuvers. In AGI, robotics systems allow machine intelligence to manifest physically. It is pivotal for introducing the sensory perception and physical manipulation capabilities that AGI systems require. For example, embedding a robotic arm with AGI may allow the arm to sense, grasp, and peel oranges as humans do. When researching AGI, engineering teams use AWS RoboMaker to simulate robotic systems virtually before assembling them.\\n\\nWhat are the challenges in artificial general intelligence research?\\n\\nComputer scientists face some of the following challenges in developing AGI.\\n\\nMake connections\\n\\nCurrent AI models are limited to their specific domain and cannot make connections between domains. However, humans can apply the knowledge and experience from one domain to another. For example, educational theories are applied in game design to create engaging learning experiences. Humans can also adapt what they learn from theoretical education to real-life situations. However, deep learning models require substantial training with specific datasets to work reliably with unfamiliar data.\\n\\nEmotional intelligence\\n\\nDeep learning models hint at the possibility of AGI, but have yet to demonstrate the authentic creativity that humans possess. Creativity requires emotional thinking, which neural network architecture can't replicate yet. For example, humans respond to a conversation based on what they sense emotionally, but NLP models generate text output based on the linguistic datasets and patterns they train on.\\n\\nSensory perception\\n\\nAGI requires AI systems to interact physically with the external environment. Besides robotics abilities, the system must perceive the world as humans do. Existing computer technologies need further advancement before they can differentiate shapes, colors, taste, smell, and sound accurately like humans.\\n\\nHow can AWS help with your AI and AGI efforts?\\n\\nAWS provides managed artificial intelligence services that help you train, deploy, and scale generative AI applications. Organizations use our AI tools and foundational models to innovate AI systems with their own data for personalized use cases.\\n\\nAmazon Bedrock is a fully-managed service wherein developers can use API calls to access generative AI models they deploy. You can select, customize, train, and deploy industry-leading foundational models on Bedrock to work with proprietary data.\\n\\nAmazon SageMaker Jumpstart helps software teams accelerate AI development by building, training, and deploying foundational models in a machine-learning hub.\\n\\nUse Amazon Elastic Compute Cloud UltraClusters to power your generative AI workloads with supercomputing GPUs to process massive datasets with low latency.\\n\\nGet started with AGI by signing up for an AWS account today\\n\\nNext Steps on AWS\\n\\nCheck out additional product-related resources\\n\\nInnovate faster with the most comprehensive set of AI services\\n\\nSign up for a free account\\n\\nInstant get access to the AWS Free Tier.\\n\\nSign up\\n\\nStart building in the console\\n\\nGet started building in the AWS management console.\\n\\nSign in\")]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccbGjXkm3gGh",
        "outputId": "c5183e50-2c60-4c9a-ab5c-12cf65b91b2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTCBqa7S3ir3"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator='\\n',\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiUbU04o32X2"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5hDteBt38dm",
        "outputId": "40e4aea2-e610-4cfb-a163-7242a7869a8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZZm0VSe5UIZ",
        "outputId": "2d20d7fc-a720-4e27-d580-13ec5307c79a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://medium.com/@onkarmishra/stable-diffusion-explained-1f101284484d'}, page_content='Sitemap\\nSign in\\nWrite\\nSign in\\nStable Diffusion Explained\\nHow does Stable diffusion work? Explaining the tech behind text to image generation.\\nOnkar Mishra\\nOnkar Mishra\\n6 min read\\nJun 8, 2023\\n--\\nLarge text to image models have achieved remarkable success in enabling high quality synthesis of images from text prompts. Diffusion models can be applied to text to image generation tasks to achieve state of art image generating results.\\nStable Diffusion model has achieved state of the art results for image generation. Stable Diffusion is based on a particular type of diffusion model called Latent Diffusion model, proposed in High-Resolution Image Synthesis with Latent Diffusion Models and created by the researchers and engineers from CompVis, LMU and RunwayML. The model was initially trained on 512x512 images from a subset of the LAION-5B database.')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKhs4JWa5WzN",
        "outputId": "1af66096-a7de-4dd7-87cd-b73b98807de1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-2497576997>:1: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embeddings = OpenAIEmbeddings()\n"
          ]
        }
      ],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mb5M-6QQ5b7i",
        "outputId": "9919c852-7176-4162-b22d-fde46322930e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_result = embeddings.embed_query(\"Hello World\")\n",
        "len(\"query_result\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXLyHPWQ5kVA"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(text_chunks, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L2BZTj46MII",
        "outputId": "04edca84-14c0-47af-89d1-ef84ff0c4a93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-44520985>:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI()\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBeXXdIB6OLU"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorstore.as_retriever())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb5qNfYF6og0",
        "outputId": "982cb375-b1d3-48bf-a775-09435df51782"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-24-2923946682>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain({\"question\": \"What are the three main components in latent diffusion?\"}, return_only_outputs=True)\n"
          ]
        }
      ],
      "source": [
        "result = chain({\"question\": \"What are the three main components in latent diffusion?\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "c75tvrrP9FIe",
        "outputId": "be78a3a9-def4-4f8b-f379-4527830c6e96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The three main components in latent diffusion are an autoencoder (VAE), a U-Net, and a text-encoder such as CLIP's Text Encoder.\\n\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-AQuc5g9HSD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
