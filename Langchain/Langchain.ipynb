{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc02e491-Hee",
        "outputId": "dd6500ed-b21f-48e5-cb72-39827543d1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.24 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8C0InPm-dP_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHwSxefmIdBi"
      },
      "source": [
        "## OPENAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vKPDgmR_OXF",
        "outputId": "86704b2e-776c-4d9e-a3fa-0bf4a125269c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5XOkRI7I_vT"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "# llm = OpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-GLiIZcJb7z",
        "outputId": "fccb29d0-41cc-4751-8e3a-97813fc86029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "The biological name of humans is Homo sapiens.\n"
          ]
        }
      ],
      "source": [
        "text = \"what is the biological name of humans\"\n",
        "print(llm.predict(text))\n",
        "# Or print(llm(text))\n",
        "# Or print(llm.invoke(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANAYtnoJKkcX"
      },
      "source": [
        "## HUGGING FACE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZK0uDhcJ7Y7",
        "outputId": "0cfc62e4-7751-42b6-c606-4f0d7a5560e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg-vB2uVMyOI",
        "outputId": "3dc1a242-63c8-4874-d338-827d47d56088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.63)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.44)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain-community huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNWREVMONIi4",
        "outputId": "95f8cad8-dcdc-4803-9f7f-a65dd66cce64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.3.25\n",
            "0.32.4\n"
          ]
        }
      ],
      "source": [
        "import langchain, huggingface_hub\n",
        "print(langchain.__version__)\n",
        "print(huggingface_hub.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUbtuWZLNHh"
      },
      "outputs": [],
      "source": [
        "# from langchain_community.llms import HuggingFaceHub\n",
        "# # model = google/flan-t5-xl\n",
        "# llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs={\"temperature\": 0, \"max_length\":64})\n",
        "# response = llm.invoke(\"Translate english to latin: Latin is no more\")\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417,
          "referenced_widgets": [
            "dbe60647d9d24540ae3e1827d23734b0",
            "e6f67208f1474855b8e1ac002da5738a",
            "8fa2512c32984ed3962c2351919f5acd",
            "77634e4689d24f89bd5064d2bf03fadb",
            "e31adf44110c4103902fca4882200db7",
            "8d951deb5a174f30be5f35863059a09d",
            "50eeec232a86464eae48c1a7c4e3b33d",
            "d643e1a4e1c84ed7bd73602916a6b875",
            "43d1f96e56184f32bfc8ee36d4e95911",
            "e385f4d6e7784b14b3320d464255a6e7",
            "206afcbb1d36451b96478bda2e500e2f",
            "118abefd78c246169876fde4e4814221",
            "0638a87e7b3743328563d1eb52af5712",
            "fc92b0adf49b4aee84b149ad76f0d68b",
            "d1d28b001a7e41e9a42d451a586b2a3e",
            "2e84e477d99e466f8b7e63c8b994a041",
            "6068ddea0bc04c2ab09c7d29a44d1fab",
            "64e53872c83d414793dc478a44ab6f3c",
            "3371c281b55848649d15ac9fe52ae998",
            "c6289bb49de14d5ab4ae07f0243ba136",
            "3029b3d323ed49a2b1512a82cfefc466",
            "673aee1a3c614cf9bc6c876f4938bc23",
            "92cbf25314604ccd90a8b09d2e1f35f9",
            "92e7866e54de4761bf9bf5fe034cf4e9",
            "ea0bb33e4fe44457a16bbdf49405616a",
            "4fbe617a082e45388c44d45697d12579",
            "e9ca24322a57475da4e1d4c244444a51",
            "87ad1fdf0eba4f24bcb38ce8c457c377",
            "bde63f1e63a14d18b48eca078dbe696d",
            "a0921a5dfd684aeaa84daa502ac28a76",
            "54f46301af2641a68691b8812ed7b8e0",
            "1f7e9fe4d2264b19a37ba899b116b3d5",
            "d5dfa3e2c38e4cc3b5816ec682e76977",
            "518c51a405384d10b48ea7e4454b2c91",
            "4a2a63b055644f1a88acd46783952802",
            "a3f098680ce9429f93314856e6dd3d3f",
            "76241d8acf2d41d1a2774593c75807b1",
            "a86ff14f78c6455aa419a2f2cc1f2011",
            "5e3e03972f3f4933bb932bb5cee5133c",
            "e180f765788344809fc1284d60bd928e",
            "3fc5f3c0cc6b4bbc8b08ba494f826a08",
            "282b6378f4db4f35be6621e43e4ac3d3",
            "37853abba1cd4a94b6c09aa96b6aaf71",
            "d3cd418b597d4b8a8e57d12e73935d78",
            "6b54e8b3d08645529bffa73b3d8b39f0",
            "8318e557983f4fca8983a8e02631b929",
            "6453810b3d5b48e7a058057c94b0ee19",
            "61413db9244c4a9baece80f7e24ffdc3",
            "3ea1db3538334cfe84414363908691bd",
            "db613ad4b4f444c1a62137818e3e52d8",
            "de217d1d23b24a538f806f57441ed341",
            "8ae245792670444d867e77fb144c36d4",
            "184a6b0567ac4eadb614d643811e5f98",
            "1d5f01969dfc43bdae8544e0ff396082",
            "6e896e3e635849ee9a47a7b3d7c0870f",
            "8593adf046b14bc2b219b4b18fc9616a",
            "48e7e7fac3d142fbab02ceaa2464203d",
            "7e4b3b75960c4c299e1fbefdc5b41901",
            "bd104e6d00154bf1904fd622ce65136b",
            "71a9d6bab0af4ecfbc4cf98bfbb486d3",
            "876f26337f5a46659d449c79906e3248",
            "762216e1d5a94d25910c8d1fecf4e061",
            "f49691a0a1b84aa6b07bbfefa1afe672",
            "2e5c07962ab24b238ffc324fcd5eb90b",
            "f2ca71aafa58424f839bcadd25894951",
            "6988f36e49104366a2987e9f2e143cd9",
            "33e84737bdde416d990cd1fc4328b559",
            "0f162eb39510481f8804b88750475ff7",
            "f18e4dd72b314acfa93d4361efbdf6c8",
            "08f501eb518b4bd3bd8bf79b8ad7a851",
            "317a88e54b734a8fa4a923c24f7b3728",
            "b907cf86c13a46a6af412667d09097be",
            "8142034b8c964695bb9154bf7ad10ae9",
            "682b34503dcc40c48b51104e1baa289f",
            "3b1a44fafca5443abd70435ddf2fed32",
            "1d687994025f464eb9579bfe07323e46",
            "d1561b44205f4412a6a7a5d1c7c12f3f"
          ]
        },
        "id": "hwxUsojoMCdi",
        "outputId": "7065393d-7c97-476b-bbc0-05d4d7553bff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbe60647d9d24540ae3e1827d23734b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "118abefd78c246169876fde4e4814221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92cbf25314604ccd90a8b09d2e1f35f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "518c51a405384d10b48ea7e4454b2c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b54e8b3d08645529bffa73b3d8b39f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8593adf046b14bc2b219b4b18fc9616a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33e84737bdde416d990cd1fc4328b559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=64) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latin is no more\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
        "\n",
        "result = translator(\"Translate English to Latin: Latin is no more\", max_length=64, do_sample=False)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n057ETXbOVrO"
      },
      "source": [
        "## PROMPT TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSoKcPibNuKZ",
        "outputId": "f4f035bb-92cb-4caf-d133-e7a4363b7542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I want to opena restaurent for indian food. Suggest a fancy name for this restaurent.\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = \"['cuisine']\",\n",
        "    template = \"I want to opena restaurent for {cuisine} food. Suggest a fancy name for this restaurent.\"\n",
        ")\n",
        "\n",
        "p = prompt_template_name.format(cuisine='indian')\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTV3uFnTdlNK"
      },
      "source": [
        "## CHAINS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn3-BtsudUM6"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "eM0jHsIBdyLr",
        "outputId": "d5d12d78-3b65-4692-cbb6-713da0b36ca7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What is a good name for a company that makes automobiles'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "prompt.format(product=\"automobiles\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J88qAo0teU5w",
        "outputId": "e2ca963e-9db1-4f63-f225-98039e0c7de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Roadmaster Motors\" or \"DriveTech Automotive\" or \"Velocity Vehicles\" or \"AutoCraft Co.\" or \"Revolution Motors\" or \"Momentum Motors\" or \"Innovative AutoWorks\" or \"Precision Motors\" or \"Cruiser Cars\" or \"Empire Auto Group\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "response = chain.run(\"automobiles\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUDRb8KifT7z"
      },
      "source": [
        "### Simple Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7Qyxyf-el-8"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = \"['cuisine']\",\n",
        "    template = \"I want to opena restaurent for {cuisine} food. Suggest a fancy name for this restaurent.\"\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = \"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzQZXLD_gP2F",
        "outputId": "6dec6661-fba7-4aa9-c17c-1ad73b2d9e97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Tandoori Chicken: Marinated chicken pieces cooked in a clay oven and served with a side of spicy mint chutney.\n",
            "2. Butter Chicken: Tender chicken pieces cooked in a creamy tomato and butter sauce, served with basmati rice.\n",
            "3. Vegetable Samosas: Crispy pastry filled with a savory mix of potatoes, peas, and spices.\n",
            "4. Lamb Vindaloo: Spicy and tangy curry made with tender chunks of lamb, potatoes, and a blend of spices.\n",
            "5. Palak Paneer: Homemade cottage cheese cubes cooked in a creamy spinach sauce.\n",
            "6. Garlic Naan: Traditional Indian flatbread topped with garlic and cooked in a tandoor oven.\n",
            "7. Chana Masala: Chickpeas cooked in a flavorful blend of spices and served with rice or naan.\n",
            "8. Chicken Biryani: Fragrant basmati rice cooked with chicken, aromatic spices, and herbs.\n",
            "9. Daal Makhani: Slow-cooked black lentils with butter, cream, and spices.\n",
            "10. Mango Lassi: A refreshing yogurt-based drink with mango and a touch of cardamom.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
        "\n",
        "content = chain.run(\"indian\")\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad39l0EGjXK-"
      },
      "source": [
        "### Sequential Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AZoIybrgxt4"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = \"['cuisine']\",\n",
        "    template = \"I want to opena restaurent for {cuisine} food. Suggest a fancy name for this restaurent.\"\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key = \"restaurant_name\")\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = \"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
        ")\n",
        "\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key = \"menu_items\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHbj3Lz8jljt"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains = [name_chain, food_items_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name', 'menu_items']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is7PjbrPkVmJ",
        "outputId": "033ef625-e78f-4793-d7ea-6e299492a56f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'cuisine': 'indian', 'restaurant_name': \"\\n\\n1. Maharaja's Palace\\n2. Spice Kingdom\\n3. Tandoori Nights\\n4. Curry House\\n5. Masala Mansion\\n6. Bollywood Bites\\n7. Saffron Delight\\n8. Flavors of India\\n9. The Great Indian Feast\\n10. Naan & Curry\\n11. Haveli Heights\\n12. Chai & Chaat\\n13. Garam Masala Grill\\n14. Namaste India\\n15. The Royal Thali\\n16. Desi Dhaba\\n17. Mogul's Kitchen\\n18. Chaat Corner\\n19. The Tandoor Experience\\n20. Taj Mahal Tastes\", 'menu_items': \"\\n\\n1. Maharaja's Palace:\\n- Tandoori Chicken\\n- Butter Chicken\\n- Lamb Rogan Josh\\n- Vegetable Biryani\\n- Palak Paneer\\n- Naan Bread\\n- Mango Lassi\\n- Gulab Jamun\\n- Chicken Tikka Masala\\n- Samosas\\n- Vegetable Korma\\n\\n2. Spice Kingdom:\\n- Chicken Vindaloo\\n- Lamb Madras\\n- Vegetable Jalfrezi\\n- Prawn Biryani\\n- Aloo Gobi\\n- Garlic Naan\\n- Mango Chutney\\n- Raita\\n- Malai Kofta\\n- Onion Bhaji\\n- Chicken Tikka\\n\\n3. Tandoori Nights:\\n- Tandoori Fish Tikka\\n- Tandoori Shrimp\\n- Tandoori Chicken Tikka\\n- Chicken Tikka Masala\\n- Lamb Seekh Kebab\\n- Vegetable Samosas\\n- Garlic Naan\\n- Chana Masala\\n- Chicken Biryani\\n- Mango Lassi\\n- Kheer (rice pudding)\\n\\n4. Curry House:\\n- Chicken Curry\\n- Beef Vindaloo\\n- Vegetable Masala\\n- Lamb Korma\\n- Vegetable Biryani\\n- Onion Bhaji\"}\n"
          ]
        }
      ],
      "source": [
        "print(chain({\"cuisine\": \"indian\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l59qqHFb2LgE"
      },
      "source": [
        "## AGENTS AND TOOLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlA4lNUJkmtH",
        "outputId": "6ed885d4-c289-4ce4-880f-4c6911535edd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.14.0)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=296b5a8b5d634a068d3f7dff140e2175412cd22c317a61f024c88f1f4ee3410a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDxojigX2S36"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne55Vjk22hv1"
      },
      "outputs": [],
      "source": [
        "llms = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u-NQiGWW27yL",
        "outputId": "05728a6e-bfb2-4be1-d75d-efdbd19427bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the answer\n",
            "Action: wikipedia\n",
            "Action Input: \"GDP of India 2025\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: List of Indian metropolitan areas by GDP\n",
            "Summary: The following is a list of metropolitan areas in India by their  nominal gross domestic product (GDP) and their contribution to their respective states and union territories. The metropolitan area definition is based on the work by the Joint Research Center of the European Union, and are based on the satellite images of the built environment of the urban areas. It may include adjoining administrative units of the urban agglomeration. The GDP of India as of 2022-23 is ₹269.5 trillion (US$3.43 trillion), and the top 50 cities contribute to nearly 42 percent of the national GDP.\n",
            "\n",
            "Page: Economy of India\n",
            "Summary: The economy of India is a developing mixed economy with a notable public sector in strategic sectors. It is the world's fourth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP); on a per capita income basis, India ranked 136th by GDP (nominal) and 119th by GDP (PPP). From independence in 1947 until 1991, successive governments followed the Soviet model and promoted protectionist economic policies, with extensive Sovietization, state intervention, demand-side economics, natural resources, bureaucrat-driven enterprises and economic regulation. This is characterised as dirigism, in the form of the Licence Raj. The end of the Cold War and an acute balance of payments crisis in 1991 led to the adoption of a broad economic liberalisation in India and indicative planning. India has about 1,900 public sector companies, with the Indian state having complete control and ownership of railways and highways. The Indian government has major control over banking, insurance, farming, fertilizers and chemicals, airports,  essential utilities. The state also exerts substantial control over digitalization,  telecommunication, supercomputing, space, port and shipping industries, which were effectively nationalised in the mid-1950s but has seen the emergence of key corporate players.\n",
            "Nearly 70% of India's GDP is driven by domestic consumption; the country remains the world's fourth-largest consumer market. Aside private consumption, India's GDP is also fueled by government spending, investments, and exports. In 2022, India was the world's 10th-largest importer and the 8th-largest exporter. India has been a member of the World Trade Organization since 1 January 1995. It ranks 63rd on the ease of doing business index and 40th on the Global Competitiveness Index. India has one of the world's highest number of billionaires along with extreme income inequality. Economists and social scientists often consider India a welfare state. India's overall social welfare spending stood at 8.6% of GDP in 2021-22, which is much lower than the average for OECD nations. With 586 million workers, the Indian labour force is the world's second-largest. Despite having one of the longest working hours, India has one of the lowest workforce productivity levels in the world. Economists say that due to structural economic problems, India is experiencing jobless economic growth.\n",
            "During the Great Recession, the economy faced a mild slowdown. India endorsed Keynesian policy and initiated stimulus measures (both fiscal and monetary) to boost growth and generate demand. In subsequent years, economic growth revived.\n",
            "In 2021–22, the foreign direct investment (FDI) in India was $82 billion. The leading sectors for FDI inflows were the Finance, Banking, Insurance and R&D. India has free trade agreements with several nations and blocs, including ASEAN, SAFTA, Mercosur, South Korea, Japan, Australia, the United Arab Emirates, and several others which are in effect or under negotiating stage.\n",
            "The service sector makes up more than 50% of GDP and remains the fastest growing sector, while the industrial sector and the agricultural sector employs a majority of the labor force. The Bombay Stock Exchange and National Stock Exchange are some of the world's largest stock exchanges by market capitali\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This information is helpful, but I need to narrow it down to just the GDP of India in 2025\n",
            "Action: wikipedia\n",
            "Action Input: \"GDP of India in 2025\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Economy of India\n",
            "Summary: The economy of India is a developing mixed economy with a notable public sector in strategic sectors. It is the world's fourth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP); on a per capita income basis, India ranked 136th by GDP (nominal) and 119th by GDP (PPP). From independence in 1947 until 1991, successive governments followed the Soviet model and promoted protectionist economic policies, with extensive Sovietization, state intervention, demand-side economics, natural resources, bureaucrat-driven enterprises and economic regulation. This is characterised as dirigism, in the form of the Licence Raj. The end of the Cold War and an acute balance of payments crisis in 1991 led to the adoption of a broad economic liberalisation in India and indicative planning. India has about 1,900 public sector companies, with the Indian state having complete control and ownership of railways and highways. The Indian government has major control over banking, insurance, farming, fertilizers and chemicals, airports,  essential utilities. The state also exerts substantial control over digitalization,  telecommunication, supercomputing, space, port and shipping industries, which were effectively nationalised in the mid-1950s but has seen the emergence of key corporate players.\n",
            "Nearly 70% of India's GDP is driven by domestic consumption; the country remains the world's fourth-largest consumer market. Aside private consumption, India's GDP is also fueled by government spending, investments, and exports. In 2022, India was the world's 10th-largest importer and the 8th-largest exporter. India has been a member of the World Trade Organization since 1 January 1995. It ranks 63rd on the ease of doing business index and 40th on the Global Competitiveness Index. India has one of the world's highest number of billionaires along with extreme income inequality. Economists and social scientists often consider India a welfare state. India's overall social welfare spending stood at 8.6% of GDP in 2021-22, which is much lower than the average for OECD nations. With 586 million workers, the Indian labour force is the world's second-largest. Despite having one of the longest working hours, India has one of the lowest workforce productivity levels in the world. Economists say that due to structural economic problems, India is experiencing jobless economic growth.\n",
            "During the Great Recession, the economy faced a mild slowdown. India endorsed Keynesian policy and initiated stimulus measures (both fiscal and monetary) to boost growth and generate demand. In subsequent years, economic growth revived.\n",
            "In 2021–22, the foreign direct investment (FDI) in India was $82 billion. The leading sectors for FDI inflows were the Finance, Banking, Insurance and R&D. India has free trade agreements with several nations and blocs, including ASEAN, SAFTA, Mercosur, South Korea, Japan, Australia, the United Arab Emirates, and several others which are in effect or under negotiating stage.\n",
            "The service sector makes up more than 50% of GDP and remains the fastest growing sector, while the industrial sector and the agricultural sector employs a majority of the labor force. The Bombay Stock Exchange and National Stock Exchange are some of the world's largest stock exchanges by market capitalisation. India is the world's sixth-largest manufacturer, representing 2.6% of global manufacturing output. Nearly 65% of India's population is rural, and contributes about 50% of India's GDP. India faces high unemployment, rising income inequality, and a drop in aggregate demand. India's gross domestic savings rate stood at 29.3% of GDP in 2022.\n",
            "\n",
            "Page: List of Indian metropolitan areas by GDP\n",
            "Summary: The following is a list of metropolitan areas in India by their  nominal gross domestic product (GDP) and their contribution to their respective states and union territories. The metropolitan area definition is based on the work by the\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m This is still too broad, I need to use a calculator to find the GDP of India in 2025\n",
            "Action: Calculator\n",
            "Action Input: \"GDP of India in 2025\"\u001b[0m"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "LLMMathChain._evaluate(\"\nGDP_2025 = GDP_2020 * (1 + growth_rate)**(2025 - 2020)\n\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSyntaxError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m    206\u001b[0m             output = str(\n\u001b[0;32m--> 207\u001b[0;31m                 numexpr.evaluate(\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \u001b[0mexpression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, sanitize, _frame_depth, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(ex, local_dict, global_dict, out, order, casting, _frame_depth, sanitize, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpr_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_names_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0m_names_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpr_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetExprNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_uses_vml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_names_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpr_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mgetExprNames\u001b[0;34m(text, context, sanitize)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetExprNames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringToExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanitize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressionToAST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numexpr/necompiler.py\u001b[0m in \u001b[0;36mstringToExpression\u001b[0;34m(s, types, context, sanitize)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<expr>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# make VariableNode's for the names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSyntaxError\u001b[0m: invalid syntax (<expr>, line 1)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-91fc1a855d72>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the GDP of India in 2025\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    384\u001b[0m         }\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;31m# We now enter the agent loop (until it returns something).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_continue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             next_step_output = self._take_next_step(\n\u001b[0m\u001b[1;32m   1621\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     ) -> Union[AgentFinish, list[tuple[AgentAction, str]]]:\n\u001b[1;32m   1325\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1326\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     ) -> Union[AgentFinish, list[tuple[AgentAction, str]]]:\n\u001b[1;32m   1325\u001b[0m         return self._consume_next_step(\n\u001b[0;32m-> 1326\u001b[0;31m             [\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                 for a in self._iter_next_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_action\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m             yield self._perform_agent_action(\n\u001b[0m\u001b[1;32m   1412\u001b[0m                 \u001b[0mname_to_tool_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36m_perform_agent_action\u001b[0;34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[0m\n\u001b[1;32m   1431\u001b[0m                 \u001b[0mtool_run_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llm_prefix\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0;31m# We then call the tool on the tool input to get an observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             observation = tool.run(\n\u001b[0m\u001b[1;32m   1434\u001b[0m                 \u001b[0magent_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtool_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_to_raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtool_call_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_tool_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mconfig_param\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_get_runnable_config_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                     \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtool_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mconfig_param\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtool_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtool_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"content_and_artifact\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/tools/simple.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, config, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconfig_param\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_get_runnable_config_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Tool does not support sync invocation.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mawarning_emitting_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    384\u001b[0m         }\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         return self.invoke(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRunnableConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             outputs = (\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         )\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_llm_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_run_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     async def _acall(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_process_llm_result\u001b[0;34m(self, llm_output, run_manager)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_match\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mexpression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_expression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAnswer: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"yellow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm_math/base.py\u001b[0m in \u001b[0;36m_evaluate_expression\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m    212\u001b[0m             )\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0;34mf'LLMMathChain._evaluate(\"{expression}\") raised error: {e}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;34m\" Please try again with a valid numerical expression\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: LLMMathChain._evaluate(\"\nGDP_2025 = GDP_2020 * (1 + growth_rate)**(2025 - 2020)\n\") raised error: invalid syntax (<expr>, line 1). Please try again with a valid numerical expression"
          ]
        }
      ],
      "source": [
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "# llm-math for any kinda math related questions\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent.run(\"What is the GDP of India in 2025\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZnupB964mIJ"
      },
      "source": [
        "## MEMORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiFJYfOr3026"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP9PgQtL4uPD"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for the restaurant\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTg1HVu95LhT",
        "outputId": "e36cb20c-9401-41e2-838e-47d63cec42e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"El Maravilloso Cantina\" \n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "name = chain.run('Mexican')\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvEhOA6K5aIT",
        "outputId": "eca2167b-3693-4daa-ba93-fd00063998f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"La Dolce Vita Ristorante\"\n"
          ]
        }
      ],
      "source": [
        "name = chain.run('Italian')\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W8qAikF5hI3"
      },
      "outputs": [],
      "source": [
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chKAYF785j9Q",
        "outputId": "de515dc5-11ba-4f98-c754-87d412e58519"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX4tZl_Q5nhl"
      },
      "source": [
        "### Conversation Buffer Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQI0WXYO5lyx",
        "outputId": "06bc8ec7-add4-41a8-ded0-1318e0d3ea49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Los Sabores de México\" (The Flavors of Mexico)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run('Mexican')\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYP3r9J66F4F",
        "outputId": "0229ec42-aa5b-48eb-c15b-1e8824d2b6da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Al-Ajami Palace\" \n"
          ]
        }
      ],
      "source": [
        "name = chain.run('Arabic')\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ctXVmGqK6aTk",
        "outputId": "f2261d2c-f8ce-4735-e502-4a42d920429f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Human: Mexican\\nAI: \\n\\n\"Los Sabores de México\" (The Flavors of Mexico)\\nHuman: Arabic\\nAI: \\n\\n\"Al-Ajami Palace\" '"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.memory.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "LUUEDez96dTQ",
        "outputId": "48b39e22-60c7-4ea7-af18-4a4cd3538915"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain.memory.buffer.ConversationBufferMemory</b><br/>def __init__(*args: Any, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain/memory/buffer.py</a>.. deprecated:: 0.3.1 Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/ It will not be removed until langchain==1.0.0.\n",
              "\n",
              "A basic memory implementation that simply stores the conversation history.\n",
              "\n",
              "This stores the entire conversation history in memory without any\n",
              "additional processing.\n",
              "\n",
              "Note that additional processing may be required in some situations when the\n",
              "conversation history is too large to fit in the context window of the model.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 11);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "langchain.memory.buffer.ConversationBufferMemory"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpV8mIbZ66Jq"
      },
      "source": [
        "### Conversation Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAgEtzpT6fuV",
        "outputId": "5dc32f22-ddfe-4fb3-f36b-1e15eb70a9da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "SWB6VvoU7MU3",
        "outputId": "2c3394cc-d6a9-4f6c-ef62-9089ab59e1f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The first FIFA World Cup was held in 1930 in Uruguay. The host country, Uruguay, won the tournament by defeating Argentina 4-2 in the final. The first ever goal in World Cup history was scored by Lucien Laurent of France in their match against Mexico.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Who won the first world cup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "ao7t8LuTCrZz",
        "outputId": "5d5fc274-43be-4d17-e6ac-817732720489"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The popularly known \"God of Cricket\" is a title given to the Indian cricketer Sachin Tendulkar. He is considered one of the greatest batsmen in the history of cricket and holds numerous records, including the most runs scored in both Test and One Day International matches. He was also the first player to score 100 international centuries.'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"How is the popularly known as God of Cricket\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KyiNOtCIDQlb",
        "outputId": "baef0d32-2ec1-493c-b958-98ffca341ce7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The teams that clashed in the final of the first World Cup were Uruguay and Argentina.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Which teams clashedn in the final?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZxZgAcXDeuT",
        "outputId": "ddcac70c-8fe0-4f0f-eece-7de9891395e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Who won the first world cup\n",
            "AI:  The first FIFA World Cup was held in 1930 in Uruguay. The host country, Uruguay, won the tournament by defeating Argentina 4-2 in the final. The first ever goal in World Cup history was scored by Lucien Laurent of France in their match against Mexico.\n",
            "Human: How is the popularly known as God of Cricket\n",
            "AI:  The popularly known \"God of Cricket\" is a title given to the Indian cricketer Sachin Tendulkar. He is considered one of the greatest batsmen in the history of cricket and holds numerous records, including the most runs scored in both Test and One Day International matches. He was also the first player to score 100 international centuries.\n",
            "Human: Which teams clashedn in the final?\n",
            "AI:  The teams that clashed in the final of the first World Cup were Uruguay and Argentina.\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3fsff7CDx9R"
      },
      "source": [
        "### Conversation Buffer Window Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "TbrbB9C6DmZW",
        "outputId": "e71c0002-8056-4700-e821-57c8b262801b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The first cricket World Cup was held in 1975 and was won by the West Indies team. The final match was played between West Indies and Australia, with West Indies winning by 17 runs. The tournament was hosted by England and was a 60-over per side competition. The man of the match for the final was Clive Lloyd from the West Indies team.'"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2) #remember only one conversation\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory = memory\n",
        ")\n",
        "convo.run(\"Who won the first cricket World Cup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BgVvkEJ2EYss",
        "outputId": "a76d93e5-ec92-4cbf-c7da-bc03250c9c8f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The answer to 5+5 is 10.'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"What is 5+5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "f1md0PKzFtDT",
        "outputId": "336c02a3-e556-4904-c3e5-118594db2903"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Rome was built by the ancient Romans, specifically Romulus and Remus according to legend. However, historians believe that the city was actually founded by various tribes and groups of people who settled in the area.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Rome was built by whom\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tIIK5wBjEjDW",
        "outputId": "1a59d4e1-da8a-46c8-a361-de3171637053"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' I am not sure which specific teams you are referring to. Could you please provide more context or information so I can accurately answer your question?'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Name the captains of both the teams\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djFKfVd0Ev3w",
        "outputId": "b7053f6a-c72e-4d00-8be7-4b815a75cd78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Rome was built by whom\n",
            "AI:  Rome was built by the ancient Romans, specifically Romulus and Remus according to legend. However, historians believe that the city was actually founded by various tribes and groups of people who settled in the area.\n",
            "Human: Name the captains of both the teams\n",
            "AI:  I am not sure which specific teams you are referring to. Could you please provide more context or information so I can accurately answer your question?\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5owJqXvBG37Y"
      },
      "source": [
        "## DOCUMENT LOADERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nerRuS4-FoTl",
        "outputId": "a40ed6a9-1ba5-4349-c5fa-1854146813df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.6.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Downloading pypdf-5.6.0-py3-none-any.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.2/304.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0u1whhnmYgKG"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/s41592-024-02318-2.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDH-j0fBZhIz",
        "outputId": "53681e83-6572-4788-ea1c-8408618a6258"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 0, 'page_label': '1329'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1329\\nnature methods\\nhttps://doi.org/10.1038/s41592-024-02318-2\\nArticle\\nKeypoint-MoSeq: parsing behavior by \\nlinking point tracking to pose dynamics\\nCaleb Weinreb1, Jonah E. Pearl\\u2009  \\u20091, Sherry Lin\\u2009  \\u20091, \\nMohammed Abdal Monium Osman\\u2009  \\u20091, Libby Zhang\\u2009  \\u20092,3, \\nSidharth Annapragada1, Eli Conlin1, Red Hoffmann1, Sofia Makowska1, \\nWinthrop F. Gillis1, Maya Jay1, Shaokai Ye\\u2009  \\u20094, Alexander Mathis\\u2009  \\u20094, \\nMackenzie W. Mathis\\u2009  \\u20094, Talmo Pereira\\u2009  \\u20095, Scott W. Linderman\\u2009  \\u20093,6  & \\nSandeep Robert Datta\\u2009  \\u20091 \\nKeypoint tracking algorithms can flexibly quantify animal movement \\nfrom videos obtained in a wide variety of settings. However, it remains \\nunclear how to parse continuous keypoint data into discrete actions. This \\nchallenge is particularly acute because keypoint data are susceptible \\nto high-frequency jitter that clustering algorithms can mistake for \\ntransitions between actions. Here we present keypoint-MoSeq, a machine \\nlearning-based platform for identifying behavioral modules (‘syllables’) \\nfrom keypoint data without human supervision. Keypoint-MoSeq uses a \\ngenerative model to distinguish keypoint noise from behavior, enabling it \\nto identify syllables whose boundaries correspond to natural sub-second \\ndiscontinuities in pose dynamics. Keypoint-MoSeq outperforms commonly \\nused alternative clustering methods at identifying these transitions, \\nat capturing correlations between neural activity and behavior and at \\nclassifying either solitary or social behaviors in accordance with human \\nannotations. Keypoint-MoSeq also works in multiple species and generalizes \\nbeyond the syllable timescale, identifying fast sniff-aligned movements in \\nmice and a spectrum of oscillatory behaviors in fruit flies. Keypoint-MoSeq, \\ntherefore, renders accessible the modular structure of behavior through \\nstandard video recordings.\\nWork from ethology demonstrates that behavior—a chain of actions \\ntraced by the body’s movement over time—is both continuous and \\ndiscrete1–3. The rapid advance of keypoint tracking methods (includ-\\ning SLEAP4, DeepLabCut5 and others6,7) has given researchers broad \\naccess to the continuous dynamics that underlie animal behavior 8. \\nBut parsing these dynamics into chains of discrete actions remains \\nan open problem9–11. While several action segmentation approaches \\nexist12–17, their underlying logic and assumptions differ, with different \\nmethods often giving distinct descriptions of the same behavior13,15. \\nAn important gap, therefore, exists between our access to movement \\nkinematics and our ability to understand their underlying structure.\\nOne method for parsing behavior in mice is Motion Sequenc -\\ning (MoSeq) 16,18–21. MoSeq uses unsupervised machine learning to \\ntransform its inputs—which are not keypoints, but three-dimensional \\n(3D) depth videos—into a set of behavioral motifs (like rears, turns \\nand pauses) called syllables. T o identify syllables, MoSeq searches for \\nReceived: 5 April 2023\\nAccepted: 22 May 2024\\nPublished online: 12 July 2024\\n Check for updates\\n1Department of Neurobiology, Harvard Medical School, Boston, MA, USA. 2Department of Electrical Engineering, Stanford University, Stanford, CA, USA. \\n3Wu Tsai Neurosciences Institute, Stanford University, Stanford, CA, USA. 4Brain Mind and Neuro-X Institute, School of Life Sciences, Ecole Polytechnique \\nFédérale de Lausanne (EPFL), Lausanne, Switzerland. 5Salk Institute for Biological Studies, La Jolla, CA, USA. 6Department of Statistics, Stanford \\nUniversity, Stanford, CA, USA. \\u2009e-mail: scott.linderman@stanford.edu; srdatta@hms.harvard.edu'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 1, 'page_label': '1330'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339 1330\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\neven missing data) and the behavioral syllables they represent. We \\nvalidate this model, called keypoint-MoSeq, using accelerometry meas-\\nurements, neural activity recordings and supervised behavior labels \\nfrom expert observers, and show that it generalizes beyond mouse \\nsyllables to capture behaviors at multiple timescales and in several \\nspecies. Because keypoint tracking can be applied in diverse settings \\n(including natural environments), requires no specialized hardware \\nand affords direct control over which body parts to track and at what \\nresolution, we anticipate that keypoint-MoSeq will serve as a general \\ntool for parsing the structure of behavior. T o facilitate broad adoption, \\nwe have directly integrated keypoint-MoSeq with widely used tracking \\nmethods (including SLEAP and DeepLabCut) and made the code freely \\naccessible for academic users at http://www.moseq4all.org/.\\nResults\\nMouse syllables are evident in depth-based video recordings as dis-\\ncontinuities of movement that reoccur with sub-second cadence16. T o \\ntest if the same sub-second structure is present in keypoint data, we \\nrecorded conventional videos of mice exploring an open field arena \\nand used a neural network to track eight keypoints (two ears and six \\npoints along the dorsal midline). We also captured simultaneous depth \\nvideos for comparison to depth-based MoSeq (Fig. 1a).\\nSimilar sub-second discontinuities appeared in both the depth \\nand keypoint data, with a keypoint-based change score (total velocity \\nof keypoints after egocentric alignment) spiking at the transitions \\ndiscontinuities in behavioral data at a timescale that is set by the user; \\nthis timescale is specified through a ‘stickiness’ hyperparameter that \\ninfluences the frequency with which syllables can transition. In the \\nmouse, where MoSeq has been extensively applied, pervasive dis -\\ncontinuities at the sub-second-to-second timescale mark boundaries \\nbetween syllables, and the stickiness hyperparameter is explicitly set \\nto capture this timescale\\n16.\\nPrevious studies have applied MoSeq to characterize the effects \\nof genetic mutations, drugs, neural manipulations and changes in the \\nsensory or physical environment16,22–24. MoSeq syllables are encoded \\nin the dorsolateral striatum (DLS)—an area important for action  \\nselection—and can be individually reinforced through closed-loop \\ndopamine stimulation 22,23, arguing that MoSeq-identified syllables \\nare meaningful units of behavior used by the brain to organize action \\nsequences. But MoSeq’s reliance on depth cameras is a substantial \\nconstraint; depth cameras are difficult to deploy, suffer from high \\nsensitivity to reflections and have limited temporal resolution25. In prin-\\nciple, these limits could be overcome by applying MoSeq to keypoint \\ndata. But attempts to do so have thus far failed: researchers applying \\nMoSeq-like models to keypoint data have reported flickering state \\nsequences that switch much faster than the animal’s actual behavior13.\\nHere we confirm this finding and trace its cause to jitter in the \\nkeypoint estimates, which is mistaken by MoSeq for behavioral transi-\\ntions. T o address this challenge, we reformulated the model underlying \\nMoSeq to simultaneously infer correct pose dynamics (from noisy or \\nTemporal oﬀset (s)d\\nMoSeq\\n(keypoints)\\ntransition prob.\\nError\\nmagnitude\\nLow-con/f.shortidence\\ndetections\\nMoSeq\\n(depth)\\ntransition prob.\\nFrame1 Frame2 Frame2Frame1\\nc g\\nMoSeq\\n(keypoints)\\nMoSeq\\n(depth)\\nSyllable duration ( s)\\n1 / 4\\n1 / 2\\n1\\n2\\n0.17\\n0.16\\n0.15\\n0.4\\n0.2\\n0 0.14\\n4\\n8\\n16\\n MoSeq (keypoints)\\ntransition (s)\\nKeypoint\\nchange score (z)\\n–log 10 (con/f.shortidence)\\n1 s\\ne\\nb\\nf\\nHuman variabilityK e ypoint jitter while mouse is motionle ss\\nFast\\njitter\\nFluctuation\\nfrequency (Hz) MoSeq (keypoints)\\ntransition (s)\\n* *\\n*  P < 10 –7\\na\\nMoSeq (depth)\\nMoSeq (keypoints)\\n1 s\\nAligned \\nkeypoins\\nMoSeq\\n(depth)\\nMoSeq\\n(keypoints)\\n≤ –0.1 0.4\\n ≤ –0.1 0.4\\n ≤ 0 0.1\\n0.08≤ 0\\nCross-\\ncorrelation:\\n–10 0 10\\n0 0.5 1.0–5 0 5–1 0 1\\n–10 0 10 –10 0 10 –10 0 10\\nInfrared\\nDepth\\nFig. 1 | Keypoint trajectories exhibit sub-second structure. a, Left: \\nsimultaneous depth and 2D infrared (IR) recording setup. Middle: pose \\nrepresentations using the depth data (top) or IR (bottom, tracked keypoints \\nindicated). Right: Example syllable sequences from MoSeq applied to depth \\ndata (referred to as ‘MoSeq (depth)’) or to keypoint data (referred to as ‘MoSeq \\n(keypoints)’). Figure created with SciDraw under a CC BY 4.0 license. b, Keypoint \\nchange scores or low-confidence detection scores, relative to the onset of MoSeq \\ntransitions (x axis) derived from either depth (gray) or keypoint (black) data. \\nDifferences in each case were significant (P\\u2009=\\u20092\\u2009×\\u200910\\n−7 over N\\u2009=\\u200920 model fits, \\nMann–Whitney U test; plots show mean and range across model fits).  \\nc, Comparison of syllable durations for MoSeq (keypoints) and MoSeq (depth), \\nshowing mean and inter-95% confidence interval range across N\\u2009=\\u200920 model \\nfits. d, Left: keypoint detection errors, including high-frequency fluctuations \\nin keypoint coordinates (top row) and error-induced syllable switches (bottom \\nrow). Right: keypoint coordinates before (frame1) and during (frame2) an \\nexample keypoint detection error. This error (occurring in the tail keypoint) \\ncauses a shift in egocentric alignment, hence changes across the other tracked \\nkeypoints. e, 5-s interval during which the mouse is immobile yet the keypoint \\ncoordinates fluctuate. Left: egocentrically aligned keypoint trajectories. Right: \\npath traced by each keypoint during the 5-s interval. f, Variability in keypoint \\npositions assigned by eight human labelers. g, Cross-correlation between various \\nfeatures and keypoint fluctuations at a range of frequencies. Each heat map \\nrepresents a different scalar time series (such as ‘transition probability’—the \\nlikelihood of a syllable transition on each frame). Each row shows the cross-\\ncorrelation between that time series and the time-varying power of keypoint \\nfluctuations at a given frequency.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 2, 'page_label': '1331'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1331\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nbetween depth-based MoSeq syllables (Fig. 1b). Yet when we applied \\nMoSeq directly to the keypoint data, it failed to recognize these dis -\\ncontinuities as syllable transitions, instead generating implausibly \\nbrief syllables that aligned poorly with the keypoint change score \\n(Fig. 1b,c). These observations are consistent with prior work show -\\ning that MoSeq underperforms alternative clustering methods when \\napplied to keypoints13,26.\\nWe wondered whether this poor performance could be explained \\nby noise in the keypoint data, which might introduce subtle disconti-\\nnuities that are falsely recognized by MoSeq as behavioral transitions. \\nIn our data, this noise took the form of high-frequency jitter that \\nreflected errors in body part detection or rapid jumps in the inferred \\nlocation of a stationary body part (Fig. 1d,e, Extended Data Fig. 1a,b \\nand Supplementary Video 1). Much of the jitter—which was perva-\\nsive across camera angles and tracking methods—seemed to reflect \\ninherent ambiguity in the true location of a keypoint, as frame-to-  \\nframe fluctuations in detected keypoint position had a similar \\nscale as the variability in human labeling (Fig. 1f and Extended Data  \\nFig. 1b–e). We confirmed that the jitter was unrelated to true move-\\nment by tracking the same body part using multiple cameras; \\nalthough overall movement trajectories were almost identical across \\ncameras, high-frequency fluctuations around those trajectories were \\nuncorrelated, suggesting that the fluctuations are a tracking artifact \\n(Extended Data Fig. 1f,g).\\nConsistent with the possibility that keypoint noise dominates \\nMoSeq’s view of behavior, syllable transitions derived from keypoints—\\nbut not depth—frequently overlapped with jitter and low-confidence \\nestimates of keypoint position (Fig. 1b,g). We were unable to correct \\nthis defect through simple smoothing: application of a low-pass filter—\\nwhile removing jitter—also blurred true transitions, preventing MoSeq \\nfrom identifying syllable boundaries (Extended Data Fig. 1h). Median \\nfiltering and Gaussian smoothing also yielded no improvement. These \\ndata reveal that high-frequency tracking noise prevents MoSeq from \\naccurately segmenting behavior.\\nCentroid\\nHeading\\nNoise scale\\nKeypoints\\nDiscrete\\nsyllable state\\nLow-dimensional\\npose state (PCA)\\nLow-dim.\\npose state\\nCentroid\\ny-coord\\nHeading\\nAligned\\nkeypoints\\nleft-right\\nObserved variable\\nLatent variable\\nCross-correlation\\nof transition probs.\\nMoSeq (depth)\\nKeypoint\\nchange score\\nLow-con/f.shortidence\\ndetectionsa\\nb\\nMoSeq Keypoint-MoSeq\\n0.5 s\\nTime to transition ( s)\\nChange score (z)\\nc\\n–log 10 (con/f.shortidence)\\nDepth\\nchange score\\nState duration\\nProbability\\nTime to transition ( s) Time to transition ( s)\\nTime to transition ( s)\\nDuration\\ndistribution\\nBack groom\\nBefore model /f.shortitting After model /f.shortitting\\ne\\nd\\nTurn ( small)\\nTurn (mid)\\nTurn (lar ge )\\nLocomotion ( slo w)\\nLocomotion (mid)\\nLocomotion (f ast)\\nSide groom\\nRear\\nDiscrete\\nsyllable state\\nLow-dim.\\npose state\\nChange score (z)\\n* * *\\nKeypoint-MoSeq\\nMoSeq (keypoints)\\nShu/f_f.shortle\\n–5 0 5–1 0 1\\n–0.5 0 0.5 0 0.5 1.0\\n–1 0 1\\n0.17\\n0.16\\n0.15\\n0.14\\n0.4\\n0.2\\n0\\n0.2\\n0.1\\n0\\n0.2\\n0.1\\n0\\nzt\\nxt\\nzt\\nxt\\nstk\\nytk\\nht\\nvt\\nFig. 2 | Hierarchical modeling of keypoint trajectories decouples noise from \\npose dynamics. a, Graphical models illustrating traditional MoSeq and keypoint-\\nMoSeq. In both models, a discrete syllable sequence governs pose dynamics \\nin a low-dimensional pose state; these pose dynamics are either described \\nusing principal component analysis (PCA; as in ‘MoSeq’; left) or inferred from \\nkeypoint observations in conjunction with the animal’s centroid and heading, \\nas well as a noise scale that discounts keypoint detection errors (as in ‘keypoint-\\nMoSeq’; right). b, Example of error correction by keypoint-MoSeq. Left: before \\nfitting, all variables (y axis) are perturbed by incorrect positional assignment \\nof the tail-base keypoint (whose erroneous location is shown in the bottom \\ninset). Right: Keypoint-MoSeq infers plausible trajectories for each variable \\n(shading represents the 95% confidence interval). The inset shows several likely \\nkeypoint coordinates for the tail-base inferred by the model. c, T op: various \\nfeatures averaged around syllable transitions from keypoint-MoSeq (red) versus \\ntraditional MoSeq applied to keypoint data (black), showing mean and inter-95% \\nconfidence interval range across N\\u2009=\\u200920 model fits. Bottom: cross-correlation of \\nsyllable transition probabilities between each model and depth MoSeq. Shaded \\nregions indicate bootstrap 95% confidence intervals. Peak height represents the \\nrelative frequency of overlap in syllable transitions. Differences in each case were \\nsignificant (*P\\u2009=\\u20092\\u2009×\\u200910\\n−7 over N\\u2009=\\u200920 model fits, Mann–Whitney U test). d, Duration \\ndistribution of the syllables from each of the indicated models. Shading as in  \\nc. e, Average pose trajectories for example keypoint-MoSeq syllables. Each \\ntrajectory includes ten poses, starting 165\\u2009ms before and ending 500\\u2009ms after \\nsyllable onset.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 3, 'page_label': '1332'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339 1332\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nHierarchical modeling decouples noise from behavior\\nKeypoint jitter contaminates MoSeq syllables because MoSeq assumes \\nthat each keypoint is a faithful representation of a point on the ani -\\nmal, and thus cannot distinguish noise from real behavior. T o address \\nthis issue, we rebuilt MoSeq as a switching linear dynamical system \\n(SLDS)—a class of model that explicitly disentangles signal from noise \\nin time-series data27,28. This model—called ‘keypoint-MoSeq’—has three \\nhierarchical levels (Fig. 2a): a discrete state sequence that governs tra-\\njectories in a low-dimensional pose space, which then combines with \\nlocation and heading information to yield actual keypoint coordinates. \\nWhen fit to data, keypoint-MoSeq estimates for each frame the animal’s \\nlocation and pose, the noise in each keypoint29 and the identity of the \\ncurrent behavioral syllable (Fig. 2a). Because of its structure, when a \\nsingle keypoint implausibly jumps from one location to another, the \\nmodel can attribute this sudden displacement to noise and preserve a \\nsmooth pose trajectory; if all the keypoints suddenly rotate within the \\negocentric reference frame, the model can adjust the inferred heading \\nfor that frame and restore a plausible sequence of coordinates (Fig. 2b).\\nUnlike traditional MoSeq, keypoint-MoSeq homed in on behav -\\nioral syllables rather than noise in the keypoint data, yielding syllable \\ntransitions that overlapped more strongly with changepoints in pose, \\ncorrelated better with syllable transitions from depth MoSeq and clus-\\ntered less around low-confidence neural network detections (Fig. 2c). \\nKeypoint-MoSeq also outperformed traditional MoSeq when the latter \\nwas applied to filtered keypoint data, or to keypoints inferred with a \\npose estimation method (Lightning Pose) that includes a jitter penalty \\nin its training objective (Extended Data Fig. 2a,b). Furthermore, when \\nwe simulated missing data by ablating subsets of keypoints within \\nrandom (0–3\\u2009s) intervals, keypoint-MoSeq was better able to preserve \\nsyllable labels and boundaries than traditional MoSeq (Extended Data \\nFig. 2c–f). From a modeling perspective, the output of MoSeq was \\nsensible: cross-likelihood analysis revealed that keypoint-based syl -\\nlables were mathematically distinct trajectories in pose space, and \\nsubmitting synthetic keypoint data that lacked any underlying block \\nstructure to keypoint-MoSeq resulted in models that failed to identify \\ndistinct syllables (Extended Data Fig. 2g,h).\\nBecause keypoint-MoSeq produces slightly different syllable \\nsegmentations when run multiple times with different random seeds, \\nwe developed a likelihood-based metric that allows post hoc ranking \\nof model runs (Extended Data Fig. 3a–g); the metric tends to be lowest \\nfor outlier models and highest for those that are consensus-like, pro-\\nviding a rational basis for model selection (Extended Data Fig. 3h–k). \\nThe metric revealed that 500 fitting iterations (~30\\u2009min of compute \\ntime on a GPU for ~5\\u2009h of data) are sufficient to achieve a good model fit \\nwith our open field dataset. Rather than choosing a single best model, \\nusers can also estimate an approximate probability distribution over \\nsyllable labels, although full Bayesian convergence remains impractical \\n(Extended Data Fig. 3l).\\nIn our open field data, keypoint-MoSeq identified 25 syllables \\nthat were easily distinguishable to human observers (Extended Data \\nFig. 4a and Supplementary Videos 2 and 3). These included catego -\\nries of behavior (for example, rearing, grooming and walking), and \\nvariations within categories (for example, turn angle, speed; Fig. 2e). \\nImportantly, keypoint-MoSeq preserved access to the kinematic and \\nmorphological parameters that underlie each behavioral syllable \\n(Extended Data Fig. 4b). Thus, keypoint-MoSeq can provide an inter-\\npretable segmentation of behavior from standard two-dimensional \\n(2D) keypoint tracking data.\\nKeypoint-MoSeq is sensitive to behavioral transitions\\nT o characterize keypoint-MoSeq, we related the discovered syllables \\nto orthogonal measures of behavior and neural activity and compared \\nthem to the behavioral states identified by alternative behavior analysis \\nmethods. These alternatives, which include VAME, MotionMapper and \\nB-SOiD, all work by transforming keypoint data into a feature space that \\nreflects the local dynamics around each frame, and then clustering \\nframes according to those features12,13,17,30.\\nWhen applied to our open field data, behavioral states from \\nVAME, B-SOiD and MotionMapper were usually brief (median dura-\\ntion 33–100\\u2009ms, compared to ~400\\u2009ms for keypoint-MoSeq) and their \\ntransitions aligned poorly with changepoints in keypoint data, sug -\\ngesting diminished sensitivity to the natural breakpoints in mouse \\nbehavior (Fig. 3a–c). This observation was not parameter dependent, \\nbecause it remained true across a broad range of temporal windows \\n(used by B-SOiD and MotionMapper) and after comprehensive scans \\nover latent dimension, state number, clustering mode and preproc -\\nessing options (across all methods as applicable; Extended Data \\nFig. 5a).\\nRearing offers a clear example of the differing sensitivity of each \\nmethod to temporal structure. B-SOiD and keypoint-MoSeq both \\nlearned a specific set of rear states, and each encoded the mouse’s \\nheight with comparable accuracy (Fig. 3d,e). Yet the rear states had \\ndifferent dynamics. Whereas keypoint-MoSeq typically detected two \\nsyllable transitions per rear (one entering the rear and one exiting), \\nB-SOiD detected five to ten different transitions per rear, including \\nswitches between distinct rear states as well as flickering between \\nrear and non-rear states (Fig. 3f and Extended Data Fig. 5b). Whereas \\nmouse height increased at transitions into keypoint-MoSeq’s rear state \\nand fell at transitions out of it, height tended to peak symmetrically at \\ntransitions into and out of B-SOiD’s rear states (Fig. 3g); this observa-\\ntion suggests that—at least in this example—B-SOiD does not effectively \\nidentify the boundaries between syllables, but instead fragments them \\nthroughout their execution.\\nWe also evaluated each method using an orthogonal kinematic \\nmeasurement: 3D head angle and acceleration from head-mounted \\ninertial measurement units (IMUs; Fig. 3h). Behavioral transitions \\nwere identifiable in the IMU data as sudden changes in acceleration \\n(quantified by jerk) and orientation (quantified by angular veloc -\\nity). These measures tended to overlap with state transitions from \\nkeypoint-MoSeq but less so (or not at all) for B-SOiD, MotionMapper \\nand VAME (Fig. 3i). Furthermore, IMU-extracted behavioral features \\n(like head pitch or acceleration) typically rose and fell symmetrically \\naround B-SOiD, MotionMapper and VAME-identified transitions, while \\nkeypoint-MoSeq identified asymmetrical changes in these features \\n(Fig. 3i and Extended Data Fig. 6a).\\nThe fact that keypoint-MoSeq more clearly identifies behavioral \\nboundaries does not necessarily mean that it is better at capturing the \\noverall content of behavior. Indeed, coarse kinematic parameters were \\ncaptured equally well by all four of the tested methods (Extended Data \\nFig. 5c). However, the fact that movement parameters—as measured \\nby accelerometry—change suddenly at the onset of keypoint-MoSeq \\nsyllables, but not at the onset of B-SOiD, VAME or MotionMapper states, \\nprovides evidence that these methods afford fundamentally different \\nviews of temporal structure in behavior.\\nState transitions align with fluctuations in neural data\\nA core use case for unsupervised behavioral classification is to under-\\nstand how the brain generates self-motivated behaviors outside a rigid \\ntask structure9; in this setting, boundaries between behavioral states \\nserve as surrogate timestamps for alignment of neural data. For exam-\\nple, we recently used depth MoSeq to show that dopamine fluctuations \\nin DLS are temporally aligned to syllable transitions during spontane-\\nous behavior22. Here we asked whether the same result was apparent \\nin keypoint-based segmentations of behavior (Fig. 4a).\\nSyllable-associated dopamine fluctuations (as captured by \\ndLight photometry) were remarkably similar between depth MoSeq \\nand keypoint-MoSeq, but much lower in amplitude (or nonexistent) \\nwhen assessed using B-SOiD, VAME and MotionMapper (Fig. 4b  and \\nExtended Data Fig. 7a). We wondered if this apparent discrepancy \\nin syllable-associated dopamine could be explained by differences'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 4, 'page_label': '1333'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1333\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nin how each method represents the temporal structure of behavior. \\nIf, as we have shown, B-SOiD, VAME and MotionMapper can capture \\nthe content of behavior but not the timing of transitions, then aver -\\nage dopamine levels should vary consistently across their behav -\\nior states but lack clear dynamics (increases or decreases) at state \\nonsets. Indeed, for all four methods, almost every state was associated \\nwith a consistent above-average or below-average dopamine level \\n(Fig. 4c,d and Extended Data Fig. 7b), and yet dopamine dynamics \\nvaried widely. Whereas dopamine usually increased at the initiation \\nof keypoint-MoSeq syllables, it was usually flat (having just reached \\na peak or nadir) at state onsets identified by alternative methods \\n(Fig. 4c–e). Furthermore, aligning the dopamine signal to randomly \\nsampled times throughout the execution of each behavioral state—\\nrather than its onset—altered state-associated dopamine dynamics \\nfor keypoint-MoSeq, but made little difference for alternative meth-\\nods (Fig. 4f and Extended Data Fig. 7c,d). These results suggest that \\nkeypoint-MoSeq syllable onsets are meaningful landmarks for neural \\ndata analysis, while state onsets identified by alternative methods are \\noften functionally indistinguishable from random timepoints during \\na behavior.\\nB- S OiD\\nV AME\\nMotionMapper\\nkp-MoSeq\\nAligned k e ypoint tr ajectorie s (r ostr o-caudal)\\n1 s\\na\\nK e ypoint change scor e\\nb\\nMedian height ( cm)\\nd\\nSt ate s (r ank ed)\\n12 cm0Height: f\\nB- S OID\\nkp-MoSeq\\n10\\n5\\nHeight ( cm)\\n1 s\\nIR DepthDepth IR\\nK e ypoint tr acking\\nkp-MoSeq\\nY a w\\nPitch R oll\\nY a w\\nR oll\\nPitch\\n180°\\nAng.\\nv el.\\n90°/s\\nHe ad orient ation\\nh\\n6 m/s3 10 m/s2\\nA cc .\\nJerk\\nLine ar acceler ation\\nx\\ny\\nzy\\nx\\nz\\n1 s\\nDur ation ( s)\\nSt ate dur ations\\n–2 0\\n2 0\\nDegrees\\nPitch\\nT ime r elativ e to st ate onset ( s)\\nLine ar acceler ation\\n–0 . 3\\n0 . 3\\nz-score\\ni\\n30\\n7 0\\nDegrees/s\\nDegr ee s / s\\n45\\n4 0\\nAngular v elocity Line ar jerk\\nkp-MoSeqB- S OiD V AME MMP er kp-MoSeqB- S OiD V AME MMP er\\n–0 . 6\\n0 . 6\\nz-scor e\\n5\\n0\\nT ime r elativ e to st ate onset ( s)\\nT ime r elativ e to st ate onset ( s)\\nT ime r elativ e to st ate onset ( s)\\nkp-MoSeqB- S OiD V AME MMP er kp-MoSeqB- S OiD V AME MMP er\\nz-score\\nSt ate s St ate s\\nSt ate s St ate s\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\n10\\n5\\n10\\n5\\n10\\n5\\n10\\n50 21\\nB- S OID kp-MoSeq\\ng\\nT ime r elativ e to onset/ oﬀ set ( s)\\nHeight ( cm) Onset of\\nr e ar st ate s\\nO ﬀset of\\nr e ar st ate s\\ne\\nT ime ( s)\\nK e ypoint change scorec\\nChange scor e (z) 1\\n0\\n–1\\nChange scor e (z)\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\nCorr elation\\nto true height\\n–1 10 –1 10\\n0.3\\n0\\n0.75\\n0.50\\n0.25\\n6\\n4\\n–1 10\\n–1 10 –1 10 –1 10 –1 10\\n–1 10 –1 10 –1 10 –1 10\\n–1 10 –1 10 –1 10 –1 10\\n–1 10 –1 10 –1 10 –1 10\\nFig. 3 | Keypoint-MoSeq captures the temporal structure of behavior.  \\na, Output from four methods applied to the same 2D keypoint dataset.  \\nb, Distribution of state durations for each method in a. c, Left: average keypoint \\nchange scores (z-scored) around transitions identified by each method. \\nRight: distribution of change scores at the transition point (‘MMper’ refers to \\nMotionMapper). d, Distribution of mouse heights (measured by depth camera) \\nfor each unsupervised behavior state. States are classified as rear specific (and \\ngiven a non-gray color in the plot) if they have median height\\u2009>\\u20096\\u2009cm. e, Accuracy \\nof models trained to predict mouse height from behavior labels showing the \\ndistribution of accuracies across N\\u2009=\\u200910 recordings. f, Bottom: state sequences \\nfrom keypoint-MoSeq and B-SOiD during a pair of example rears. States are \\ncolored as in d. T op: mouse height over time with rears shaded gray. Callouts \\nshow depth and IR views of the mouse during two example frames. g, Mouse \\nheight aligned to the onsets (solid lines) or offsets (dashed lines) of rear-specific \\nstates defined in d, showing mean and 95% confidence of the mean. h, Signals \\ncaptured from a head-mounted IMU, including absolute 3D head orientation \\n(top) and relative linear acceleration (bottom). Each signal and its rate of change, \\nincluding angular velocity (ang. vel.) and jerk (the derivative of acceleration), \\nare plotted during a 5-s interval. Figure created with SciDraw under a CC BY 4.0 \\nlicense. i, IMU signals aligned to the onsets of each behavioral state. Each heat \\nmap row represents a state. Line plots show the median across states for angular \\nvelocity and jerk (average and standard across N\\u2009=\\u200910 model fits). Keypoint-\\nMoSeq peaks at a higher value for both signals (P\\u2009<\\u20090.0005, N\\u2009=\\u200910,  \\nMann–Whitney U test).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 5, 'page_label': '1334'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339 1334\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nKeypoint-MoSeq generalizes across experimental setups and \\nbehaviors\\nKeypoint tracking is a powerful means of pose estimation because \\nit generalizes widely across experimental setups. T o test whether \\nkeypoint-MoSeq inherits this flexibility, we asked if it could quantify \\nchanges in behavior induced by environmental enrichment. Mice \\nwere recorded in either an empty arena or one that contained bed -\\nding, chew toys and a transparent shelter (Extended Data Fig. 8a). \\nThe enriched environment was too complex for traditional depth \\nMoSeq but yielded easily to keypoint-based pose estimation. Based \\non these poses, keypoint-MoSeq identified 39 syllables, of which 21 \\nvaried between environments: syllables upregulated in the enriched \\nenvironment tended to involve manipulation and orientation toward \\nnearby affordances (for example, ‘investigation’ , ‘stationary right turn’ \\nand ‘stop and dig’), whereas those upregulated in the empty box were \\nlimited to locomotion and rearing (‘dart forward’ and ‘rear-up in corner’; \\nExtended Data Fig. 8b,c). These results suggest that keypoint-MoSeq \\nmay be useful in a broad range of experimental contexts, includ -\\ning those whose cluttered structure precludes the effective use of  \\ndepth cameras.\\nT o test if keypoint-MoSeq can also generalize across laborato -\\nries—and to better understand the mapping between syllables and \\nhuman-identified behaviors—we next analyzed a pair of published \\nbenchmark datasets 31,32. The first dataset included human annota -\\ntions for four mouse behaviors in an open field (locomotion, rearing, \\nface grooming and body grooming) and keypoint detections from the \\nT opViewMouse model in the DLC Model Zoo33 (Fig. 5a–c). The second \\ndataset (part of the CalMS21 benchmark 32) included a set of three \\nmanually annotated social behaviors (mounting, investigation and \\nattack) as well as keypoints for a pair of interacting mice (Fig. 5d–f ). \\nKeypoint-MoSeq recovered syllables from both datasets whose aver-\\nage duration was ~400\\u2009ms, while, as before, the B-SOiD, MotionMap-\\nper and VAME identified behavioral states that were much shorter \\n(Extended Data Fig. 9a). Keypoint-MoSeq states also conformed more \\nclosely to human-identified behavioral states (Fig. 5c,f and Extended \\nData Fig. 9b). Although this advantage was modest overall, there were \\nsome important differences: in the CalMS21 dataset, for example, \\nMotionMapper, B-SOiD and VAME only identified a single behavior \\nconsistently, with B-SOiD and VAME only capturing mounting and \\nMotionMapper only capturing investigation in 100% of model fits; \\nCorrelation\\n(onsets vs. random)\\ndLight (z -scored ∆ F/ F)a\\nb\\nf\\nDerivative of\\nz-scored ∆ F/ F\\nTime relative to transition (s)\\nSum of ∆ F/ F (z )\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\n95th percentile of shu/f_f.shortle\\nDiﬀ. of  ∆ F/ F (z )\\nz-scored ∆F/F\\nState 1\\nState N\\nTemporal asymmetrye\\n+–\\n+–\\nState 1\\nState N\\nSize of /f.shortluctuation\\nd\\n++\\n++\\nRandom\\nframes\\nOnsets\\nStates\\nAlignment to onsets vs.\\nrandom frames from state\\n dLight ∆ F/ F (z )\\nPause Dart Pause Turn\\nTime relative to onset (s) Time relative to onset (s)\\n∆F/ F (z )\\nKeypoint-MoSeq VAME\\n∆F/ F (z )\\nc\\nkp-MoSeq\\nMMper\\nVAME\\nB-SOiD\\nMoSeq (depth) kp-MoSeqB-SOiD VAME MMper\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\nB- S OiD\\nV AME\\nMMper\\nkp-MoSeq\\n1 s\\n–1\\n0.4\\n0.2\\n0\\n–0.2\\n10\\n–1\\n0.2\\n0\\n–0.2\\n0\\n–0.2\\n0\\n0\\n0.25\\n10 –1 10 –1 10 –1 10\\n–1 10 –1 10 –1 10 –1 10\\n0.2\\n0.1\\n0\\n0.3\\n0.2\\n0.1\\n0\\n1\\n0\\n–1\\nFig. 4 | Keypoint-MoSeq syllable transitions align with fluctuations in striatal \\ndopamine. a, Illustration depicting simultaneous recordings of dopamine \\nfluctuations in the DLS obtained from fiber photometry (top) and unsupervised \\nbehavioral segmentation of 2D keypoint data (bottom). Adapted from ref. 22, \\nSpringer Nature Limited. b, Derivative of the dopamine signal aligned to state \\ntransitions from MoSeq (depth) and each keypoint-based method, showing the \\nmean and 95% confidence of the mean. The derivative peaks at a higher value for \\nkeypoint-MoSeq compared to the non-MoSeq methods (P\\u2009<\\u200910\\n−5, N\\u2009=\\u200920 model \\nfits per method, Mann–Whitney U test). c, Average dopamine signal (z-scored \\nchange in fluorescence, ΔF/F) aligned to the onset of example states identified by \\nkeypoint-MoSeq and VAME. Shading marks the 95% confidence interval around \\nthe mean. d, Distributions capturing the magnitude of state-associated dopamine \\nfluctuations across states from each method (merging N\\u2009=\\u200920 model fits per \\nmethod), where magnitude is defined as the mean total absolute value in a 1-s \\nwindow centered on state onset. Box plots show median and interquartile range \\n(IQR). e, Distributions capturing the temporal asymmetry of state-associated \\ndopamine fluctuations, where asymmetry is defined as the difference in mean \\ndopamine signal during 500\\u2009ms after versus 500\\u2009ms before state onset. Keypoint-\\nMoSeq syllables have a higher asymmetry score on average than those from other \\nmethods (P\\u2009<\\u200910\\n−4, N\\u2009=\\u200920 model fits per method, Mann–Whitney U test).  \\nf, T emporal randomization affects keypoint-MoSeq-identified neurobehavioral \\ncorrelations, but not those identified by other methods. T op: schematic of \\nrandomization. The dopamine signal was aligned either to the onsets of each \\nstate, as in c, or to random frames throughout the execution of each state. \\nBottom: distributions capturing the correlation of state-associated dopamine \\nfluctuations before versus after randomization. Keypoint-MoSeq syllables have \\na lower correlation on average than those from other methods (P\\u2009<\\u200910\\n−4, N\\u2009=\\u200920 \\nmodel fits per method, Mann–Whitney U test).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 6, 'page_label': '1335'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1335\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nkeypoint-MoSeq, in contrast, defined at least one state specific to each \\nof the three behaviors in 100% of model fits (Extended Data Fig. 9c).\\nThe above benchmark datasets differed widely in the number \\nof keypoints tracked (7 for CalMS21 versus 21 for the T opViewMouse \\nmodel; Fig. 5a,d), raising the question of how the pose representation \\nfed to keypoint-MoSeq influences its outputs. One possibility—  \\nsuggested by the higher syllable count for depth MoSeq (~50) compared \\nto keypoint-MoSeq fit to 2D keypoints (~25)—is that higher-dimensional \\ninput data allows MoSeq to make finer distinctions between behav -\\niors. T o test this rigorously, we used multiple cameras to estimate \\n≥ 0.75\\nAttack (2%)\\nMount (29%)\\nInvestigate (6%)\\nBackground (63%)\\nd\\nLow rear\\nHigh rear\\nHigh turn\\nLow turn\\nRear Turn Low\\nrear\\nHigh\\nrear\\nHigh\\nturn\\nLow\\nturn\\n3D kp-MoSeq syllables\\n3D kps vs. 2D kps 3D kps vs. depth\\nDuration (s)\\nprobability\\nNo. syllables\\n2D kps\\n3D kps\\nDepth\\nLow rear High rearHigh turn Low turn\\nDepth MoSeq syllables\\nLow rear High rearHigh turn Low turn\\n3D kp-MoSeq syllables\\nRearTurn\\n2D kp-MoSeq syllables\\ng\\ni\\njh\\n2D kps\\n3D kps\\nTop-down depth\\nNMI\\nBackground (49%)\\nLocomote (24%)\\nRear (20%)\\nFace groom (3%)\\nBody groom (1%)\\nUnsupervised behavior states\\nB-SOID VAME kp-MoSeq\\n0.9\\n0\\nProbability\\na\\nNMI\\nb\\ne\\nCross-correlation\\nof transition probs. Temporal oﬀset (s)\\nk\\nl\\n7.50Height (cm)\\nc\\nf\\nDepth MoSeq syllables\\n0.25Probability 0\\n2D kp-MoSeq syllables\\n0.5Probability 0\\nMMper\\nMMP er\\nB- S OID\\nV AME\\nkp-MoSeq\\nUnsupervised behavior states\\nB-SOID VAME kp-MoSeqMMper\\nChange score (z)\\nTime relative to\\nkeypoint-MoSeq\\ntransition\\n0 . 4\\n0 . 3\\n0 .2\\n–2 2–1 10\\nGroom Rear-up Lever press Rear-down\\nm o\\nRetrore/f.shortlective\\nmarker locations\\n*\\n*\\nn\\nRat location during\\nlever-press syllable\\nLever\\nlocation\\nPause\\nRear groom\\nPaw groom\\nRear up\\nLocomotion\\nTurn left\\nRear down\\nTurn right\\nMixed\\n2D kps vs. 3D kps\\nDepth vs. 3D kps\\n 2D kpsDepth 3D kps\\n0\\nProbability\\n0.2\\n0.1\\n0\\n–1 10 0 21 0 21 0 21\\n30\\n40\\n50\\n0.15\\n0.10\\n0.05\\n0.06\\n0.04\\nFig. 5 | Keypoint-MoSeq generalizes across experimental setups. a, Frame \\nfrom an open field benchmark dataset. b, Confusion matrices showing overlap \\nbetween human-labeled behaviors and unsupervised states. c, Normalized \\nmutual information (NMI) between supervised and unsupervised labels, \\nshowing the distribution of NMI values across N\\u2009=\\u200920 model fits. Keypoint-\\nMoSeq consistently had higher NMI (*P\\u2009<\\u200910−6, Mann–Whitney U test). d, Frame \\nfrom the CalMS21 social behavior benchmark dataset, showing 2D keypoints of \\nthe resident mouse. e,f, Comparison between human labels and unsupervised \\nbehavior states of the resident mouse, as in b and c (P\\u2009<\\u200910−5, Mann–Whitney U \\ntest). g, Multi-camera arena for simultaneous recording of 3D keypoints (3D kps), \\n2D keypoints (2D kps) and depth videos. Figure created with SciDraw under a  \\nCC BY 4.0 license. h, Comparison of MoSeq outputs from each modality. \\nLeft: cross-correlation between 3D transition probabilities and those for 2D \\nkeypoints and depth. Shading shows bootstrap 95% confidence intervals; \\nmiddle: distribution of syllable durations, showing mean and inter-95% \\nconfidence interval range across N\\u2009=\\u200920 model fits. Right: number of states with \\nfrequency\\u2009>\\u20090.5%, showing the distribution of state counts across 20 runs of \\neach model. i, Overlap of syllables from 2D keypoints (left) or depth (right) with \\neach 3D keypoint-based syllable. j–l, Average pose trajectories for the syllables \\nmarked in i. k, 3D trajectories are plotted from the side (first row) and top (second \\nrow). l, Average pose (as depth image) 100\\u2009ms after syllable onset. m, Location of \\nmarkers for rat motion capture. Figure created with SciDraw under a CC BY 4.0 \\nlicense. n, Left: average keypoint change score (z) aligned to syllable transitions. \\nShading shows 95% confidence intervals of the mean. Right: durations of \\nkeypoint-MoSeq states and inter-changepoint intervals. o, Left: pose trajectories \\nof example syllables learned from rat motion capture data. Right: random sample \\nof rat centroid locations during execution of the ‘lever-press’ syllable.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 7, 'page_label': '1336'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339 1336\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nkeypoints in 3D (including six keypoints that were not visible in the \\noverhead-camera 2D dataset) and confirmed that the 3D keypoints had \\nhigher intrinsic dimensionality than 2D keypoints (Fig. 5g and Extended \\nData Fig. 9d,e). Despite this difference in dimensionality, similar \\nchangepoints were evident in both datasets, and keypoint-MoSeq iden-\\ntified syllables with similarly timed transitions (Fig. 5h and Extended \\nData Fig. 9f).\\nThere was a bigger change, however, in how behaviors were cat-\\negorized. Keypoint-MoSeq made finer-grained behavior distinctions \\nbased on 3D data as compared to 2D data, especially for behaviors that \\nvaried in height (Fig. 5i–l and Supplementary Video 4). Turning, for \\nexample, was grouped as a single state based on the 2D keypoint data \\nbut partitioned into three states with different head positions based \\non the 3D keypoint data (nose to the ground versus nose in the air; \\nFig. 5j–l). Rearing was even more fractionated, with a single 2D syllable \\nsplitting six ways based on body angle and trajectory in the 3D keypoint \\ndata. Depth-based MoSeq fractionated these behaviors still further. \\nThis analysis suggests that higher-dimensional input data permit \\nricher descriptions of behavior, but even relatively low-dimensional \\n2D keypoint data still capture the timing of behavioral transitions.\\nKeypoint-MoSeq parses behavior across species and \\ntimescales\\nT o test if keypoint-MoSeq generalizes across rodent species, we ana-\\nlyzed previously published 3D motion capture data derived from \\nrats. In this dataset, rats were adorned with reflective body piercings \\nHind-R\\nMid-R\\nF or e-R\\nHind-L\\nMid-L\\nF or e-L\\nSpectral power\\nNose\\nFrequency (Hz)\\n0 5 10 15 20\\n0 25 50 75 100\\nKeypoint-MoSeq\\n0 1Time (s)\\nKeypoints\\nTarget\\ntimescale\\n0 25 50 75 100\\nStride cycle (%) Frequency (Hz)\\nSpectral power\\n10 10\\n10 3\\nStride cycle (%)\\n0 25 50 75 100\\nStride cycle (%)\\n0\\n0 10 20\\nFrequency (Hz)\\n0 10 20\\n25 50 75 100\\nStride cycle (%)\\n0.5\\n0\\nMotif\\nprobability\\nStickiness parameter = 103 Stickiness parameter = 104 Stickiness parameter = 1010h\\nj k\\ni\\nPower spectra during fast locomotion\\nMotifsKeypoints\\nKeypoint-MoSeq:\\nexample motifs\\ng\\nc\\nThermistor\\nOlf actor y\\nepithelium\\nNasal ca vity\\na\\nb\\n200 ms\\nKeypoint\\nvelocity\\nTherm.\\ntemp.\\nInhalation E xhalation\\nSwingStance\\nOnsets of motif 26\\nPre (50 ms) Post (50 ms)\\nInhaleExhale\\nP(inhale) pre vs. post \\nmotif onset\\n43\\n26\\n40\\n12\\nSigni/f.shorticant motif\\n–log 10 (P value)\\nP(inhale) P(inhale)\\nTime from\\nmotif onset (s)\\nTime from\\nmotif onset (s)\\nMotif 43 Motif 26\\nMotif 40 Motif 12\\nd\\ne\\nf\\n0.5\\n0.2\\n100\\n75\\n50\\n25\\n0\\n0 0.1\\n0.5\\n0.2\\n–0.2 0.20 –0.2 0.20\\nMotif instances\\nFig. 6 | Keypoint-MoSeq segments behavior at multiple timescales. a, Setup \\nfor recording 3D pose and respiration, including location of thermistor, which \\nmonitors temperature fluctuations caused by respiration. Figure created \\nwith SciDraw under a CC BY 4.0 license. b, 3D keypoint velocities (top) and \\nthermistor signal (bottom) over a 1-s interval. Keypoint traces are colored as in \\na and vertically spaced to ease visualization. c, Power spectra of 3D keypoint \\nvelocities (top) and thermistor signal (bottom). d, Example motif that aligns \\nwith inhale-to-exhale transition. The heat map shows respiration states across \\nmany instances of the motif. e, Volcano plot revealing respiration-aligned \\nmotifs. The x axis reflects change of inhalation probability during the 50\\u2009ms \\nbefore versus after motif onset. f, Keypoint trajectories (top) and motif-aligned \\ninhalation probabilities (bottom) for four motifs highlighted in e. Gray shading \\n(bottom) shows the 2.5th-to-97.5th-percentile range of a shuffle distribution. \\ng, Average pose trajectories for three fly motifs. h, Example of motif sequences \\nduring locomotion. T op: Keypoint-MoSeq output for models tuned to a range \\nof timescales. Each row shows the output of a different model. Bottom: Aligned \\nkeypoint trajectories (anteroposterior coordinate). i, Frequency of motifs across \\nthe stride cycle during fast locomotion. Each line corresponds to one motif,  \\nand each panel represents a model with a different target timescale.  \\nj, T op: progression through the stride cycle. Bottom: probability that each leg \\nis in stance or swing phase at each point in the stride; soft boundaries reflect \\nvariation in step timing. k, Power spectral density of keypoints (left) or motif \\nlabels (right) during fast locomotion. Colors in the right-hand plot correspond to \\nmodels with a range of values for the stickiness hyperparameter, which sets the \\ntarget timescale.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 8, 'page_label': '1337'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1337\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nand recorded in a circular home cage arena with a lever and water \\nspout for operant training (Fig. 5m; Rat7M dataset34). As with mice, \\nkeypoint-MoSeq syllables aligned with changepoints in the keypoint \\ndata (Fig. 5n) and included a diversity of behaviors, including a sylla-\\nble specific to lever pressing in the arena (Fig. 5o and Supplementary \\nVideo 5).\\nMice combine postural movements, respiration and whisking to \\nsense their environment. Recent work suggests that rodents coordinate \\nthese behaviors in time, generating rhythmic head movements that \\nsynchronize with the sniff cycle 35,36. Using an autoregressive hidden \\nMarkov model (AR-HMM), for example, head-movement motifs were \\ndiscovered that align to respiration and arise during olfactory naviga-\\ntion21. Respiration, therefore, defines a fast timescale of mouse behav-\\nior that coexists with—but is distinct from—the ~400-ms timescale of \\nbehavioral syllables.\\nT o test if keypoint-MoSeq can capture behavioral motifs at this \\nfaster timescale, we used 120-Hz cameras to track 3D keypoints of mice \\nand measured respiration with an implanted thermistor 37 (Fig. 6a). \\nConsistent with prior work, we observed respiration-synchronized \\nfluctuations in nose velocity, although synchrony was weak or absent in \\nother parts of the body (Fig. 6b,c). We then fit keypoint-MoSeq models \\nwith a range of target timescales (~35\\u2009ms to ~300\\u2009ms; Extended Data \\nFig. 10a). Motifs were defined as ‘respiration coupled’ if they consist-\\nently aligned with transitions in respiration state (inhale-to-exhale or \\nexhale-to-inhale; Fig. 6d,e). Although respiration coupling was evident \\nacross all models, its prominence peaked at shorter timescales (Extended \\nData Fig. 10a), especially when fit to a subset of anterior keypoints that \\nemphasized neck and nose movements (Extended Data Fig. 10b). The \\nbest-synchronized motifs (from the full-body model) tended to coincide \\nwith exhalation and involved isolated movements in which the nose \\nflutters down (Fig. 6e,f). These results suggest that keypoint-MoSeq can \\ncharacterize fast, sniff-aligned movements in the mouse.\\nGiven that keypoint-MoSeq can parse two different timescales of \\nmouse behavior, we wondered if it could also segment fly behavior, \\nwhich similarly occurs at multiple well-defined timescales. Flies tend \\nto switch between distinct, oscillatory pose trajectories17. These move-\\nments can be finely subdivided, as in the coordinated stance and swing \\nphases of locomotion38, or more coarsely segmented at the transitions \\nbetween distinct oscillatory modes (for example, locomotion versus \\ngrooming), as they are by MotionMapper\\n17. T o capture these distinct \\nlevels of organization, we fit keypoint-MoSeq to 2D keypoints from \\nflies exploring a flat substrate17,39 (Extended Data Fig. 10c). The result-\\ning behavioral motifs varied from tens to hundreds of milliseconds \\ndepending on keypoint-MoSeq’s target timescale. At longer time -\\nscales, keypoint-MoSeq identified recognizable behaviors such as \\nlocomotion, head grooming or left-wing grooming, similarly to the \\nbehaviors reported by MotionMapper (Fig. 6g, Supplementary Video 6  \\nand Extended Data Fig 10d–f).\\nAt shorter time scales, keypoint-MoSeq divided these behav -\\niors into their constituent phases. Fast locomotion, for example, \\nwas split between six phase-locked motifs that tiled the stride cycle \\n(Fig. 6h). As target timescales grew longer, locomotion merged from \\nsix to two phases (corresponding to the alternating swings and stances \\nof a canonical tripod gait) before eventually collapsing to a single \\nmotif that encompassed the full stride cycle (Fig. 6h–j and Extended \\nData Fig. 10g). This shift was evident in the power spectral density \\nof keypoint-MoSeq’s output, which began with a prominent peak at \\n~12\\u2009Hz during fast locomotion (corresponding to the stride cycle) \\nthat slowly disappeared as keypoint-MoSeq’s target timescale was \\nincreased (Fig. 6k ). The same hierarchy of timescales appeared for \\nnon-locomotion behaviors as well (Extended Data Fig. 10h). These \\nresults demonstrate that keypoint-MoSeq is useful as a tool for fly \\nbehavior analysis and suggest a principle for setting its target timescale \\nthat depends on whether researchers wish to subdivide the distinct \\nphases of oscillatory behaviors.\\nDiscussion\\nSyllables are broadly useful for understanding behavior16,22–24, but their \\nscope has been limited by the past requirement for depth data. Here \\nwe show that keypoint-MoSeq affords similar insight as depth-based \\nMoSeq while benefiting from the generality of markerless keypoint \\ntracking. Whereas depth MoSeq was limited to a narrow range of spatial \\nscales and frame rates, keypoint-MoSeq can be applied to mammals \\nand insects, parsing behaviors at the second or millisecond timescale. \\nAnd because keypoint tracking is more robust to occlusion and environ-\\nmental clutter, it is now possible to parse syllables amid environmental \\nenrichment, in animals behaving alone or socially, with or without \\nheadgear and neural implants.\\nThe core innovation enabling keypoint-MoSeq is a probabilis -\\ntic model that effectively handles occlusions, tracking errors and \\nhigh-frequency jitter. These noise sources are pervasive in pose track-\\ning5,26; because standard methods like SLEAP and DLC process each \\nframe separately, keypoint coordinates tend to jump from frame to \\nframe even when the subject’s pose has not discernably changed. A \\nnewer generation of pose tracking methods, such as GIMBAL29, Deep \\nGraph Pose26 and Lightning Pose40, correct for some of these errors; \\nand two-step pipelines that build on these methods may be less prone \\nto keypoint jitter. Here, we describe a different solution: combining \\nnoise-correction and behavior segmentation in a single end-to-end \\nmodel that leverages learned patterns of animal motion to infer the \\nmost plausible pose trajectory from noisy or missing data.\\nKeypoint-MoSeq is somewhat resilient to noise, but it will perform \\nbest with clean keypoint data that capture most parts of the body. \\nAlthough directly modeling the raw pixel intensities of depth16 or 2D \\nvideo41 provides the most detailed access to spontaneous behavior, \\ntechnical challenges like reflections, occlusions and variation in per-\\nspective and illumination remain a challenge in those settings. The \\ndevelopment of keypoint-MoSeq—together with advances in marker-\\nless pose tracking—should enable MoSeq to be used in a variety of  \\nthese adversarial circumstances, such as when animals are obstructed \\nfrom a single axis of view, when multiple animals are interacting simul-\\ntaneously, when the environment changes dynamically and when \\nanimals wear elaborate headgear.\\nCompared to keypoint-MoSeq, the alternative methods for unsu-\\npervised behavior segmentation that we tested (B-SOiD12, MotionMap-\\nper17 and VAME13) tend to emit shorter behavior motifs that often start \\nor stop in the middle of what humans might identify as a behavioral \\nmodule or motif (for example, a rear). Our analysis suggests two pos-\\nsible reasons for this difference. First, unlike alternative methods, \\nMoSeq can discretize behavior at a particular user-defined timescale \\nand, therefore, is better able to identify clear boundaries between \\nbehavioral elements that respect the natural rhythmicity in move -\\nments associated with syllables, sniffs or steps. The resulting parsi -\\nmony prevents over-fractionation of individual behaviors. Second, \\nthe hierarchical structure of keypoint-MoSeq’s underlying generative \\nmodel means it can detect noise in keypoint trajectories and distinguish \\nthis noise from actual behavior without smoothing away meaningful \\nbehavioral transitions.\\nThat said, we stress that there is no one best approach for behav-\\nioral analysis, as all methods involve trade-offs 42,43. For example, \\nkeypoint-MoSeq does not yield a single fixed description of behav -\\nior, since its output is probabilistic. In principle, one could summa -\\nrize this uncertainty in the form of a posterior distribution. Because \\nproper posterior estimation is impractical using our current fitting \\nprocedure, we have defined an alternative approach whereby users \\ngenerate an ensemble of candidate model fits and identify a consensus \\nmodel for downstream analysis. Users wishing to better quantify model \\nuncertainty can also apply subsequent analyses to the full ensemble \\nof models. Keypoint-MoSeq is also limited to describing behavior at a \\nsingle timescale. Although users may vary this timescale across a broad \\nrange, keypoint-MoSeq cannot simultaneously analyze behavior across'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 9, 'page_label': '1338'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339 1338\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nmultiple timescales or explicitly represent the hierarchical nesting of \\nbehavior motifs. Finally, because keypoint-MoSeq learns the identity \\nof syllables from the data itself, it may miss especially rare behavioral \\nevents that could otherwise be captured using supervised methods.\\nT o facilitate the adoption of keypoint-MoSeq, we built a website \\n(http://www.moseq4all.org/) that includes free access to the code \\nfor academics as well as extensive documentation and guidance for \\nimplementation. As demonstrated here, the model underlying MoSeq \\nis modular and thus accessible to extensions and modifications that can \\nincrease its alignment to behavioral data. For example, a time-warped \\nversion of MoSeq was recently reported that incorporates a term to \\nexplicitly model variation in movement vigor 19. We anticipate that \\nthe application of keypoint-MoSeq to a wide variety of experimental \\ndatasets will both yield important information about the strengths and \\nfailure modes of model-based methods for behavioral classification, \\nand prompt continued innovation.\\nOnline content\\nAny methods, additional references, Nature Portfolio reporting sum-\\nmaries, source data, extended data, supplementary information, \\nacknowledgements, peer review information; details of author contri-\\nbutions and competing interests; and statements of data and code avail-\\nability are available at https://doi.org/10.1038/s41592-024-02318-2.\\nReferences\\n1. Tinbergen, N. The Study of Instinct (Clarendon Press, 1951).\\n2. Dawkins, R. In Growing Points in Ethology (Bateson, P. P. G. & \\nHinde, R. A. eds.) Chap 1 (Cambridge University Press, 1976).\\n3. Baerends, G. P. The functional organization of behaviour.  \\nAnim. Behav. 24, 726–738 (1976).\\n4. Pereira, T. D. et al. SLEAP: a deep learning system for multi-animal \\npose tracking. Nat. Methods 19, 486–495 (2022).\\n5. Mathis, A. et al. DeepLabCut: markerless pose estimation of \\nuser-defined body parts with deep learning. Nat. Neurosci. 21, \\n1281–1289 (2018).\\n6. Sun, J. J. et al. Self-supervised keypoint discovery in behavioral \\nvideos. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern \\nRecognit. 2022, 2161–2170 (2022).\\n7. Graving, J. M. et al. DeepPoseKit, a software toolkit for fast and \\nrobust animal pose estimation using deep learning. eLife 8, \\ne47994 (2019).\\n8. Mathis, A., Schneider, S., Lauer, J. & Mathis, M. W. A primer on \\nmotion capture with deep learning: principles, pitfalls, and \\nperspectives. Neuron 108, 44–65 (2020).\\n9. Datta, S. R., Anderson, D. J., Branson, K., Perona, P. & Leifer, A. \\nComputational neuroethology: a call to action. Neuron 104,  \\n11–24 (2019).\\n10. Anderson, D. J. & Perona, P. Toward a science of computational \\nethology. Neuron 84, 18–31 (2014).\\n11. Pereira, T. D., Shaevitz, J. W. & Murthy, M. Quantifying behavior to \\nunderstand the brain. Nat. Neurosci. 23, 1537–1549 (2020).\\n12. Hsu, A. I. & Yttri, E. A. B-SOiD, an open-source unsupervised \\nalgorithm for identification and fast prediction of behaviors.  \\nNat. Commun. 12, 5188 (2021).\\n13. Luxem, K. et al. Identifying behavioral structure from deep \\nvariational embeddings of animal motion. Commun. Biol. 5,  \\n1267 (2022).\\n14. Marques, J. C., Lackner, S., Félix, R. & Orger, M. B. Structure of \\nthe Zebrafish locomotor repertoire revealed with unsupervised \\nbehavioral clustering. Curr. Biol. 28, 181–195 (2018).\\n15. Todd, J. G., Kain, J. S. & de Bivort, B. L. Systematic exploration \\nof unsupervised methods for mapping behavior. Phys. Biol. 14, \\n015002 (2017).\\n16. Wiltschko, A. B. et al. Mapping sub-second structure in mouse \\nbehavior. Neuron 88, 1121–1135 (2015).\\n17. Berman, G. J., Choi, D. M., Bialek, W. & Shaevitz, J. W. Mapping \\nthe stereotyped behaviour of freely moving fruit flies. J. R. Soc. \\nInterface https://doi.org/10.1098/rsif.2014.0672 (2014).\\n18. Batty, E. et al. BehaveNet: nonlinear embedding and Bayesian \\nneural decoding of behavioral videos. in Advances in Neural \\nInformation Processing Systems 32 (eds H. Larochelle et al.) \\n15706–15717 (Curran Associates, 2019).\\n19. Costacurta, J. C. et al. Distinguishing discrete and  \\ncontinuous behavioral variability using warped autoregressive \\nHMMs. in Advances in Neural Information Processing Systems 35 \\n(eds S. Koyejo et al.) 23838–23850 (Curran Associates,  \\n2022).\\n20. Jia, Y. et al. Selfee, self-supervised features extraction of animal \\nbehaviors. eLife 11, e76218 (2022).\\n21. Findley, T. M. et al. Sniff-synchronized, gradient-guided olfactory \\nsearch by freely moving mice. eLife 10, e58523 (2021).\\n22. Markowitz, J. E. et al. Spontaneous behaviour is structured by \\nreinforcement without explicit reward. Nature 614, 108–117 \\n(2023).\\n23. Markowitz, J. E. et al. The striatum organizes 3D behavior via \\nmoment-to-moment action selection. Cell 174, 44–58 (2018).\\n24. Wiltschko, A. B. et al. Revealing the structure of \\npharmacobehavioral space through motion sequencing.  \\nNat. Neurosci. https://doi.org/10.1038/s41593-020-00706-3 \\n(2020).\\n25. Lin, S. et al. Characterizing the structure of mouse behavior using \\nmotion sequencing. Preprint at https://arxiv.org/abs/2211.08497 \\n(2022).\\n26. Wu, A. et al. Deep Graph Pose: a semi-supervised deep graphical \\nmodel for improved animal pose tracking. in Proceedings of the \\n34th International Conference on Neural Information Processing \\nSystems (Curran Associates, 2020).\\n27. Murphy, K. P. Machine Learning (MIT Press, 2012).\\n28. Linderman, S. et al. In Proceedings of the 20th International \\nConference on Artificial Intelligence and Statistics Vol. 54  \\n(eds Aarti, S. et al.) 914–922 (PMLR, Proceedings of Machine \\nLearning Research, 2017).\\n29. Zhang, L., Dunn, T., Marshall, J., Olveczky, B. & Linderman, S. In \\nProceedings of The 24th International Conference on Artificial \\nIntelligence and Statistics Vol. 130 (eds Banerjee Arindam & \\nFukumizu Kenji) 2800–2808 (PMLR, Proceedings of Machine \\nLearning Research, 2021).\\n30. Klibaite, U. et al. Deep phenotyping reveals movement \\nphenotypes in mouse neurodevelopmental models. Mol. Autism \\n13, 12 (2022).\\n31. Bohnslav, J. P. et al. DeepEthogram, a machine learning pipeline \\nfor supervised behavior classification from raw pixels. eLife 10, \\ne63377 (2021).\\n32. Sun, J. J. et al. Caltech mouse social interactions (CalMS21) \\ndataset. https://doi.org/10.22002/D1.1991 (2021).\\n33. Ye, S., Mathis, A. & Mathis, M. W. Panoptic animal pose \\nestimators are zero-shot performers. Preprint at https://arxiv.org/\\nabs/2203.07436 (2022).\\n34. Marshall, J. D. et al. Continuous whole-body 3D kinematic \\nrecordings across the rodent behavioral repertoire. Neuron 109, \\n420–437 (2021).\\n35. Moore, J. D. et al. Hierarchy of orofacial rhythms revealed  \\nthrough whisking and breathing. Nature 497, 205–210  \\n(2013).\\n36. Kurnikova, A., Moore, J. D., Liao, S. -M., Deschênes, M. & Kleinfeld, D.  \\nCoordination of orofacial motor actions into exploratory behavior \\nby rat. Curr. Biol. 27, 688–696 (2017).\\n37. McAfee, S. S. et al. Minimally invasive highly precise monitoring of \\nrespiratory rhythm in the mouse using an epithelial temperature \\nprobe. J. Neurosci. Methods 263, 89–94 (2016).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 10, 'page_label': '1339'}, page_content='Nature Methods | Volume 21 | July 2024 | 1329–1339\\n 1339\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\n38. DeAngelis, B. D., Zavatone-Veth, J. A. & Clark, D. A.  \\nThe manifold structure of limb coordination in walking \\nDrosophila. Elife https://doi.org/10.7554/eLife.46409  \\n(2019).\\n39. Pereira, T. D. et al. Fast animal pose estimation using deep neural \\nnetworks. Nat. Methods 16, 117–125 (2019).\\n40. Dan, B. et al. Lightning Pose: improved animal pose estimation \\nvia semi-supervised learning, Bayesian ensembling, and \\ncloud-native open-source tools. Preprint at bioRxiv https://doi.\\norg/10.1101/2023.04.28.538703 (2023).\\n41. Batty, E. et al. In NeurIPS vol. 32 (eds H. Wallach et al.)  \\n(Curran Associates, 2019).\\n42. Berman, G. J., Bialek, W. & Shaevitz, J. W. Predictability and \\nhierarchy in Drosophila behavior. Proc. Natl Acad. Sci. USA 113, \\n11943–11948 (2016).\\n43. Berman, G. J. Measuring behavior across scales. BMC Biol. 16,  \\n23 (2018).\\nPublisher’s note Springer Nature remains neutral with regard to \\njurisdictional claims in published maps and institutional affiliations.\\nOpen Access This article is licensed under a Creative Commons \\nAttribution 4.0 International License, which permits use, sharing, \\nadaptation, distribution and reproduction in any medium or format, \\nas long as you give appropriate credit to the original author(s) and the \\nsource, provide a link to the Creative Commons licence, and indicate \\nif changes were made. The images or other third party material in this \\narticle are included in the article’s Creative Commons licence, unless \\nindicated otherwise in a credit line to the material. If material is not \\nincluded in the article’s Creative Commons licence and your intended use \\nis not permitted by statutory regulation or exceeds the permitted use, you \\nwill need to obtain permission directly from the copyright holder. To view \\na copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\\n© The Author(s) 2024'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 11, 'page_label': '1340'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nMethods\\nEthical compliance\\nAll experimental procedures were approved by the Harvard Medical \\nSchool Institutional Animal Care and Use Committee (protocol number \\n04930) and were performed in compliance with the ethical regulations \\nof Harvard University as well as the Guide for Animal Care and Use of \\nLaboratory Animals.\\nAnimal care and behavioral experiments\\nUnless otherwise noted, behavioral recordings were performed \\non 8–16-week-old C57/BL6 mice (The Jackson Laboratory stock no. \\n000664). Mice were transferred to our colony at 6–8 weeks of age \\nand housed in a reverse 12-h light/12-h dark cycle. We single-housed \\nmice after stereotactic surgery and group-housed them otherwise. \\nOn recording days, mice were brought to the laboratory, habituated \\nin darkness for at least 20\\u2009min, and then placed in the behavioral arena \\nfor 30–60\\u2009min. We recorded 6 male mice for 10 sessions (6\\u2009h) in the \\ninitial round of open field recordings; 5 male mice for 52 sessions (50\\u2009h) \\nduring the accelerometry recordings; 16 male mice for 16 sessions (8\\u2009h) \\nduring the environmental enrichment experiment; and 5 male mice \\nfor 9 sessions (6\\u2009h) during the thermistor recordings. The dopamine \\nphotometry recordings were obtained from a recent study 22. They \\ninclude 6 C57/BL6 mice and 8 DAT-IRES-cre (The Jackson Laboratory \\nstock no. 006660) mice of both sexes, recorded for 378 sessions. Of \\nthese, we selected a random subset of 95 sessions (~50\\u2009h) for bench -\\nmarking keypoint-MoSeq.\\nStereotactic surgery procedures\\nFor all stereotactic surgeries, mice were anesthetized using 1–2% iso-\\nflurane in oxygen, at a flow rate of 1\\u2009l min−1 for the duration of the pro-\\ncedure. Anteroposterior (AP) and mediolateral (ML) coordinates (in \\nmillimeters) were zeroed relative to bregma, and the dorsoventral (DV) \\ncoordinate was zeroed relative to the pial surface. All mice were moni-\\ntored daily for 4\\u2009days following surgery and were allowed to recover \\nfor at least 1 week. Mice were then habituated to handling and brief \\nhead-fixation before beginning recordings.\\nFor dopamine recordings, 400\\u2009nl of AAV5.CAG.dLight1.1 (Addgene, \\n111067; titer: 4.85\\u2009×\\u20091012) was injected at a 1:2 dilution into the DLS (AP \\n0.260; ML 2.550; DV −2.40), and a single 200-μm-diameter, 0.37–\\n0.57-NA fiber cannula was implanted 200\\u2009μm above the injection site \\n(see ref. 22 for additional details).\\nFor accelerometry recordings, we surgically attached a Mill-Max \\nconnector (DigiKey, ED8450-ND) and head bar to the skull and secured \\nit with dental cement (Metabond). A nine-degree-of-freedom absolute \\norientation IMU (Bosch, BN0055) was mounted on the Mill-Max con-\\nnector using a custom printed circuit board (PCB) with a net weight \\nbelow 1\\u2009g.\\nFor thermistor surgeries, we adapted a protocol previously \\ndescribed 37. We first prepared the implant (GAG22K7MCD419, TE \\nConnectivity) by stripping the leads and soldering them to two male \\nMill-Max pins (0.05-inch pitch, 851-93-050-10-001000). The pins and \\ntheir solder joins were then entirely covered in Prime-Dent light-curable \\ncement, and cured for 10–20\\u2009s, to ensure the longevity and stability of \\nthe electrical connection. Each implant was tested by touching two \\nleads of a multimeter (set to measure resistance) to the female side of \\nthe Mill-Max, breathing gently on the thermistor, and checking for a \\nresistance drop of roughly 20\\u2009kΩ to 18\\u2009kΩ.\\nT o implant the thermistor, a midline incision was made from ~1\\u2009mm \\nbehind lambda to ~1\\u2009mm anterior to the nasal suture, and the skull \\ncleaned and lightly scored. A craniotomy was made just anterior to \\nthe nasal suture (well posterior to the position originally reported37), \\nlarge enough for the thermistor to fit fully inside. The thermistor was \\nfully inserted along the AP axis so that it lay flat in the horizontal plane \\ninside the nasal cavity. The craniotomy was then sealed with KwikSil, \\nand the thermistor wire was secured to the skull 1–2\\u2009mm posterior to the \\ncraniotomy with cyanoacrylate glue (Loctite 454). Then dental cement \\n(Metabond) was used to attach the Mill-Max connector in an upright \\nposition between bregma and lambda, and a head bar was cemented \\nto the skull at lambda.\\nMicrosoft Azure recording setup\\nFor the initial set of open field recordings (Figs. 1 , 2, 3a–g and 5g–l), \\nmice were recorded in a square arena with transparent floor and walls \\n(30\\u2009cm length and width). Microsoft Azure Kinect cameras captured \\nsimultaneous depth and near-IR video at 30\\u2009Hz. Six cameras were used \\nin total: one above, one below and four side cameras at right angles at \\nthe same height as the mouse.\\nAccelerometry recordings\\nFor the accelerometry recordings, we used a single Microsoft Azure \\nKinect camera placed above the mouse, and an arena with transparent \\nfloor and opaque circular walls (45-cm diameter). Data were transferred \\nfrom the IMU using a lightweight tether attached to a custom-built \\nactive commutator. The IMU was connected to a T eensy microcon -\\ntroller, which was programmed using the Adafruit BNO055 library with \\ndefault settings (sample rate: 100\\u2009Hz, units: m/s2). T o synchronize the \\nIMU measurements and video recordings, we used an array of near-IR \\nLEDs to display a rapid sequence of random 4-bit codes that updated \\nthroughout the recording. The code sequence was later extracted from \\nthe behavioral videos and used to fit a piecewise linear model between \\ntimestamps from the videos and timestamps from the IMU.\\nThermistor recordings\\nT o record mouse respiration and movement at high frame rates, we \\nbuilt a multi-camera recording arena using six Basler ace acA1300-\\n200um Monochrome USB 3.0 Cameras (Edmund Optics, 33-978) that \\nrecorded from above, from below and four side views. The cameras \\nwere triggered at 120\\u2009Hz using an Arduino. Video compression was \\nperformed in real time on a GPU using a custom library (https://github.\\ncom/calebweinreb/multicamera_acquisition/). Mice were recorded in \\nan open-top glass cube and illuminated with 32 near-IR high-power LED \\nstars (LEDSupply, CREEXPE-FRD-3). T o avoid reflections and satura-\\ntions effects, the bottom camera was triggered slightly out of phase \\nwith the top cameras, and the LEDs were split into two groups: one \\ngroup below the arena that turned on during the bottom camera’s \\nexposure, and one group above the arena that turned on during the \\ntop and side cameras’ exposure.\\nT o record the thermistor signal, we designed a custom PCB that \\nused an op-amp (INA330AIDGST, T exas Instruments) to transform \\nthe thermistor’s resistance fluctuations into voltages, and another \\ncircuit element to keep the voltage within the 0–3.3\\u2009V range. The PCB \\nwas connected to an Arduino (separate from the one controlling the \\ncameras) that recorded the output. The PCB parts list, schematic and \\nmicrocontroller code are available upon reasonable request to the \\nlaboratory of S.R.D.\\nBefore behavioral recording sessions with the thermistor, mice \\nwere briefly head-fixed, and a cable with a custom headstage was \\ninserted into the head-mounted Mill-Max adaptor. The cable was \\ncommutated with an assisted electric commutator from Doric Lenses \\nand connected to the input of the op-amp on the custom PCB. T o syn-\\nchronize the thermistor and video data, we piped a copy of the camera \\ntrigger signal from the camera-Arduino to the thermistor-Arduino and \\nrecorded this signal alongside the thermistor output.\\nEnvironmental enrichment recordings\\nT o test the effects of environmental enrichment on behavior, we built \\nan arena for overhead video recording of an open-topped home cage. \\nThe home cage was surrounded on each side by a 16-inch vertical bar-\\nrier, illuminated from above by three near-IR LED starts (LEDSupply, \\nCREEXPE-FRD-3) and recorded with a Basler ace acA1300-200um'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 12, 'page_label': '1341'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nMonochrome USB 3.0 Camera (Edmund Optics 33-978). For half the \\nrecordings, the cage was filled with bedding, nesting material, chew \\nsticks and a transparent, dome-shaped hut. For the other half, the cage \\nwas completely empty (except for the mouse).\\nSoftware\\nThe following publicly available software packages were used for analy-\\nsis: Python (version 3.8), NumPy (version 1.24.3), Scikit-learn (version \\n1.2.2), PyT orch (version 1.9), Jax (version 0.3.22), SciPy (version 1.10.1), \\nMatplotlib (version 3.7.1), Statsmodels (version 0.13.5), Motionmap-\\nperpy (version 1.0), DeepLabCut (version 2.2.1), SLEAP (version 1.2.3), \\nB-SOiD (version 1.5.1), VAME (version 1.1), GIMBAL (version 0.0.1), \\nHRNet (unversioned), LightningPose (version 0.0.4) and segmenta-\\ntion_models_pytorch (version 0.3.3).\\nStatistics\\nAll reported P values for comparisons between distributions were \\nderived from Mann–Whitney U tests unless stated otherwise. In all \\ncomparisons to ‘shuffle’ , the shuffle represents a cyclic permutation \\nof the data.\\nProcessing depth videos\\nApplying MoSeq to depth videos involves: (1) mouse tracking and \\nbackground subtraction; (2) egocentric alignment and cropping; (3) \\nPCA; and (4) probabilistic modeling. We applied steps 2–4 as described \\nin the MoSeq2 pipeline25. For step 1, we trained a convolutional neural \\nnetwork (CNN) with a Unet++44 architecture to segment the mouse \\nfrom background using ~5,000 hand-labeled frames as training data.\\nKeypoint tracking for Microsoft Azure IR recordings\\nWe used CNNs with an HRNet45 architecture (https://github.com/ste-\\nfanopini/simple-HRNet/) with a final stride of two for pose tracking. \\nThe networks were trained on ~1,000 hand-labeled frames each for the \\noverhead, below-floor and side-view Microsoft Azure cameras. Frame \\nlabeling was crowdsourced through a commercial service (Scale AI). \\nThe crowdsourced labels were comparable to those from experts in our \\nlaboratory (Extended Data Fig. 1d). For the overhead camera, we tracked \\ntwo ears and six points along the dorsal midline (tail base, lumbar spine, \\nthoracic spine, cervical spine, head and nose). For the below-floor cam-\\nera, we tracked the tip of each forepaw, the tip and base of each hind paw, \\nand four points along the ventral midline (tail base, genitals, abdomen \\nand nose). For the side cameras, we tracked the same eight points as for \\nthe overhead camera and included the six limb points that were used for \\nthe below-floor camera (14 total). We trained a separate CNN for each \\ncamera angle. Target activations were formed by centering a Gaussian \\nwith a 10-pixel (px) standard deviation on each keypoint. We used the \\nlocation of the maximum pixel in each output channel of the neural net-\\nwork to determine keypoint coordinates and used the value at that pixel \\nto set the confidence score. The resulting mean absolute error (MAE) \\nbetween network detections and manual annotations was 2.9\\u2009px for the \\ntraining data and 3.2\\u2009px for held-out data. We also trained DeepLabCut \\nand SLEAP models on the overhead-camera and below-floor-camera \\ndatasets. For DeepLabCut, we used version 2.2.1, setting the architec-\\nture to resnet50 architecture and the ‘pos_dist_thresh’ parameter to \\n10, resulting in train and test MAEs of 3.4\\u2009px and 3.8\\u2009px, respectively. \\nFor SLEAP, we used version 1.2.3 with the baseline_large_rf.single.json \\nconfiguration, resulting in train and test MAEs of 3.5\\u2009px and 4.7\\u2009px. For \\nLightning Pose40, we used version 0.0.4 and default parameters with \\n‘pca_singleview’ and ‘temporal’ loss terms.\\nKeypoint tracking for thermistor recordings\\nWe trained separate keypoint detection networks for the Basler cam-\\nera arena (used for the thermistor recordings). CNNs with an HRNet \\narchitecture were trained on ~1,000 hand-labeled frames each for the \\noverhead and below-floor cameras and ~3,000 hand-labeled frames \\nfor the side-view cameras. The same keypoints were used as the ones \\nfor the Microsoft Azure dataset.\\n3D pose inference\\nUsing 2D keypoint detections from six cameras, 3D keypoint coordi-\\nnates were triangulated and then refined using GIMBAL, a model-based \\napproach that leverages anatomical constraints and motion continu-\\nity\\n29. T o fit GIMBAL, we computed initial 3D keypoint estimates using \\nrobust triangulation (that is, by taking the median across all camera \\npairs, as in 3D-DeepLabCut46) and then filtered to remove outliers \\nusing the EllipticEnvelope method from sklearn; we then fit the skel-\\netal parameters and directional priors for GIMBAL using expecta-\\ntion maximization with 50 pose states. Finally, we applied the fitted \\nGIMBAL model to each recording, using the following parameters \\nfor all keypoints: obs_outlier_variance\\u2009=\\u20091e6, obs_inlier_variance\\u2009=\\u200910, \\npos_dt_variance\\u2009=\\u200910. The latter parameters were chosen based on \\nthe accuracy of the resulting 3D keypoint estimates, as assessed from \\nvisual inspection. Camera calibration and initial triangulation were \\nperformed using a custom library (https://github.com/calebweinreb/\\nmulticam-calibration/tree/main/multicam_calibration/).\\nKeypoint change score\\nWe defined the keypoint ‘change score’ as the total velocity of key-\\npoints after egocentric alignment. The goal of the change score is to \\nhighlight sudden shifts in pose. It was calculated by: (1) transforming \\nkeypoints into egocentric coordinates; (2) smoothing the transformed \\ncoordinates with Gaussian kernel (sigma\\u2009=\\u20091 frame); (3) calculating total \\nchange in coordinates across each frame; and (4) z-scoring. Formally, \\nthe score can be defined as:\\nChangescore(t) = zscore(| yt −yt−1|)\\nwhere yt are the keypoint coordinates after Gaussian smoothing.\\nSpectral analysis of keypoint jitter\\nT o analyze keypoint jitter, we quantified the magnitude of fluctuations \\nacross a range of frequencies by computing a spectrogram for each \\nkeypoint along each coordinate axis. Spectrograms were computed \\nusing the python function scipy.signal.spectrogram with nperseg\\u2009=\\u2009128 \\nand noverlap\\u2009=\\u2009124. The spectrograms were then combined through \\naveraging: each keypoint was assigned a spectrogram by averaging \\nover the two coordinate axes, and the entire animal was assigned a \\nspectrogram by averaging over all keypoints.\\nWe used the keypoint-specific spectrograms to calculate \\ncross-correlations with −log10 (neural network detection confidence), \\nas well as the ‘error magnitude’ (Fig. 1g). Error magnitude was defined as \\nthe distance between the detected 2D location of a keypoint (based on a \\nsingle camera angle) and a re-projection of its 3D position (based on con-\\nsensus across six camera angles; see ‘3D pose inference’ above). We also \\ncomputed the cross-correlation between nose and tail-base fluctuations \\nat each frequency, as measured by the overhead and below-floor cam-\\neras, respectively. Finally, we averaged spectral power across keypoints \\nto compute the cross-correlation with model transition probabilities \\n(Fig. 1g). The model transition probabilities were defined for each frame \\nas the fraction of N\\u2009=\\u200920 model fits in which a transition occurred on that \\nframe. Formally, if z(i) denotes the syllable sequence learned by model \\nfit i, then the transition probability at time t is calculated as\\n1\\nN\\nN\\n∑\\ni=1\\nδ(z(i)\\nt ≠ z(i)\\nt−t)\\nApplying keypoint-MoSeq\\nDatasets were modeled separately and multiple models with different \\nrandom seeds were fit for each dataset (see Supplementary Table 1 for \\nnumber of fits per dataset).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 13, 'page_label': '1342'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nModeling consisted of two phases: (1) fitting an AR-HMM to a fixed \\npose trajectory derived from PCA of egocentric-aligned keypoints; and \\n(2) fitting a full keypoint-MoSeq model initialized from the AR-HMM. \\nReferences in the text to ‘MoSeq applied to keypoints’ or ‘MoSeq (key-\\npoints)’ , for example, in Figs. 1 and 2, refer to output of step 1. Both steps \\nare described below, followed by a detailed description of the model \\nand inference algorithm in the ‘mathematical notation’ section. In all \\ncases, we excluded rare states (frequency\\u2009<\\u20090.5%) from downstream \\nanalysis. We have made the code available as a user-friendly package \\nvia https://keypoint-moseq.readthedocs.io/en/latest/. With a con-\\nsumer GPU, keypoint-MoSeq requires 30–60\\u2009min of computation \\ntime to model 5\\u2009h of data. The computation time scales linearly with \\ndataset size.\\nFitting an initial AR-HMM\\nWe first modified the keypoint coordinates, defining keypoints with \\nconfidence below 0.5 as missing data and in imputing their values via \\nlinear interpolation, and then augmenting all coordinates with a small \\namount of random noise; the noise values were uniformly sampled \\nfrom the interval [−0.1, 0.1] and helped prevent degeneracy during \\nmodel fitting. Importantly, these preprocessing steps were only applied \\nduring AR-HMM fitting—the original coordinates were used when fit-\\nting the full keypoint-MoSeq model.\\nNext, we centered the coordinates on each frame, aligned them \\nusing the tail–nose angle, and then transformed them using PCA with \\nwhitening. The number of principal components (PCs) was chosen for \\neach dataset as the minimum required to explain 90% of total variance. \\nThis resulted in four PCs for the overhead-camera 2D datasets, six PCs \\nfor the below-floor camera 2D datasets and six PCs for the 3D dataset.\\nWe then used Gibbs sampling to infer the states and parameters \\nof an AR-HMM, including the state sequence z, the autoregressive \\nparameters A, b and Q, and the transition parameters π and β. The \\nhyperparameters for this step, listed in ‘mathematical notation’ \\nbelow, were generally identical to those in the original depth MoSeq \\nmodel. The one exception was the stickiness hyperparameter κ, which \\nwe adjusted separately for each dataset to ensure a median state  \\nduration of 400\\u2009ms.\\nFitting a full keypoint-MoSeq model\\nWe next fit the full set of variables for keypoint-MoSeq, which include \\nthe AR-HMM variables mentioned above, as well as the location v and \\nheading h, latent pose trajectory x, per-keypoint noise level σ2 and \\nper-frame/per-keypoint noise scale s. Fitting was performed using \\nGibbs sampling for 500 iterations, at which point the log joint prob-\\nability appeared to have stabilized.\\nThe hyperparameters for this step are enumerated in ‘mathemati-\\ncal notation’ . In general, we used the same hyperparameter values \\nacross datasets. The two exceptions were the stickiness hyperparam-\\neter κ, which again had to be adjusted to maintain a median state dura-\\ntion of 400\\u2009ms, and s0, which determines a prior on the noise scale. \\nBecause low-confidence keypoint detections often have high error, \\nwe set s0 using a logistic curve that transitions between a high-noise \\nregime (s0\\u2009=\\u2009100) for detections with low confidence and a low-noise \\nregime (s0 = 1) for detections with high confidence:\\ns0 = 1+100(1+e20(confidence−0.4))\\n−1\\nThe κ value used for each dataset is reported in Supplementary Table 2.\\nTrajectory plots\\nT o visualize the modal trajectory associated with each syllable (Fig. 2e), \\nwe (1) computed the full set of trajectories for all instances of all sylla-\\nbles, (2) used a local density criterion to identify a single representative \\ninstance of each syllable and (3) computed a final trajectory using the \\nnearest neighbors of the representative trajectory.\\nComputing the trajectory of individual syllable instances\\nLet yt, vt and ht denote the keypoint coordinates, centroid and heading \\nof the mouse at time t, and let F(v, h; y) denote the rigid transformation \\nthat egocentrically aligns y using centroid v and heading h. Given a \\nsyllable instance with onset time T, we computed the corresponding \\ntrajectory XT by centering and aligning the sequence of poses \\n(yT−5,…, yT+15) using the centroid and heading on time T. In other words\\nXT = [F(vT,hT;yT−5),…, F(vT,hT;yT+15)]\\nIdentifying a representative instance of each syllable\\nThe collection of trajectories computed above can be thought of as a \\nset of points in a high dimensional trajectory space (for K keypoints \\nin 2D, this space would have dimension 40K). Each point has a sylla-\\nble label, and the segregation of these labels in the trajectory space \\nrepresents the kinematic differences between syllables. T o capture \\nthese differences, we computed a local probability density function \\nfor each syllable, and a global density function across all syllables. \\nWe then selected a representative trajectory X for each syllable by \\nmaximizing the ratio:\\nLocaldensity(X)\\nGlobaldensity(X)\\nThe density functions were computed as the mean distance from each \\npoint to its 50 nearest neighbors. For the global density, the nearest \\nneighbors were selected from among all instances of all syllables. For \\nthe local densities, the nearest neighbors were selected from among \\ninstances of the target syllable.\\nComputing final trajectories for each syllable\\nFor each syllable and its representative trajectory X, we identified the \\n50 nearest neighbors of X from among other instances of the same \\nsyllable and then computed a final trajectory as the mean across these \\nnearest neighbors. The trajectory plots in Fig. 2e consist of ten \\nevenly-space poses along this trajectory, that is, the poses at times \\nT−5,T−3,…, T+13.\\nTesting robustness to missing data\\nT o test the ability of keypoint-MoSeq to infer syllables and sequences \\nin the face of missing data, we artificially ablated random subsets of \\nkeypoints at randomly timed intervals and then modeled the ablated \\ndata (Extended Data Fig. 2c–f). The ablation intervals began on every \\n10th second of the recording and lasted between 33\\u2009ms and 3\\u2009s (uni-\\nformly at random). For each interval, anywhere between 1 and 8 \\nkeypoints were selected (uniformly at random). Ablation entailed \\n(1) erasing the keypoint coordinates and then filling the gap by linear \\ninterpolation; (2) setting the corresponding confidence values to 0. \\nWe then applied keypoint-MoSeq 20 times with different random \\nseeds, using a single, fixed set of parameters derived previously \\nfrom standard model fitting on the unablated dataset. Fixing the \\nparameters ensured that syllable labels would be comparable across \\nrepeated model fits.\\nCross-syllable likelihoods\\nWe defined each cross-syllable likelihood as the probability (on aver-\\nage) that instances of one syllable could have arisen based on the \\ndynamics of another syllable. The probabilities were computed based \\non the discrete latent states zt, continuous latent states xt and autore-\\ngressive parameters A, b and Q output by keypoint-MoSeq. The \\ninstances I(n) of syllable n were defined as the set of all sequences \\n(ts,…, te) of consecutive timepoints such that zt\\u2009=\\u2009n for all ts ≤ t≤ te and \\nzts−1 ≠ n≠ zte+1. For each such instance, one can calculate the probability \\nP(xts,…, xte\\n||Am,bm,Qm) that the corresponding sequence of latent states'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 14, 'page_label': '1343'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\narose from the autoregressive dynamics of syllable m. The cross-syllable \\nlikelihood Cnm is defined in terms of these probabilities as\\nCnm = 1\\n|I(n)| ∑\\n(ts,…,te)∈I(n)\\n(xts,…, xte\\n||Am,bm,Qm)\\n(xts,…, xte\\n||An,bn,Qn)\\nGenerating synthetic keypoint data\\nT o generate the synthetic keypoint trajectories used for Extended Data \\nFig. 2h, we fit a linear dynamical system (LDS) to egocentrically aligned \\nkeypoint trajectories and then sampled randomly generated outputs \\nfrom the fitted model. The LDS was identical to the model underlying \\nkeypoint-MoSeq (see ‘mathematical notation’), except that it only had \\none discrete state, lacked centroid and heading variables and allowed \\nseparate noise terms for the x and y coordinates of each keypoint.\\nExpected marginal likelihood score\\nBecause keypoint-MoSeq can at best produce point estimates of the \\nmodel parameters—which will differ from run to run—users typically \\nrun the model several times and then rank the resulting fits. For ranking \\nmodel fits, we defined a custom metric called the expected marginal \\nlikelihood score. The score evaluates a given set of autoregressive \\nparameters (A, b, Q) by the expected value of the marginal log likeli-\\nhood: Ex∼P(x| y) logP(x|A,b,Q). In practice, given an ensemble of pose \\ntrajectories x(i) and parameters θ(i) =( A,b,Q) derived from N separate \\nMCMC chains, the scores are computed as:\\nScore(θ(i)) = 1\\n1−N∑\\nj≠i\\nlogP(x(j)|θ\\n(i)\\n)\\nThe scores shown in Extended Data Fig. 3j–m were computed using \\nan ensemble of N\\u2009=\\u200920 chains. We chose this custom score instead of a \\nmore standard metric (such as held-out likelihood) because computing \\nthe latter is intractable for the keypoint-MoSeq model.\\nEnvironmental enrichment analysis\\nWe fit a single keypoint-MoSeq model to the environmental enrichment \\ndataset, which included recordings in an enriched home cage and con-\\ntrol recordings in an empty cage. The transition graph (Extended Data \\nFig. 8b) was generated with keypoint-MoSeq’s analysis pipeline (https://\\nkeypoint-moseq.readthedocs.io/en/latest/analysis.html#  \\nsyllable-transition-graph/) using node positions from a force directed \\nlayout. Detection of differentially used syllables was also performed \\nusing the analysis pipeline, which applies a Kruskal–Wallis test for sig-\\nnificant differences in the per-session frequency of each syllable (https://\\nkeypoint-moseq.readthedocs.io/en/latest/analysis.html#  \\ncompare-between-groups/). Syllables were clustered into three  \\ngroups by applying community detection (networkx.community.lou-\\nvain_communities) to a complete graph where nodes are syllables and \\nedges were weighted by the bigram probabilities bij = P(zt = i, zt+1 = j)).\\nApplying published methods for behavior analysis\\nWe applied B-SOiD, VAME and MotionMapper using default param-\\neters, except for the parameter scans in Extended Data Fig. 5 (see Sup-\\nplementary Table 3 for a summary for all parameter choices). In general, \\nwe were unable to uniformly improve the performance of any method \\nby deviating from these default parameters. For example, switching \\nVAME’s state-partition method from hidden Markov model (HMM) to \\nk-means led to higher change score alignment (Extended Data Fig. 5a) \\nbut caused a decrease in alignment to supervised behavior labels \\n(Fig. 5e,f shows performance under an HMM; performance under \\nk-means is not shown). Our application of each method is described \\nin detail below.\\nB-SOiD is an automated pipeline for behavioral clustering that: (1) \\npreprocesses keypoint trajectories to generate pose and movement \\nfeatures; (2) performs dimensionality reduction on a subset of frames \\nusing uniform manifold approximation and projection; (3) clusters \\npoints in the uniform manifold approximation and projection space; \\nand (4) uses a classifier to extend the clustering to all frames12. We \\nfit B-SOiD separately for each dataset. In each case, steps 2–4 were \\nperformed multiple times with different random seeds (see Supple-\\nmentary Table 1 for number of fits per dataset), and the pipeline was \\napplied with standard parameters; 50,000 randomly sampled frames \\nwere used for dimensionality reduction and clustering, and the min_\\ncluster_size range was set to 0.5–1%. Because B-SOiD uses a hardcoded \\nwindow of 100\\u2009ms to calculate pose and movement features, we reran \\nthe pipeline with falsely inflated frame rates for the window-size scan \\nin Extended Data Fig. 5a. In all analyses involving B-SOiD, rare states \\n(frequency\\u2009<\\u20090.5%) were excluded from the analysis.\\nVAME is a pipeline for behavioral clustering that: (1) preprocesses \\nkeypoint trajectories and transforms them into egocentric coordi-\\nnates; (2) fits a recurrent neural network; (3) clusters the latent code \\nof the recurrent neural network13. We applied these steps separately to \\neach dataset, in each case running step 3 multiple times with different \\nrandom seeds (see Supplementary Table 1 for number of fits per data-\\nset). For step 1, we used the same parameters as in keypoint-MoSeq—\\negocentric alignment was performed along the tail–nose axis, and we \\nset the pose_confidence threshold to 0.5. For step 2, we set time_win-\\ndow\\u2009=\\u200930 and zdims\\u2009=\\u200930 for all datasets, except for the zdim-scan in \\nExtended Data Fig. 5a. VAME provides two different options for step \\n3: fitting an HMM (default) or applying k-means (alternative). We fit \\nan HMM for all datasets and additionally applied k-means to the initial \\nopen dataset. In general, we approximately matched the number of \\nstates/clusters in VAME to the number identified by keypoint-MoSeq, \\nexcept when scanning over state number in Extended Data Fig. 5a. \\nIn all analyses involving VAME, rare states (frequency\\u2009<\\u20090.5%) were \\nexcluded from analysis.\\nMotionMapper performs unsupervised behavioral segmentation \\nby: (1) applying a wavelet transform to preprocessed pose data; (2) \\nnonlinearly embedding the transformed data in 2D; and (3) cluster-\\ning the 2D data with a watershed transform17. We applied these steps \\nseparately to each dataset, in each case running steps 2–3 multiple \\ntimes with different random seeds (see Supplementary Table 1 for \\nnumber of fits per dataset). There are several published implementa-\\ntions of MotionMapper, which perform essentially the same set of \\ntransformations but differ in programming language. We obtained \\nsimilar results from a recent Python implementation from the Berman \\nlaboratory (https://github.com/bermanlabemory/motionmapperpy/) \\nand a published MATLAB implementation30. All results in the paper are \\nfrom the Python implementation, which we applied as follows. Data \\nwere first egocentrically aligned along the tail–nose axis and then \\nprojected into eight dimensions using PCA. T en log-spaced frequencies \\nbetween 0.25\\u2009Hz and 15\\u2009Hz were used for the wavelet transform, and \\ndimensionality reduction was performed using t-distributed stochastic \\nneighbor embedding. The threshold for watershedding was chosen to \\nproduce at least 25 clusters, consistent with keypoint-MoSeq for the \\noverhead-camera data. Rare states (frequency\\u2009<\\u20090.5%) were excluded \\nfrom analysis. For the parameter scan in Extended Data Fig. 5a, we var-\\nied each of these parameters while holding the others fixed, including \\nthe threshold for watershedding, the number of initial PCA dimensions, \\nand the frequency range of wavelet analysis. We also repeated a subset \\nof these analyses using an alternative autoencoder-based dimension-\\nality reduction approach, as described in the motionmapperpy tuto-\\nrial (https://github.com/bermanlabemory/motionmapperpy/blob/ \\nmaster/demo/motionmapperpy_mouse_demo.ipynb/).\\nPredicting kinematics from state sequences\\nWe trained decoding models based on spline regression to predict \\nkinematic parameters (height, velocity and turn speed) from state \\nsequences output by keypoint-MoSeq and other behavior segmentation'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 15, 'page_label': '1344'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nmethods (Fig. 3e and Extended Data Fig. 5c). Let zt represent an unsu-\\npervised behavioral state sequence and let B denote a spline basis, where \\nBt,i is the value of spline i and frame t. We generated such a basis using \\nthe ‘bs’ function from the Python package ‘patsy’ , passing in six \\nlog-spaced knot locations (1.0, 2.0, 3.9, 7.7, 15.2 and 30.0) and obtaining \\nbasis values over a 300-frame interval. This resulted in a 300-by-5 basis \\nmatrix B. The spline basis and state sequence were combined to form a \\n5N-dimensional design matrix, where N is the number of distinct behav-\\nioral states. Specifically, for each instance (ts,…, te) of state n (see \\n‘Cross-syllable likelihoods’ for a definition of state instances), we \\ninserted the first te −ts frames of B into dimensions 5n,…, 5n+5 of the \\ndesign matrix, aligning the first frame of B to frame ts in the design \\nmatrix. Kinematic features were regressed against the design matrix \\nusing Ridge regression from scikit-learn and fivefold cross-validation. \\nWe used a range of values from 10−3 to 103 for the regularization param-\\neter α and reported the results with greatest accuracy.\\nRearing analysis\\nT o compare the dynamics of rear-associated states across methods, \\nwe systematically identified all instances of rearing in our initial  \\nopen field dataset. During a stereotypical rear, mice briefly stood \\non their hind legs and extended their head upwards, leading to a  \\ntransient increase in height from its modal value of 3–5\\u2009cm to a peak \\nof 7–10\\u2009cm. Rears were typically brief, with mice exiting and then \\nreturning to a prone position within a few seconds. We encoded  \\nthese features using the following criteria. First, rear onsets were \\ndefined as increases in height from below 5\\u2009cm to above 7\\u2009cm that \\noccurred within the span of a second, with onset formally defined \\nas the first frame where the height exceeded 5\\u2009cm. Next, rear offsets \\nwere defined as decreases in height from above 7\\u2009cm to below 5\\u2009cm \\nthat occurred within the span of a second, with offset formally defined \\nas the first frame where the height fell below 7\\u2009cm. Finally, we defined \\ncomplete rears as onset–offset pairs defining an interval with length \\nbetween 0.5\\u2009s and 2\\u2009s. Height was determined from the distribution \\nof depth values in cropped, aligned and background-segmented \\nvideos. Specifically, we used the 98th percentile of the distribution \\nin each frame.\\nAccelerometry processing\\nFrom the IMU, we obtained absolute rotations ry, rp and rr (yaw, pitch and \\nroll) and accelerations ax, ay and az (dorsal/ventral, posterior/anterior \\nand left/right). T o control for subtle variations in implant geometry \\nand chip calibration, we centered the distribution of sensor readings \\nfor each variable on each session. We defined total acceleration as the \\nnorm of the three acceleration components:\\n|a| = √a2\\nx +a2\\ny +a2\\nz\\nSimilarly, we defined total angular velocity as the norm |ω| of rotation \\nderivative:\\nω= (\\ndry\\ndt ,\\ndrp\\ndt , drr\\ndt)\\nFinally, to calculate jerk, we smoothed the acceleration signal with a \\n50-ms Gaussian kernel, generating a time series ˜a, and then computed \\nthe norm of its derivative:\\nJerk= |||\\nd˜a\\ndt\\n|||\\nAligning dopamine fluctuations to behavior states\\nFor a detailed description of photometry data acquisition and pre-\\nprocessing, see ref. 22. Briefly, photometry signals were: (1) normal-\\nized using ΔF/F0 with a 5-s window; (2) adjusted against a reference to \\nremove motion artifacts and other non-ligand-associated fluctuations; \\n(3) z-scored using a 20-s sliding window; and (4) temporally aligned to \\nthe 30-Hz behavioral videos.\\nGiven a set of state onsets (either for a single state or across all \\nstates), we computed the onset-aligned dopamine trace by averaging \\nthe dopamine signal across onset-centered windows. From the result-\\ning traces, each of which can be denoted as a time series of dopamine \\nsignal values (d−T,…, dT), we defined the total fluctuation size (Fig. 4d) \\nand temporal asymmetry (Fig. 4e) as\\nTemporalasymmetry =\\n1\\n15\\n15\\n∑\\nt=0\\ndt −\\n1\\n15\\n0\\n∑\\nt=−15\\ndt\\nTotalfluctuationsize =\\n15\\n∑\\nt=−15\\n||dt|\\n|\\nA third metric—the average dopamine during each state (Extended \\nData Fig. 7b)—was defined simply as the mean of the dopamine signal \\nacross all frames bearing that state label. For each metric, shuffle distri-\\nbutions were generated by repeating the calculation with a temporally \\nreversed copy of the dopamine times series.\\nSupervised behavior benchmark\\nVideos and behavioral annotations for the supervised open field behav-\\nior benchmark (Fig. 5a–c) were obtained from ref. 31. The dataset \\ncontains 20 videos that are each 10–20-min long. Each video includes \\nframe-by-frame annotations of five possible behaviors: locomote, rear, \\nface groom, body groom and defecate. We excluded ‘defecate’ from the \\nanalysis because it was extremely rare (<0.1% of frames).\\nFor pose tracking, we used DLC’s SuperAnimal inference API that \\nperforms inference on videos without the need to annotate poses in \\nthose videos47. Specifically, we used SuperAnimal-T opViewMouse \\nthat applies DLCRNet-50 as the pose estimation model. Keypoint \\ndetections were obtained using DeepLabCut’s API function deep-\\nlabcut.video_inference_superanimal. The API function uses a pre-\\ntrained model called SuperAnimal-T opViewMouse and performs video \\nadaptation that applies multi-resolution ensemble (that is, the image \\nheight resized to 400, 500 and 600 with a fixed aspect ratio) and rapid \\nself-training (model trained on zero shot predictions with confidence \\nabove 0.1) for 1,000 iterations to counter domain shift and reduce \\njittering predictions.\\nKeypoint coordinates and behavioral annotations for the super-\\nvised social behavior benchmark (Fig. 5d–f) were obtained from the \\nCalMS21 dataset32 (task1). The dataset contains 70 videos of resident–\\nintruder interactions with frame-by-frame annotations of four pos-\\nsible behaviors: attack, investigate, mount or other. All unsupervised \\nbehavior segmentation methods were fitted to 2D keypoint data for \\nthe resident mouse.\\nWe used four metrics13 to compare supervised annotations and \\nunsupervised states from each method. These included NMI, homo-\\ngeneity, adjusted rand score and purity. All metrics besides purity \\nwere computed using the Python library scikit-learn (that is, with the \\nfunction normalized_mutual_info_score, homogeneity_score, adjusted_\\nrand_score). The purity score was defined as in ref. 13.\\nThermistor signal processing\\nDuring respiration, the movement of air through a mouse’s nasal cavity \\ngenerates fluctuations in temperature that can be detected by a ther-\\nmistor; temperature decreases during inhalations (because the mouse \\nis warmer than the air around it) and rises between inhalations. Below \\nwe refer to the between-inhalation intervals as ‘exhales’ but note that \\nthey may also contain pauses in respiration—pauses and exhales likely \\ncannot be distinguished because warming of the thermistor occurs \\nwhether or not air is flowing.\\nT o segment inhales and exhales using the thermistor signal, we first \\napplied a 60-Hz notch filter (scipy.signal.iirnotch, q\\u2009=\\u200910) and a low-pass'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 16, 'page_label': '1345'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nfilter (scipy.signal.butter, order\\u2009=\\u20093, cutoff\\u2009=\\u200940\\u2009Hz, analog\\u2009=\\u2009false) to \\nthe raw signal, and then used a median filter to subtract the slow DC \\noffset component of the signal. We then performed peak detection \\nusing scipy.signal.find_peaks (minimium inter-peak distance of 50\\u2009ms, \\nminimum and maximum widths of 10\\u2009ms and 1,500\\u2009ms, respectively). T o \\ndistinguish true peaks (inhalation onsets) from spurious peaks (noise), \\nwe varied the minimum prominence parameter from 10−4 to 1 while \\nkeeping other parameters fixed, and then used the value at which the \\nnumber of peaks stabilized. Using the chosen minimum prominence, \\nthe signal was then analyzed twice—once at the chosen value, and again \\nwith a slightly more permissive minimum prominence (1/8 of the chosen \\nvalue). Any low-amplitude breaths detected with the more permissive \\nsetting that overlapped with periods of breathing between 1\\u2009Hz and 6\\u2009Hz \\nwere added to the detections. This same process was then repeated to \\nfind exhale onsets but with the thermistor signal inverted. Finally, inhales \\nand exhales were paired, and any instances of two inhales/exhales in a \\nrow were patched by inserting an exhale/inhale at the local extremum \\nbetween them. Detections were then inspected manually, and any \\nrecordings with excessive noise, unusually high breathing rates (>14\\u2009Hz), \\nor unusual autocorrelation profiles were removed from further analyses.\\nClassifying sniff-aligned syllables\\nT o test whether syllables were significantly sniff aligned, we com-\\npared the probability of inhalation in the 50\\u2009ms before versus 50\\u2009ms \\nafter syllable onset. Specifically, for each syllable, we quantified the \\npre-inhalation versus post-inhalation fraction across all instances \\nof that syllable, and then compared the pre-distribution and \\npost-distribution values using a paired t-test. Syllables with P\\u2009<\\u20090.001 \\nwere considered significant.\\nFly gait analysis\\nFor the analysis of fly behavior, we used a published dataset of keypoint \\ncoordinates39, which were derived from behavioral videos originally \\nreported in ref. 17. The full dataset contains 1-h recordings (100\\u2009fps) \\nof single flies moving freely on a backlit 100-mm-diameter arena. Key-\\npoints were tracked using LEAP (test accuracy ~2.5\\u2009px). MotionMapper \\nresults (including names for each cluster) were also included in the \\npublished dataset. We chose four 1-h sessions (uniformly at random) for \\nanalysis with keypoint-MoSeq. All results reported here were derived \\nfrom this 4-h dataset.\\nThe analysis of syllable probabilities across the stride cycle \\n(Fig. 6i–k) was limited to periods of ‘fast locomotion’ , as defined by \\nthe MotionMapper labeling (state label 7). T o identify the start and end \\nof each stride cycle, we applied PCA to egocentric keypoint coordinates \\n(restricted to fast locomotion frames). We found that the first PC oscil-\\nlated in a manner reflecting the fly’s gait, and thus smoothed the first \\nPC using a one-frame Gaussian filter and performed peak detection \\non the smoothed signal. Each inter-peak interval was defined as one \\nstride. Stances and swings (Fig. 6j and Extended Data Fig. 10g) were \\ndefined by backward and forward motion of the leg tips, respectively \\n(in egocentric coordinates).\\nMathematical notation\\n1. χ−2(ν, τ2) denotes the scaled inverse Chi-squared distribution.\\n2. ⊗ denotes the Kronecker product.\\n3. ΔN is the N-dimensional simplex.\\n4. IN is the N\\u2009×\\u2009N identity matrix.\\n5. 1N\\u2009×\\u2009M is the N\\u2009×\\u2009M matrix of ones.\\n6. xt1∶t2 denotes the concatenation [xt1,xt1+1,…, xt2] where t1\\u2009<\\u2009t2.\\nGenerative model\\nKeypoint-MoSeq learns syllables by fitting an SLDS model48, which \\ndecomposes an animal’s pose trajectory into a sequence of stereotyped \\ndynamical motifs. In general, SLDS models explain time-series \\nobservations y1, …, yT through a hierarchy of latent states, including \\ncontinuous states xt ∈ℝ M that represent the observations y t in a \\nlow-dimensional space, and discrete states zt ∈ {1, …, N} that govern the \\ndynamics of xt over time. In keypoint-MoSeq, the discrete states cor-\\nrespond to syllables, the continuous states correspond to pose, and \\nthe observations are keypoint coordinates. We further adapted SLDS \\nby (1) including a sticky hierarchical Dirichlet prior (HDP); (2) explicitly \\nmodeling the animal’s location and heading; and (3) including a robust \\n(heavy-tailed) observation distribution for keypoints. Below we review \\nSLDS models in general and then describe each of the customizations \\nimplemented in keypoint-MoSeq.\\nSLDSs\\nThe discrete states zt ∈ {1, …, N} are assumed to form a Markov chain, \\nmeaning\\nzt+1|zt ∼ Cat(πzt)\\nwhere πi ∈ ΔN is the probability of transitioning from discrete state i to \\neach other state. Conditional on the discrete states zt, the continuous \\nstates xt follow an L-order vector autoregressive process with Gaussian \\nnoise. This means that the expected value of each xt is a linear function \\nof the previous L states xt−L∶t−1, as shown below\\nxt|zt,xt−L∶t−1∼𝒩𝒩(Aztxt−L∶t−1 +bzt,Qzt)\\nwhere Ai ∈ℝ M×LM is the autoregressive dynamics matrix, bi ∈ℝ M is the \\ndynamics bias vector, and Qi ∈ℝ M×M is the dynamics noise matrix for \\neach discrete state i\\u2009=\\u20091, …, N. The dynamics parameters Ai, bi and Qi \\nhave a matrix normal inverse Wishart (MNIW) prior\\n[Ai|bi],Qi ∼ MNIW(ν0,S0,M0,K0)\\nwhere ν0\\u2009>\\u2009M\\u2009−\\u20091 is the degrees of freedom, S0 ∈ℝ M×M is the prior covari-\\nance matrix, M0 ∈ℝ M×(LM+1) is the prior mean dynamics matrix, and \\nK0 ∈ℝ (LM+1)×(LM+1) is the prior scale matrix. Finally, in the standard for-\\nmulation of SLDS (which we modify for keypoint data, as described \\nbelow), each observation yt ∈ℝ D is a linear function of xt plus noise:\\nyt|zt,xt∼𝒩𝒩(Cxt +d,S)\\nHere we assume that the observation parameters C, d and S do not \\ndepend on zt.\\nSticky HDP\\nA key feature of depth Moseq is the use of a sticky-HDP prior for the \\ntransition matrix. In general, HDP priors allow the number of distinct \\nstates in a HMM to be inferred directly from the data. The ‘sticky’ variant \\nof the HDP prior includes an additional hyperparameter κ that tunes \\nthe frequency of self-transitions in the discrete state sequence zt, and \\nthus the distribution of syllable durations. As in depth MoSeq, we \\nimplement a sticky-HDP prior using the weak limit approximation49, \\nas shown below:\\nβ ∼ Dir(γ/N,…, γ/N)\\nπi|β ∼ Dir(αβ1,…, αβv +κ…, αβN)\\nwhere κ is being added in the ith position. Here β∈ ΔN is a global  \\nvector of augmented syllable transition probabilities, and the hyper-\\nparameters γ, α and κ control the sparsity of states, the weight of the \\nsparsity prior and the bias toward self-transitions, respectively.\\nSLDS for postural dynamics\\nKeypoint coordinates reflect not only the pose of an animal, but also \\nits location and heading. T o disambiguate these factors, we define a'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 17, 'page_label': '1346'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\ncanonical, egocentric reference frame in which the postural dynamics \\nare modeled. The canonically aligned poses are then transformed into \\nglobal coordinates using explicit centroid and heading variables that \\nare learned by the model.\\nConcretely, let Yt ∈ℝ K×D represent the coordinates of K keypoints \\nat time t , where D∈{ 2,3}. We define latent variables vt ∈ℝ D and \\nht ∈ [0,2π] to represent the animal’s centroid and heading angle. We \\nassume that each heading angle ht has an independent, uniform prior \\nand that the centroid is autocorrelated as follows:\\nht ∼ Unif(0,2π)\\nvt|vt−1 ∼𝒩𝒩 (vt−1,σ2\\nloc)\\nAt each time point t, the pose Yt is generated via rotation and translation \\nof a centered and oriented pose ̃Yt that depends on the current continu-\\nous latent state xt:\\nYt = ̃YtR(ht)+1 Kv⊤\\nt wherevec( ̃Yt) ∼ 𝒩𝒩((Γ ⊗ID)(Cxt +d),St)\\nwhere R(ht) is a matrix that rotates by angle ht in the xy plane, and \\nΓ ∈ RK×(K−1) is defined by the truncated singular value decomposition \\nΓΔΓ⊤ = IK −1K×K/K. Note that Γ encodes a linear transformation that \\nisometrically maps ℝ(K−1)×D to the set of all centered keypoint arrange-\\nments in ℝK×D, and thus ensures that 𝔼𝔼( ̃Yt) is always centered50. The \\nparameters C∈ℝ (K−1)D×M and d ∈ℝ (K−1)D are initialized using PCA  \\napplied to the transformed keypoint coordinates ΓT ̃Yt. In principle  \\nC and d can be adjusted further during model fitting, and we describe \\nthe corresponding Gibbs updates in the inference section below. In \\npractice, however, we keep C and d fixed to their initial values when \\nfitting keypoint-MoSeq.\\nRobust observations\\nT o account for occasional large errors during keypoint tracking, we \\nuse the heavy-tailed Student’s t-distribution, which corresponds to a \\nnormal distribution whose variance is itself a random variable. Here, \\nwe instantiate the random variances explicitly as a product of two \\nparameters: a baseline variance σk for each keypoint and a time-varying \\nscale st,k. We assume:\\nσ2\\nk ∼ χ−2(νσ,σ2\\n0)\\ns2\\nt,k ∼ χ−2(νs,s0,t,k)\\nwhere νσ\\u2009>\\u20090 and νs\\u2009>\\u20090 are degrees of freedom, σ2\\n0 > 0 is a baseline scal-\\ning parameter, and s0,t,k > 0 is a local scaling parameter, which encodes \\na prior on the scale of error for each keypoint on each frame. Where \\npossible, we calculated the local scaling parameters as a function of \\nthe neural network confidences for each keypoint. The function was \\ncalibrated using the empirical relationship between confidence values \\nand error sizes. The overall noise covariance St is generated from σk and \\nst,k as follows:\\nSt = diag(σ2\\n1s2\\nt,1\\n,…, σ2\\nKs2\\nt,K\\n)⊗ID\\nRelated work\\nKeypoint-MoSeq extends the model used in depth MoSeq16, where \\na low-dimensional pose trajectory xt (derived from egocentrically \\naligned depth videos) is used to fit an AR-HMM with a transition \\nmatrix π, autoregressive parameters Ai, bi and Qi and discrete states \\nzt like those described here. Indeed, conditional on xt, the models for \\nkeypoin-MoSeq and depth MoSeq are identical. The main differences \\nare that keypoint-MoSeq treats x\\nt as a latent variable (that is, updates \\nit during fitting), includes explicit centroid and heading variables, and \\nuses a robust noise model.\\nDisambiguating poses from position and heading is a common task \\nin unsupervised behavior algorithms, and researchers have adopted a \\nvariety of approaches. VAME13, for example, isolates pose by centering \\nand aligning data ahead of time, whereas B-SOiD12 transforms the key-\\npoint data into a vector of relative distances and angles. The statistical \\npose model GIMBAL29, on the other hand, introduces latent heading \\nand centroid variables that are inferred simultaneously with the rest \\nof the model. Keypoint-MoSeq adopts this latter approach, which can \\nremove spurious correlations between egocentric features that can \\narise from errors in keypoint localization.\\nInference algorithm\\nOur full model contains latent variables v, h, x, z and s and parameters \\nA, b, Q, C, d, σ, β and π. We fit each of these variables—except for C and \\nd—using Gibbs sampling, in which each variable is iteratively resampled \\nfrom its posterior distribution conditional on the current values of all \\nthe other variables. The posterior distributions P(π, β∣z) and P(A, b, \\nQ∣z, x) are unchanged from the original MoSeq paper and will not be \\nbe reproduced here (see ref. 16, pages 42–44, and note the changes of \\nnotation Q → Σ, z → x and x → y). The Gibbs updates for variables C, d, σ, \\ns, v and h are described below.\\nResampling P(C, d∣s, σ, x, v, h, Y). Let ̃xt represent xt with a 1 appended \\nand define\\ñSt =( Γ⊤diag(σ2\\n1 st,1,…, σ2\\nKst,K)Γ)⊗ ID\\nThe posterior update is (C,d)∼𝒩𝒩(vec(C,d)|μn,Σn) where\\nΣn =( σ−2\\nC I+Sx,x)\\n−1\\nandμn = ΣnSy,x\\nwith\\nSx,x =\\nT\\n∑\\nt=1\\ñxt ̃x⊤\\nt ⊗Γ⊤ ̃S\\n−1\\nt Γ ⊗IDandSy,x =\\nT\\n∑\\nt=1\\n( ̃x⊤\\nt ⊗ ̃S\\n−1\\nΓ ⊗ID)vec( ̃Yt)\\n⊤\\nResampling P(s∣C, d, σ, x, v, h, Y). Each st,k is conditionally independ-\\nent with posterior\\nst,k|C,d,σk,x,Y∼ χ−2(νs +D,(νss0 +σ−2\\nk ∥ (Γ(Cxt +d))\\nk\\n− ̃Yt,k∥2)/(νs +D))\\nResampling P(σ∣C, d, s, x, v, h, Y). Each σk is conditionally independ-\\nent with posterior\\nσ2\\nk ∼ χ−2(νσ +DT,(νσσ2\\n0 +Sy)(νσ +DT)\\n−1\\n)\\nwhere Sy = ∑\\nN\\nt=1‖Γ(Cxt +d)k − ̃Yt,k‖\\n2\\n/st,k\\nResampling P(v∣C, d, σ, s, x, h, Y). Because the translations v1, …, vT \\nform an LDS, they can be updated by Kalman sampling. The observation \\npotentials have the form 𝒩𝒩(vt|μ,γ2ID) where\\nμ= ∑\\nk\\nγ2\\nt\\nσ2\\nkst,k\\n[Yt,k −R(ht)\\n⊤\\nΓ(Cxt +d)k], 1\\nγ2\\nt\\n= ∑\\nk\\n1\\nσ2\\nkst,k\\nResampling P(h∣C, d, σ, s, x, v, Y). The posterior of ht is the von-Mises \\ndistribution vM(θ, κ) where κ and θ∈ [0,2π] are the unique parameters \\nsatisfying [κcos(θ),κsin(θ)] = [S1,1 +S2,2,S1,2 −S2,1] for\\nS= ∑\\nk\\n1\\nst,kσ2\\nk\\nΓ(Cxt +d)k(Yt,k −vt)\\n⊤\\nResampling P(x∣C, d, σ, s, v, h, Y). T o resample x, we first express its \\ntemporal dependencies as a first-order autoregressive process, and \\nthen apply Kalman sampling. The change of variables is'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 18, 'page_label': '1347'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nA′ =\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\nI\\nI\\nI\\nA1 A2 … AL b\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\nQ′ =\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n0\\n0\\n0\\nQ\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\nC′ =\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\n00\\n⋮⋮\\n00\\nC d\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\nx′\\nt =\\n⎡\\n⎢\\n⎢\\n⎢\\n⎢\\n⎢\\n⎣\\nxt−L+1\\n⋮\\nxt\\n1\\n⎤\\n⎥\\n⎥\\n⎥\\n⎥\\n⎥\\n⎦\\nKalman sampling can then be applied to the sample the conditional \\ndistribution\\nP(x′\\n1∶T| ̃Y1∶T)∝\\nT\\n∏\\nt=1\\n𝒩𝒩(x′\\nt|A′(zt)\\nx′\\nt−1,Q′(zt)\\n)𝒩𝒩(vec( ̃Yt)|C′x′\\nt,St).\\n(Assume x′ is left-padded with zeros for negative time indices.)\\nHyperparameters\\nWe used the following hyperparameter values throughout the paper.\\nTransition matrix. \\nN= 100\\nγ= 1,000\\nα= 100\\nκ fittoeachdataset\\nAutoregressive process. \\nM setusingPCAexplainedvariancecurve\\nL= 3\\nν0 = M+2\\nS0 = 0.01IM\\nM0 =[ 0M×(L−1) IM1M×1]\\nK0 = 10IM(L+1)\\nObservation process. \\nσ2\\n0 = 1\\nνσ = 10\\n5\\nνs = 5\\ns0,t,ksetbasedonneuralnetworkconfidence\\nCentroid autocorrelation. \\nσ2\\nloc = 0.4\\nDerivation of Gibbs updates\\nDerivation of C, d updates. T o simply notation, define\\ñSt = diag(σ2\\n1st,1,…, σ2\\nKst,K), ̃xt =( xt,1), ̃C=( C,d)\\nThe likelihood of the centered and aligned keypoint locations ̃Y \\ncan be expanded as follows\\nP( ̃Y| ̃C, ̃x, ̃S) =\\nT\\n∏\\nt=1\\n𝒩𝒩(vec( ̃Yt)|(Γ ⊗ID) ̃C ̃xt, ̃St ⊗ID)\\n∝ exp[−\\n1\\n2\\nT\\n∑\\nt=1\\n( ̃x⊤\\nt ̃C\\n⊤\\n(Γ⊤ ̃S\\n−1\\nt Γ ⊗ID) ̃C ̃xt −2vec( ̃Yt)\\n⊤\\n( ̃S\\n−1\\nt\\nΓ ⊗ID) ̃C ̃xt)]\\n∝ exp[−\\n1\\n2\\nT\\n∑\\nt=1\\n(vec( ̃C)\\n⊤\\n( ̃xt ̃x⊤\\nt ⊗Γ⊤ ̃S\\n−1\\nt Γ ⊗ID)vec( ̃C))\\n(−2vec( ̃C)\\n⊤\\n( ̃x⊤\\nt ⊗ ̃S\\n−1\\nt Γ ⊗ID)vec( ̃Yt))]\\n∝ exp[−\\n1\\n2\\n(vec( ̃C)\\n⊤\\nSx,xvec( ̃C)−2vec( ̃C)\\n⊤\\nSx,y)]\\nwhere\\nSx,x =\\nT\\n∑\\nt=1\\ñxt ̃x⊤\\nt ⊗Γ⊤ ̃S\\n−1\\nt Γ ⊗ID andSx,y =\\nT\\n∑\\nt=1\\n( ̃x⊤\\nt ⊗ ̃S\\n−1\\nΓ ⊗ID)vec( ̃Yt)\\nMultiplying by the prior vec( ̃C)∼𝒩𝒩(0,σ2\\nCI) yields\\nP( ̃C| ̃Y, ̃x, ̃S)∝𝒩𝒩( vec( ̃C)|μn,Σn)\\nwhere\\nΣn = (σ−2\\nC I+Sx,x)\\n−1\\nandμn = ΣnSy,x\\nDerivation of σ k, st,k updates.  For each time t and keypoint k, let \\n̄Yt,k = Γ (Cxt +d). The likelihood of the centered and aligned keypoint \\nlocation ̃Yt,k is\\nP( ̃Yt,k| ̄Yt,k,st,k,σk)=𝒩𝒩( ̃Yt,k| ̄Yt,k, σ2\\nkst,kID)∝( σ2\\nk\\nst,k)\\n−D/2\\nexp[−\\n‖ ̃Yt,k − ̄Yt,k‖\\n2\\n2σ2\\nkst,k\\n]\\nWe can then calculate posteriors P(st,k|σk) and P(σk|st,k) as \\nfollows\\nP(st,k|σk, ̃Yt,k, ̄Yt,k) ∝ χ−1(st,k|νs,s0)𝒩𝒩( ̃Yt,k| ̄Yt,k, σ2\\nkst,kID)\\n∝ s−1−(νs+D)/2\\nt,k exp[\\n−νss0\\n2st,k\\n−\\n∥ ̃Yt,k− ̄Yt,k∥2\\n2σ2\\nkst,k\\n]\\n∝ χ−2(st,k|νs +D,(νss0 +σ−2\\nk ∥ ̃Yt,k − ̄Yt,k∥2)(νs +D)\\n−1\\n)\\nP(σk|{st,k, ̃Yt,k, ̄Yt,k}\\nT\\nt=1) ∝ χ−1(σ2\\nk|νσ,σ2\\n0)\\nT\\n∏\\nt=1\\n𝒩𝒩( ̃Yt,k| ̄Yt,k,σ2\\nkst,kID)\\n∝ σ−2−νσ−DT\\nk exp[\\n−νσσ2\\n0\\n2σ2\\nk\\n−\\n1\\n2σ2\\nk\\nT\\n∑\\nt=1\\n∥ ̃Yt,k− ̄Yt,k∥2\\nst,k\\n]\\n∝ χ−2(σ2\\nk|νσ +DT,(νσσ2\\n0 +Sy)(νσ +DT)\\n−1\\n)\\nwhere Sy = ∑t ∥ ̃Yt,k − ̄Yt,k∥2/st,k\\nDerivation of v t update. We assume an improper uniform prior on \\nvt, hence\\nP(vt|Yt) ∝ P(Yt|vt)P(vt) ∝ P(Yt|vt)\\n∝𝒩𝒩(vec((Yt −1Kv⊤\\nt )R(ht)\\n⊤\\n)|Γ (Cxt +d),St)\\n= ∏\\nk\\n𝒩𝒩(R(ht)(Yt,k −vt)|Γ(Cxt +d)k,st,kσ2\\nkID)\\n= ∏\\nk\\n𝒩𝒩(vt|Yt,k −R(ht)\\n⊤\\nΓ(Cxt +d)k,st,kσ2\\nkID)\\n=𝒩𝒩(vt|μt,γ2\\ntID)\\nwhere\\nμ= ∑\\nk\\nγ2\\nt\\nσ2\\nkst,k\\n(Yt,k −R(ht)\\n⊤\\nΓ(Cxt +d)k),\\n1\\nγ2\\nt\\n= ∑\\nk\\n1\\nσ2\\nkst,k\\nDerivation of ht update. We assume a proper uniform prior on ht, hence\\nP(ht|Yt) ∝ P(Yt|ht)P(ht) ∝ P(Yt|ht)\\n∝ exp[∑k\\n(Yt,k−vt)\\n⊤\\nR(ht)Γ(Cxt+d)k\\nst,kσ2\\nk\\n]\\n= exp[\\ntr[R(ht)Γ(Cxt+d)k(Yt,k−vt)\\n⊤\\n]\\nst,kσ2\\nk\\n]\\n∝ exptr[R(ht)S] whereS= ∑\\nk\\nΓ(Cxt +d)k(Yt,k −vt)\\n⊤\\n/(st,kσ2\\nk)\\n∝ exp[cos(ht)(S1,1 +S2,2)+sin(ht)(S1,2 −S2,1)]'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 19, 'page_label': '1348'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nLet [κcos(θ),κsin(θ)] represent [S1,1 +S2,2,S1,2 −S2,1] in polar coordi-\\nnates. Then\\nP(Yt|ht) ∝ exp[κcos(ht)cos(θ)+sin(ht)sin(θ)]\\n= exp[κcos(ht −θ)] ∝ vM(ht|θ,κ)\\nReporting summary\\nFurther information on research design is available in the Nature \\nPortfolio Reporting Summary linked to this article.\\nData availability\\nThis study used the following publicly available datasets: CalMS21 \\n(https://data.caltech.edu/records/s0vdx-0k302)32; DeepEthogram \\nbenchmark data31 (https://github.com/jbohnslav/deepethogram/); \\nRat7M (https://doi.org/10.6084/m9.figshare.c.5295370.v3)51; and fly \\nkeypoint tracking (https://doi.org/10.1038/s41592-018-0234-5). Other \\ndata raw data generated in this study have been deposited in Zenodo \\n(https://doi.org/10.5281/zenodo.10636983)52. The thermistor record-\\nings generated for this study are not publicly available at this time as \\nthey are being used for a follow-up paper. We plan to make these data \\npublicly accessible upon publication of the follow-up study and in the \\nmeantime will provide them upon reasonable request.\\nCode availability\\nSoftware links and user support for both depth and keypoint data are \\navailable at http://www.moseq4all.org/. Data loading, project configu-\\nration and visualization are enabled through the keypoint-moseq53 \\nPython library (https://github.com/dattalab/keypoint-moseq/). We \\nalso developed a stand-alone library called jax-moseq54 for core model \\ninference (https://github.com/dattalab/jax-moseq/). Both libraries \\nare freely available to the research community under an academic \\nand non-commercial research use license. This license permits free \\nacademic and non-commercial use, explicitly prohibits redistribution \\nand commercial use, and requires users to agree to terms including \\nlimitations on liability and indemnity. Full license details can be viewed \\non the respective GitHub repository pages.\\nReferences\\n44. Zhou, Z., et al. UNet++: a nested U-net architecture for medical \\nimage segmentation. in (eds Stoyanov, D. et al.) Deep Learning  \\nin Medical Image Analysis and Multimodal Learning for  \\nClinical Decision Support. DLMIA ML-CDS 2018. Lecture  \\nNotes in Computer Science, vol 11045, 3–11 (Springer \\nInternational Publishing, 2018). https://doi.org/10.1007/978- \\n3-030-00889-5_1\\n45. Sun, K., Xiao, B., Liu, D. & Wang, J. Deep high-resolution \\nrepresentation learning for human pose estimation. in 2019 IEEE/\\nCVF Conference on Computer Vision and Pattern Recognition \\n(CVPR). 5686–5696 (2019).\\n46. Nath, T. et al. Using DeepLabCut for 3D markerless pose \\nestimation across species and behaviors. Nat. Protoc. 14, \\n2152–2176 (2019).\\n47. Ye, S. et al. SuperAnimal pretrained pose estimation models for \\nbehavioral analysis. Preprint at https://arxiv.org/abs/2203.07436 \\n(2023).\\n48. Ackerson, G. A. & Fu, K.-S. On state estimation in  \\nswitching environments. IEEE Trans. Autom. Control. 15,  \\n10–17 (1970).\\n49. Fox, E. B., Sudderth, E. B., Jordan, M. I. & Willsky, A. S. A sticky \\nHDP-HMM with application to speaker diarization. Ann. Appl. Stat. \\n5, 1020–1056 (2009).\\n50. Andreella, A. & Finos, L. Procrustes analysis for high-dimensional \\ndata. Psychometrika 87, 1422–1438 (2022).\\n51. Marshall, J. D. et al. Rat 7M. figshare https://doi.org/10.6084/\\nm9.figshare.c.5295370.v3 (2021).\\n52. Weinreb, C. et al. Keypoint-MoSeq: parsing behavior by linking \\npoint tracking to pose dynamics. Zenodo https://doi.org/10.5281/\\nzenodo.10636983 (2024).\\n53. Weinreb, C. et al. dattalab/keypoint-moseq: Keypoint  \\nMoSeq 0.4.3. Zenodo https://doi.org/10.5281/zenodo.10524840 \\n(2024).\\n54. Weinreb, C. et al. dattalab/jax-moseq: JAX MoSeq 0.2.1.  \\nZenodo https://doi.org/10.5281/zenodo.10403244  \\n(2023).\\nAcknowledgements\\nS.R.D. is supported by National Institutes of Health (NIH) grants \\nRF1AG073625, R01NS114020 and U24NS109520, the Simons \\nFoundation Autism Research Initiative and the Simons Collaboration \\non Plasticity and the Aging Brain. S.R.D. and S.W.L. are supported by \\nNIH grant U19NS113201 and the Simons Collaboration on the Global \\nBrain. C.W. is a Fellow of the Jane Coffin Childs Memorial Fund for \\nMedical Research. W.F.G. is supported by NIH grant F31NS113385.  \\nM.J. is supported by NIH grant F31NS122155. S.W.L. is supported by the \\nAlfred P. Sloan Foundation. T.P. is supported by a Salk Collaboration \\nGrant. We thank J. Araki for administrative support; the HMS Research \\nInstrumentation Core, which is supported by the Bertarelli Program \\nin Translational Neuroscience and Neuroengineering and by NEI \\ngrant EY012196; and members of the laboratory of S.R.D. for useful \\ncomments on the paper. Portions of this research were conducted \\non the O2 High Performance Compute Cluster at Harvard Medical \\nSchool. Mouse illustrations were downloaded from https://www.\\nscidraw.io/.\\nAuthor contributions\\nC.W. and S.R.D. conceived the project and designed the experiments. \\nC.W. and S.W.L. designed the algorithm. C.W. implemented the \\nalgorithm with contributions from S.L., M.A.M.O., L.Z. and T.P. C.W. and \\nJ.P. collected data and S.M., W.F.G., M.J., S.A., E.C. and R.H. assisted. \\nC.W., J.P., M.A.M.O., Y.S., A.M., M.W.M. and T.P. performed analyses. \\nC.W. and S.R.D. wrote the manuscript with input from all authors. S.R.D. \\nsupervised the project.\\nCompeting interests\\nS.R.D. sits on the scientific advisory boards of Neumora and Gilgamesh \\nTherapeutics, which have licensed or sub-licensed the MoSeq \\ntechnology. The other authors declare no competing interests.\\nAdditional information\\nExtended data is available for this paper at  \\nhttps://doi.org/10.1038/s41592-024-02318-2.\\nSupplementary information The online version  \\ncontains supplementary material available at  \\nhttps://doi.org/10.1038/s41592-024-02318-2.\\nCorrespondence and requests for materials should be addressed to \\nScott W. Linderman or Sandeep Robert Datta.\\nPeer review information Nature Methods thanks  \\nMatthew Smear and the other, anonymous, reviewer(s) for their \\ncontribution to the peer review of this work. Primary Handling  \\nEditor: Nina Vogt, in collaboration with the Nature Methods  \\nteam.\\nReprints and permissions information is available at  \\nwww.nature.com/reprints.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 20, 'page_label': '1349'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 1 | Markerless pose tracking exhibits fast fluctuations \\nthat are independent of behavior yet affect MoSeq output. a) Example of a \\n5-second interval during which the mouse is still yet the keypoint coordinates \\nfluctuate, as shown in Fig. 1e, but here for SLEAP and DeepLabCut respectively. \\nLeft:  egocentrically aligned keypoint trajectories. Right: path traced by each \\nkeypoint during the 5-second interval. b) Cross-correlation between the spectral \\ncontent of keypoint fluctuations and either error magnitude (left) or a measure \\nof low-confidence keypoint detections (right). c) Magnitude of fast fluctuations \\nin keypoint position for three different tracking methods, calculated as the \\nper-frame distance from the detected trajectory of a keypoint to a smoothened \\nversion of the same trajectory, where smoothing was performed using a gaussian \\nkernel with width 100ms (N=4 million keypoint detections). d) Inter-annotator \\nvariability, shown as the distribution of distances between multiple annotations \\nof the same keypoint. Annotations were either crowd-sourced or obtained from \\nexperts (N=200 frames and N=4 labelers). e) Train- and test- error distributions \\nfor each keypoint tracking method (N=800 held out keypoint annotations). \\nf ) T op: position of the nose and tail-base over a 10-second interval, shown for \\nboth the overhead and below-floor cameras. Bottom: fast fluctuations in each \\ncoordinate, obtained as residuals after median filtering. g) Cross-correlation \\nbetween spectrograms obtained from two different camera angles for either \\nthe tail base or the nose, shown for each tracking method. h) Cross-correlation \\nof transitions rates, comparing MoSeq applied to depth and MoSeq applied to \\nkeypoints with various levels of smoothing using a low-pass, Gaussian, or median \\nfilter (N=1 model fit per filtering parameter).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 21, 'page_label': '1350'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 2 | See next page for caption.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 22, 'page_label': '1351'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 2 | Keypoint-MoSeq is robust to noise and missing data. \\na) Mean change score values at syllable transitions. Syllables were either derived \\nfrom keypoint-MoSeq applied to (unfiltered) keypoints from our custom neural \\nnetwork, or from traditional MoSeq applied to several versions of the keypoint \\ndata, including keypoints inferred from Lightning Pose, or keypoints from \\nour custom neural network followed by low-pass filtering, median filtering, or \\nno filtering. Error bars show standard deviation across N=20 model fits. The \\nchange scores are highest for keypoint-MoSeq (P < 10\\n−4 over N=20 model fits, \\nMann-Whitney U test). b) Correlations of transition probabilities (that is, the \\nprobability of a new syllable starting at each frame), comparing depth MoSeq \\nwith each of the keypoint models shown in (a). c) Example of model responses \\nto a one-second-long ablation of keypoint observations, shown for keypoint-\\nMoSeq (right) and traditional AR-HMM-based MoSeq (left). T op: Change in \\nsyllable sequences. Each heatmap row represents an independent modeling \\nrun and each column represents a frame. The set of labels on each frame define \\na distribution, and the Kullback-Leibler divergence (KL div.) between the \\nablated and unablated distributions is plotted below. Bottom: Change in low-\\ndimensional pose state. Estimated pose trajectories derived from unablated \\n(black) or ablated (blue) data. Each dimension of the latent pose space is plotted \\nseparately. Lines reflect the mean across modeling runs. d) Cross-correlation \\nof transition probabilities for ablated vs. unablated data (computed over \\nframes that were included in an ablation), shown for keypoint-MoSeq (red) \\nand traditional AR-HMM-based MoSeq (red), Shading shows bootstrap 95% \\nconfidence intervals for N=20 model fits. Solid line shows cross-correlation using \\nall N=20 models (without bootstrapping). e) Mean Kullback-Leibler divergence \\n[as described in (c)] across all ablation intervals, stratified by number of ablated \\nkeypoints (left) or duration of the ablation (right). Shading represents the 99% \\nconfidence interval of the mean. f) Mean distance between pose states estimated \\nfrom ablated vs. unablated data, with colors and shading as in (e). g) Syllable \\ncross-likelihoods, defined as the probability, on average, that time-intervals \\nassigned to one syllable (column) could have arisen from another syllable (row). \\nCross-likelihoods were calculated for keypoint-MoSeq and for depth MoSeq. The \\nresults for both methods are plotted twice, using either an absolute scale (left) or \\na log scale (right). h) Modeling results for synthetic keypoint data with a similar \\nstatistical structure as the real data but lacking in changepoints. Left: example of \\nsynthetic keypoint trajectories. Middle: autocorrelation of keypoint coordinates \\nfor real vs. synthetic data, showing similar dynamics at short timescales. Right: \\ndistribution of syllable frequencies for keypoint-MoSeq models trained on real \\nvs. synthetic data.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 23, 'page_label': '1352'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 3 | Convergence and model selection. a) Probabilistic \\ngraphical model (PGM) for keypoint-MoSeq highlighting the discrete syllable \\nstate. b) Number of syllables identified by keypoint-MoSeq as a function of \\nfitting iteration, shown for multiple independent runs of fitting (referred to as \\n‘chains’). c) Confusion matrices depicting closer agreement between syllables \\nfrom the same chain at different stages of fitting (left) compared to syllables from \\ndifferent chains at the final stage of fitting (right). d) Distributions of syllable \\nsequence similarity [quantified by normalized mutual information (NMI)], either \\nwithin chains at different iterations (N=20) or across chains (N=190). e) PGM \\nhighlighting pose state. f ) Left: within- and between- chain variation in pose \\nstate, shown for each dimension of pose (rows) across an example 10-second \\ninterval. Gray lines represent the variation across fitting iterations within each \\nchain, and black lines represent the total variation across chains and fitting \\niterations. Right: zoom-in on a 2-second interval showing close agreement in the \\nfinal pose trajectory learned by each chain. g) Distribution of the Gelman-Rubin \\nstatistic (ratio of within-chain variance to total variance) across timepoints and \\ndimensions of the pose state. h) Expected marginal likelihood (EML) scores \\n(defined as a mean over marginal likelihoods) for the final model parameters \\nlearned by each chain. Vertical bars represent standard error based on N=20 \\nchains. i) The scores shown in (h) correlate with mean NMI for each model, \\nwhich is low when a model’s syllable sequences are dissimilar from those of \\nother models (P=0.005, Pearson test). j) EML scores are higher for models fit \\nwith an autoregressive-only (AR-only) initialization stage (left) compared to \\nthose without (right; P = 0.004, N=20 fits for each method, Mann-Whitney U \\ntest). Plotted as in (h). k) EML scores (bottom) plateau within 500 iterations of \\nGibbs sampling and have a similar trajectory as the model log joint probability \\n(top). Black lines represent the median across N=20 chains and shaded regions \\nrepresent inter-quartile interval. l) Illustration of uncertainty in syllable sequence \\ngiven a fixed set of syllable definitions. T op: syllable sequences derived from \\nGibbs sampling (conditioning on fixed autoregressive parameters and transition \\nprobabilities), shown for an example 10-second window. Bottom: per-frame \\nmarginal probability estimates for each syllable. Each line is one syllable, with \\ncolors as in the heatmap above.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 24, 'page_label': '1353'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 4 | Behaviors captured by keypoint-MoSeq syllables. \\na) Average pose trajectories for syllables identified by keypoint-MoSeq. Each \\ntrajectory includes ten evenly timed poses from 165ms before to 500ms after \\nsyllable onset. b) Kinematic and morphological parameters for each syllable. \\nLeft:  Average values of five parameters (rows) for each syllable (column). Middle: \\nMean and interquartile range of each parameter for one example syllable. Right: \\ncartoons illustrating the computation of the three morphological parameters.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 25, 'page_label': '1354'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 5 | Method-to-method differences in sensitivity to \\nbehavioral changepoints are robust to parameter settings. a) Output of \\nunsupervised behavior segmentation algorithms across a range of parameter \\nsettings, applied to 2D keypoint data from two different camera angles  \\n(N=1 model fits per parameter set). The median state duration (left) and the \\naverage (z-scored) keypoint change score aligned to state transitions (right) \\nare shown for each method and parameter value. Gray pointers indicate default \\nparameter values used for subsequent analysis (see Supplementary Table 3 for \\na summary of parameters). b) Distributions showing the number of transitions \\nthat occur during each rear. c) Accuracy of kinematic decoding models that were \\nfit to state sequences from each method.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 26, 'page_label': '1355'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 6 | Accelerometry reveals kinematic transitions at the onsets of keypoint-MoSeq states. a) IMU signals aligned to state onsets from several \\nbehavior segmentation methods. Each row corresponds to a behavior state and shows the average across all onset times for that state. A single model fit is shown for \\neach method.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 27, 'page_label': '1356'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 7 | Striatal dopamine fluctuations are enriched at \\nkeypoint-MoSeq syllable onsets. a) Derivative of the dopamine signal aligned \\nto the onsets of high velocity or low velocity behavior states. States from each \\nmethod were classified evenly as high or low velocity based on the mean centroid \\nvelocity during their respective frames. Plots show mean and inter-95% range \\nacross N=20 model fits. b) Distributions capturing the average absolute value of \\nthe dopamine signal across states from each method. c) Relationship between \\nstate durations and correlations from Fig. 5f. d) Average dopamine fluctuations \\naligned to state onsets (left) or aligned to random frames throughout the \\nexecution of each state (middle), as well as the absolute difference between \\nthe two alignment approaches (right), shown for each unsupervised behavior \\nsegmentation approach.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 28, 'page_label': '1357'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 8 | Changes in behavior caused by environmental \\nenrichment. a) Example frames from conventional 2D videos of the empty bin \\n(left), and enriched environment (middle), as well as depth video of the enriched \\nenvironment (right). b) Graph showing changes in syllable-to-syllable transition \\nstatistics across environments. Edge color and width indicate the sign and \\nmagnitude of change in the frequency of each syllable pair. c) Right: changes \\nin syllable frequency across environments, with stars indicating significant \\ndifferences (P < 0.05, N=16, Mann-Whitney U test). Error bars show standard \\nerror of the mean. Left: Syllable groupings defined by clustering of the transition \\ngraph shown in (b).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 29, 'page_label': '1358'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 9 | Supervised behavior benchmark. a) Distribution of \\nstate durations from each behavior segmentation method for the open field \\nbenchmark (top) and the CalMS21 social behavior benchmark (bottom).  \\nb) Three different similarity measures applied to the output of each unsupervised \\nbehavior analysis method, showing the median (gray bars) and inter-quartile \\ninterval (black lines) across independent model fits (N=20; * P < 10\\n−5, for keypoint-\\nMoSeq vs. each other method, Mann-Whitney U test). c) Number of unsupervised \\nstates specific to each human-annotated behavior in the CalMS21 dataset, shown \\nfor 20 independent fits of each unsupervised method. A state was defined as \\nspecific if > 50% of frames bore the annotation. d) Left: Keypoints tracked in \\n2D (top) or 3D (bottom) and corresponding egocentric coordinate axes. Right: \\nexample keypoint trajectories and transition probabilities from keypoint-\\nMoSeq. Transition probability is defined, for each frame, as the probability of a \\nsyllable transition occurring on that frame. e) Cumulative fraction of explained \\nvariance for increasing number of principal components (PCs). PCs were fit to \\negocentrically aligned 2D keypoints, egocentrically aligned 3D keypoints, or \\ndepth videos respectively. f) Cross-correlation between the 3D keypoint change \\nscore and change scores derived from 2D keypoints and depth respectively \\n(based on N=20 model fits).'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 30, 'page_label': '1359'}, page_content='Nature Methods\\nArticle https://doi.org/10.1038/s41592-024-02318-2\\nExtended Data Fig. 10 | Keypoint-MoSeq identifies behavioral motifs across \\ntimescales. a-b) Alignment of mouse behavior motifs to respiration. Figure \\ncreated with SciDraw under a CC BY 4.0 license. a) Left: Keypoints used for model \\nfitting. Middle: Median motif durations for models fit with a range of stickiness \\nhyperparameters. Right: Proportion of significantly respiration-aligned motifs, \\nstratified by stickiness hyperparameter, showing mean and standard deviation \\nacross N=5 model fits. b) As (a), but restricted to upper spine, neck, head, and \\nnose keypoints. c-h) Keypoint-MoSeq partitions fly behavior across timescales. \\nc) Fly keypoints used for fitting keypoint-MoSeq and MotionMapper. d) Motif \\ndurations (left) and number of motifs (right) for models trained with a range \\nof target timescales. T en separate models were fit for each timescale. For motif \\ndurations, we pooled the duration distributions from all 20 models and plotted \\nthe median duration in black and interquartile range in gray. For motif number, \\nwe counted the number of motifs with frequency above 0.5% for each of the  \\n20 models and plotted the mean of this count in black and the standard deviation \\nin gray. e) Density of points in 2D ‘behavior space’ generated by MotionMapper. \\nEach white-line delimited region corresponds to a MotionMapper state label. \\nf) Confusion matrices showing the frequency of each MotionMapper state \\nduring each keypoint-MoSeq motif. g) Example of swing and stance annotations \\nover a 600ms window. Lines show the egocentric coordinate of each leg tip \\n(anterior-posterior axis only). Gray shading denotes the swing phase, defined \\nas the interval posterior-to-anterior limb motion. h) Cross-correlation between \\nthe spectrograms of keypoints and motif labels respectively. Heatmap rows \\ncorrespond to frequency bands of the spectrograms and columns correspond to \\nmodels with different target timescales.'),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 31, 'page_label': '1360'}, page_content=''),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 32, 'page_label': '1361'}, page_content=''),\n",
              " Document(metadata={'producer': 'PyPDF', 'creator': 'Springer', 'creationdate': '2024-07-05T16:35:05+05:30', 'author': 'Caleb Weinreb', 'keywords': '', 'moddate': '2024-07-05T16:36:42+05:30', 'subject': 'Nature Methods, doi:10.1038/s41592-024-02318-2', 'title': 'Keypoint-MoSeq: parsing behavior by linking point tracking to pose dynamics', 'source': '/content/s41592-024-02318-2.pdf', 'total_pages': 34, 'page': 33, 'page_label': '1362'}, page_content='')]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2pNaFMpZnV9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0638a87e7b3743328563d1eb52af5712": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6068ddea0bc04c2ab09c7d29a44d1fab",
            "placeholder": "​",
            "style": "IPY_MODEL_64e53872c83d414793dc478a44ab6f3c",
            "value": "model.safetensors: 100%"
          }
        },
        "08f501eb518b4bd3bd8bf79b8ad7a851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d687994025f464eb9579bfe07323e46",
            "placeholder": "​",
            "style": "IPY_MODEL_d1561b44205f4412a6a7a5d1c7c12f3f",
            "value": " 2.20k/2.20k [00:00&lt;00:00, 105kB/s]"
          }
        },
        "0f162eb39510481f8804b88750475ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b907cf86c13a46a6af412667d09097be",
            "placeholder": "​",
            "style": "IPY_MODEL_8142034b8c964695bb9154bf7ad10ae9",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "118abefd78c246169876fde4e4814221": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0638a87e7b3743328563d1eb52af5712",
              "IPY_MODEL_fc92b0adf49b4aee84b149ad76f0d68b",
              "IPY_MODEL_d1d28b001a7e41e9a42d451a586b2a3e"
            ],
            "layout": "IPY_MODEL_2e84e477d99e466f8b7e63c8b994a041"
          }
        },
        "184a6b0567ac4eadb614d643811e5f98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d5f01969dfc43bdae8544e0ff396082": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d687994025f464eb9579bfe07323e46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f7e9fe4d2264b19a37ba899b116b3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "206afcbb1d36451b96478bda2e500e2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "282b6378f4db4f35be6621e43e4ac3d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e5c07962ab24b238ffc324fcd5eb90b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e84e477d99e466f8b7e63c8b994a041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3029b3d323ed49a2b1512a82cfefc466": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "317a88e54b734a8fa4a923c24f7b3728": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3371c281b55848649d15ac9fe52ae998": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33e84737bdde416d990cd1fc4328b559": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f162eb39510481f8804b88750475ff7",
              "IPY_MODEL_f18e4dd72b314acfa93d4361efbdf6c8",
              "IPY_MODEL_08f501eb518b4bd3bd8bf79b8ad7a851"
            ],
            "layout": "IPY_MODEL_317a88e54b734a8fa4a923c24f7b3728"
          }
        },
        "37853abba1cd4a94b6c09aa96b6aaf71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1a44fafca5443abd70435ddf2fed32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ea1db3538334cfe84414363908691bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fc5f3c0cc6b4bbc8b08ba494f826a08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43d1f96e56184f32bfc8ee36d4e95911": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48e7e7fac3d142fbab02ceaa2464203d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_876f26337f5a46659d449c79906e3248",
            "placeholder": "​",
            "style": "IPY_MODEL_762216e1d5a94d25910c8d1fecf4e061",
            "value": "tokenizer.json: 100%"
          }
        },
        "4a2a63b055644f1a88acd46783952802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e3e03972f3f4933bb932bb5cee5133c",
            "placeholder": "​",
            "style": "IPY_MODEL_e180f765788344809fc1284d60bd928e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4fbe617a082e45388c44d45697d12579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f7e9fe4d2264b19a37ba899b116b3d5",
            "placeholder": "​",
            "style": "IPY_MODEL_d5dfa3e2c38e4cc3b5816ec682e76977",
            "value": " 147/147 [00:00&lt;00:00, 7.66kB/s]"
          }
        },
        "50eeec232a86464eae48c1a7c4e3b33d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518c51a405384d10b48ea7e4454b2c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a2a63b055644f1a88acd46783952802",
              "IPY_MODEL_a3f098680ce9429f93314856e6dd3d3f",
              "IPY_MODEL_76241d8acf2d41d1a2774593c75807b1"
            ],
            "layout": "IPY_MODEL_a86ff14f78c6455aa419a2f2cc1f2011"
          }
        },
        "54f46301af2641a68691b8812ed7b8e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e3e03972f3f4933bb932bb5cee5133c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6068ddea0bc04c2ab09c7d29a44d1fab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61413db9244c4a9baece80f7e24ffdc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d5f01969dfc43bdae8544e0ff396082",
            "placeholder": "​",
            "style": "IPY_MODEL_6e896e3e635849ee9a47a7b3d7c0870f",
            "value": " 792k/792k [00:00&lt;00:00, 443kB/s]"
          }
        },
        "6453810b3d5b48e7a058057c94b0ee19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae245792670444d867e77fb144c36d4",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_184a6b0567ac4eadb614d643811e5f98",
            "value": 791656
          }
        },
        "64e53872c83d414793dc478a44ab6f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "673aee1a3c614cf9bc6c876f4938bc23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "682b34503dcc40c48b51104e1baa289f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6988f36e49104366a2987e9f2e143cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b54e8b3d08645529bffa73b3d8b39f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8318e557983f4fca8983a8e02631b929",
              "IPY_MODEL_6453810b3d5b48e7a058057c94b0ee19",
              "IPY_MODEL_61413db9244c4a9baece80f7e24ffdc3"
            ],
            "layout": "IPY_MODEL_3ea1db3538334cfe84414363908691bd"
          }
        },
        "6e896e3e635849ee9a47a7b3d7c0870f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a9d6bab0af4ecfbc4cf98bfbb486d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "762216e1d5a94d25910c8d1fecf4e061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76241d8acf2d41d1a2774593c75807b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37853abba1cd4a94b6c09aa96b6aaf71",
            "placeholder": "​",
            "style": "IPY_MODEL_d3cd418b597d4b8a8e57d12e73935d78",
            "value": " 2.54k/2.54k [00:00&lt;00:00, 166kB/s]"
          }
        },
        "77634e4689d24f89bd5064d2bf03fadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e385f4d6e7784b14b3320d464255a6e7",
            "placeholder": "​",
            "style": "IPY_MODEL_206afcbb1d36451b96478bda2e500e2f",
            "value": " 662/662 [00:00&lt;00:00, 10.2kB/s]"
          }
        },
        "7e4b3b75960c4c299e1fbefdc5b41901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f49691a0a1b84aa6b07bbfefa1afe672",
            "max": 2424064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e5c07962ab24b238ffc324fcd5eb90b",
            "value": 2424064
          }
        },
        "8142034b8c964695bb9154bf7ad10ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8318e557983f4fca8983a8e02631b929": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db613ad4b4f444c1a62137818e3e52d8",
            "placeholder": "​",
            "style": "IPY_MODEL_de217d1d23b24a538f806f57441ed341",
            "value": "spiece.model: 100%"
          }
        },
        "8593adf046b14bc2b219b4b18fc9616a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48e7e7fac3d142fbab02ceaa2464203d",
              "IPY_MODEL_7e4b3b75960c4c299e1fbefdc5b41901",
              "IPY_MODEL_bd104e6d00154bf1904fd622ce65136b"
            ],
            "layout": "IPY_MODEL_71a9d6bab0af4ecfbc4cf98bfbb486d3"
          }
        },
        "876f26337f5a46659d449c79906e3248": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ad1fdf0eba4f24bcb38ce8c457c377": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae245792670444d867e77fb144c36d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d951deb5a174f30be5f35863059a09d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fa2512c32984ed3962c2351919f5acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d643e1a4e1c84ed7bd73602916a6b875",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43d1f96e56184f32bfc8ee36d4e95911",
            "value": 662
          }
        },
        "92cbf25314604ccd90a8b09d2e1f35f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92e7866e54de4761bf9bf5fe034cf4e9",
              "IPY_MODEL_ea0bb33e4fe44457a16bbdf49405616a",
              "IPY_MODEL_4fbe617a082e45388c44d45697d12579"
            ],
            "layout": "IPY_MODEL_e9ca24322a57475da4e1d4c244444a51"
          }
        },
        "92e7866e54de4761bf9bf5fe034cf4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87ad1fdf0eba4f24bcb38ce8c457c377",
            "placeholder": "​",
            "style": "IPY_MODEL_bde63f1e63a14d18b48eca078dbe696d",
            "value": "generation_config.json: 100%"
          }
        },
        "a0921a5dfd684aeaa84daa502ac28a76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f098680ce9429f93314856e6dd3d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fc5f3c0cc6b4bbc8b08ba494f826a08",
            "max": 2539,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_282b6378f4db4f35be6621e43e4ac3d3",
            "value": 2539
          }
        },
        "a86ff14f78c6455aa419a2f2cc1f2011": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b907cf86c13a46a6af412667d09097be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd104e6d00154bf1904fd622ce65136b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2ca71aafa58424f839bcadd25894951",
            "placeholder": "​",
            "style": "IPY_MODEL_6988f36e49104366a2987e9f2e143cd9",
            "value": " 2.42M/2.42M [00:00&lt;00:00, 9.84MB/s]"
          }
        },
        "bde63f1e63a14d18b48eca078dbe696d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6289bb49de14d5ab4ae07f0243ba136": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1561b44205f4412a6a7a5d1c7c12f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1d28b001a7e41e9a42d451a586b2a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3029b3d323ed49a2b1512a82cfefc466",
            "placeholder": "​",
            "style": "IPY_MODEL_673aee1a3c614cf9bc6c876f4938bc23",
            "value": " 3.13G/3.13G [00:43&lt;00:00, 164MB/s]"
          }
        },
        "d3cd418b597d4b8a8e57d12e73935d78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5dfa3e2c38e4cc3b5816ec682e76977": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d643e1a4e1c84ed7bd73602916a6b875": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db613ad4b4f444c1a62137818e3e52d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe60647d9d24540ae3e1827d23734b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6f67208f1474855b8e1ac002da5738a",
              "IPY_MODEL_8fa2512c32984ed3962c2351919f5acd",
              "IPY_MODEL_77634e4689d24f89bd5064d2bf03fadb"
            ],
            "layout": "IPY_MODEL_e31adf44110c4103902fca4882200db7"
          }
        },
        "de217d1d23b24a538f806f57441ed341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e180f765788344809fc1284d60bd928e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e31adf44110c4103902fca4882200db7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e385f4d6e7784b14b3320d464255a6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6f67208f1474855b8e1ac002da5738a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d951deb5a174f30be5f35863059a09d",
            "placeholder": "​",
            "style": "IPY_MODEL_50eeec232a86464eae48c1a7c4e3b33d",
            "value": "config.json: 100%"
          }
        },
        "e9ca24322a57475da4e1d4c244444a51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0bb33e4fe44457a16bbdf49405616a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0921a5dfd684aeaa84daa502ac28a76",
            "max": 147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54f46301af2641a68691b8812ed7b8e0",
            "value": 147
          }
        },
        "f18e4dd72b314acfa93d4361efbdf6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_682b34503dcc40c48b51104e1baa289f",
            "max": 2201,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b1a44fafca5443abd70435ddf2fed32",
            "value": 2201
          }
        },
        "f2ca71aafa58424f839bcadd25894951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49691a0a1b84aa6b07bbfefa1afe672": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc92b0adf49b4aee84b149ad76f0d68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3371c281b55848649d15ac9fe52ae998",
            "max": 3132668804,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6289bb49de14d5ab4ae07f0243ba136",
            "value": 3132668804
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
